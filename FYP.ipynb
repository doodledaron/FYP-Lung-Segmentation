{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FzfrYMubBu4G"
   },
   "source": [
    "# Lung Cancer Segmentation using nnU-Net\n",
    "Author: Tang Yu Xuan <br />\n",
    "Institution: Multimedia University<br /><br />\n",
    "## Table of Contents\n",
    "\n",
    "1. Environment Setup\n",
    "2. Data Analysis\n",
    "3. Preprocessing\n",
    "4. Training\n",
    "5. Evaluation\n",
    "6. Visualization\n",
    "7. Clinical Validation\n",
    "8. Results Export"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S9EAQVeUuuQb"
   },
   "source": [
    "# 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doodledaron\n"
     ]
    }
   ],
   "source": [
    "!whoami"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AQqX-4fyE0AE"
   },
   "source": [
    "## Install required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 56991,
     "status": "ok",
     "timestamp": 1734495028037,
     "user": {
      "displayName": "Yu Xuan Tang",
      "userId": "11252039200120625827"
     },
     "user_tz": -480
    },
    "id": "h4a_6Pd6FFUv",
    "outputId": "60ab644b-0908-4169-bac9-db3cbea7bbe6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nnunetv2\n",
      "  Downloading nnunetv2-2.5.1.tar.gz (196 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m197.0/197.0 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: torch>=2.1.2 in /usr/local/lib/python3.10/dist-packages (from nnunetv2) (2.5.1+cu121)\n",
      "Collecting acvl-utils<0.3,>=0.2 (from nnunetv2)\n",
      "  Downloading acvl_utils-0.2.2.tar.gz (26 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting dynamic-network-architectures<0.4,>=0.3.1 (from nnunetv2)\n",
      "  Downloading dynamic_network_architectures-0.3.1.tar.gz (20 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nnunetv2) (4.66.6)\n",
      "Collecting dicom2nifti (from nnunetv2)\n",
      "  Downloading dicom2nifti-2.5.1-py3-none-any.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from nnunetv2) (1.13.1)\n",
      "Collecting batchgenerators>=0.25 (from nnunetv2)\n",
      "  Downloading batchgenerators-0.25.1.tar.gz (76 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.0/77.0 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from nnunetv2) (1.26.4)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from nnunetv2) (1.5.2)\n",
      "Requirement already satisfied: scikit-image>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from nnunetv2) (0.24.0)\n",
      "Collecting SimpleITK>=2.2.1 (from nnunetv2)\n",
      "  Downloading SimpleITK-2.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.9 kB)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from nnunetv2) (2.2.2)\n",
      "Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from nnunetv2) (0.20.3)\n",
      "Requirement already satisfied: tifffile in /usr/local/lib/python3.10/dist-packages (from nnunetv2) (2024.12.12)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from nnunetv2) (2.32.3)\n",
      "Requirement already satisfied: nibabel in /usr/local/lib/python3.10/dist-packages (from nnunetv2) (5.3.2)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from nnunetv2) (3.8.0)\n",
      "Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (from nnunetv2) (0.13.2)\n",
      "Collecting imagecodecs (from nnunetv2)\n",
      "  Downloading imagecodecs-2024.9.22-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Collecting yacs (from nnunetv2)\n",
      "  Downloading yacs-0.1.8-py3-none-any.whl.metadata (639 bytes)\n",
      "Collecting batchgeneratorsv2>=0.2 (from nnunetv2)\n",
      "  Downloading batchgeneratorsv2-0.2.1.tar.gz (34 kB)\n",
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from nnunetv2) (0.8.0)\n",
      "Collecting connected-components-3d (from acvl-utils<0.3,>=0.2->nnunetv2)\n",
      "  Downloading connected_components_3d-3.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (32 kB)\n",
      "Collecting blosc2>=3.0.0b4 (from acvl-utils<0.3,>=0.2->nnunetv2)\n",
      "  Downloading blosc2-3.0.0b4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from batchgenerators>=0.25->nnunetv2) (11.0.0)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from batchgenerators>=0.25->nnunetv2) (1.0.0)\n",
      "Collecting unittest2 (from batchgenerators>=0.25->nnunetv2)\n",
      "  Downloading unittest2-1.1.0-py2.py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: threadpoolctl in /usr/local/lib/python3.10/dist-packages (from batchgenerators>=0.25->nnunetv2) (3.5.0)\n",
      "Collecting fft-conv-pytorch (from batchgeneratorsv2>=0.2->nnunetv2)\n",
      "  Downloading fft_conv_pytorch-1.2.0-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: networkx>=2.8 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.19.3->nnunetv2) (3.4.2)\n",
      "Requirement already satisfied: imageio>=2.33 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.19.3->nnunetv2) (2.36.1)\n",
      "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.19.3->nnunetv2) (24.2)\n",
      "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.19.3->nnunetv2) (0.4)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.2->nnunetv2) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.2->nnunetv2) (4.12.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.2->nnunetv2) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.2->nnunetv2) (2024.10.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.2->nnunetv2) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=2.1.2->nnunetv2) (1.3.0)\n",
      "Collecting pydicom>=2.2.0 (from dicom2nifti->nnunetv2)\n",
      "  Downloading pydicom-3.0.1-py3-none-any.whl.metadata (9.4 kB)\n",
      "Collecting python-gdcm (from dicom2nifti->nnunetv2)\n",
      "  Downloading python_gdcm-3.0.24.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->nnunetv2) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->nnunetv2) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->nnunetv2) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->nnunetv2) (1.4.7)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->nnunetv2) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->nnunetv2) (2.8.2)\n",
      "Requirement already satisfied: importlib-resources>=5.12 in /usr/local/lib/python3.10/dist-packages (from nibabel->nnunetv2) (6.4.5)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->nnunetv2) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->nnunetv2) (2024.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->nnunetv2) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->nnunetv2) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->nnunetv2) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->nnunetv2) (2024.8.30)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->nnunetv2) (1.4.2)\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from yacs->nnunetv2) (6.0.2)\n",
      "Requirement already satisfied: ndindex in /usr/local/lib/python3.10/dist-packages (from blosc2>=3.0.0b4->acvl-utils<0.3,>=0.2->nnunetv2) (1.9.2)\n",
      "Requirement already satisfied: msgpack in /usr/local/lib/python3.10/dist-packages (from blosc2>=3.0.0b4->acvl-utils<0.3,>=0.2->nnunetv2) (1.1.0)\n",
      "Requirement already satisfied: numexpr in /usr/local/lib/python3.10/dist-packages (from blosc2>=3.0.0b4->acvl-utils<0.3,>=0.2->nnunetv2) (2.10.2)\n",
      "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from blosc2>=3.0.0b4->acvl-utils<0.3,>=0.2->nnunetv2) (9.0.0)\n",
      "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from blosc2>=3.0.0b4->acvl-utils<0.3,>=0.2->nnunetv2) (0.28.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->nnunetv2) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.1.2->nnunetv2) (3.0.2)\n",
      "Collecting argparse (from unittest2->batchgenerators>=0.25->nnunetv2)\n",
      "  Downloading argparse-1.4.0-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting traceback2 (from unittest2->batchgenerators>=0.25->nnunetv2)\n",
      "  Downloading traceback2-1.4.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->blosc2>=3.0.0b4->acvl-utils<0.3,>=0.2->nnunetv2) (3.7.1)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx->blosc2>=3.0.0b4->acvl-utils<0.3,>=0.2->nnunetv2) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx->blosc2>=3.0.0b4->acvl-utils<0.3,>=0.2->nnunetv2) (0.14.0)\n",
      "Collecting linecache2 (from traceback2->unittest2->batchgenerators>=0.25->nnunetv2)\n",
      "  Downloading linecache2-1.0.0-py2.py3-none-any.whl.metadata (1000 bytes)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->blosc2>=3.0.0b4->acvl-utils<0.3,>=0.2->nnunetv2) (1.3.1)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->blosc2>=3.0.0b4->acvl-utils<0.3,>=0.2->nnunetv2) (1.2.2)\n",
      "Downloading SimpleITK-2.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (52.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.4/52.4 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dicom2nifti-2.5.1-py3-none-any.whl (43 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading imagecodecs-2024.9.22-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (43.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.3/43.3 MB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
      "Downloading blosc2-3.0.0b4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m85.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydicom-3.0.1-py3-none-any.whl (2.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m78.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading connected_components_3d-3.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m80.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fft_conv_pytorch-1.2.0-py3-none-any.whl (6.8 kB)\n",
      "Downloading python_gdcm-3.0.24.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m83.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading unittest2-1.1.0-py2.py3-none-any.whl (96 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.4/96.4 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading argparse-1.4.0-py2.py3-none-any.whl (23 kB)\n",
      "Downloading traceback2-1.4.0-py2.py3-none-any.whl (16 kB)\n",
      "Downloading linecache2-1.0.0-py2.py3-none-any.whl (12 kB)\n",
      "Building wheels for collected packages: nnunetv2, acvl-utils, batchgenerators, batchgeneratorsv2, dynamic-network-architectures\n",
      "  Building wheel for nnunetv2 (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for nnunetv2: filename=nnunetv2-2.5.1-py3-none-any.whl size=264367 sha256=b728fe89c3a5e52146468781670983ff16a768d77c651ce6c280d35f5cbebe27\n",
      "  Stored in directory: /root/.cache/pip/wheels/5d/d6/90/88743b341922dc9f6795742570aac83a1eaa55f77ee676a5a6\n",
      "  Building wheel for acvl-utils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for acvl-utils: filename=acvl_utils-0.2.2-py3-none-any.whl size=24723 sha256=e4d493b1f4aa915a29facb86ce7a0c425afa2709e93eb27b94bf9780c3c0508d\n",
      "  Stored in directory: /root/.cache/pip/wheels/d1/56/f0/c3ece6950db0c5fb6ad37b5ce1406a4a1b035840c468c53202\n",
      "  Building wheel for batchgenerators (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for batchgenerators: filename=batchgenerators-0.25.1-py3-none-any.whl size=93088 sha256=fa1cef9b3e76da8ef4197a6f78a50295ba3e44c6c96d979fe498b161aab7d600\n",
      "  Stored in directory: /root/.cache/pip/wheels/be/1b/30/b3f066999ad01855fc903fe7c93c25682333dd5645d5c75434\n",
      "  Building wheel for batchgeneratorsv2 (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for batchgeneratorsv2: filename=batchgeneratorsv2-0.2.1-py3-none-any.whl size=45185 sha256=19aa4fea36e5d976c9a1d3da4066936b926ce63ee16b88fb13a5873696c0ea19\n",
      "  Stored in directory: /root/.cache/pip/wheels/a7/20/91/33993997db216e7b946d379850c47837d2478be49377a6cb41\n",
      "  Building wheel for dynamic-network-architectures (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for dynamic-network-architectures: filename=dynamic_network_architectures-0.3.1-py3-none-any.whl size=30046 sha256=1abba92a1779a3bf3bfe4c376514407d3f1d0236ef536aa1d225cfa979a51b24\n",
      "  Stored in directory: /root/.cache/pip/wheels/55/1b/13/a6419c8dbf998b9343710355ec3edc5c8e24d9b7b22eec95fb\n",
      "Successfully built nnunetv2 acvl-utils batchgenerators batchgeneratorsv2 dynamic-network-architectures\n",
      "Installing collected packages: SimpleITK, linecache2, argparse, yacs, traceback2, python-gdcm, pydicom, imagecodecs, connected-components-3d, unittest2, dicom2nifti, fft-conv-pytorch, dynamic-network-architectures, blosc2, batchgenerators, batchgeneratorsv2, acvl-utils, nnunetv2\n",
      "  Attempting uninstall: blosc2\n",
      "    Found existing installation: blosc2 2.7.1\n",
      "    Uninstalling blosc2-2.7.1:\n",
      "      Successfully uninstalled blosc2-2.7.1\n",
      "Successfully installed SimpleITK-2.4.0 acvl-utils-0.2.2 argparse-1.4.0 batchgenerators-0.25.1 batchgeneratorsv2-0.2.1 blosc2-3.0.0b4 connected-components-3d-3.21.0 dicom2nifti-2.5.1 dynamic-network-architectures-0.3.1 fft-conv-pytorch-1.2.0 imagecodecs-2024.9.22 linecache2-1.0.0 nnunetv2-2.5.1 pydicom-3.0.1 python-gdcm-3.0.24.1 traceback2-1.4.0 unittest2-1.1.0 yacs-0.1.8\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "id": "2cc4d7b3fbcc4b00959a985aee347e7e",
       "pip_warning": {
        "packages": [
         "argparse"
        ]
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.18.7)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.43)\n",
      "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.3.6)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (4.25.5)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.2)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.32.3)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.19.2)\n",
      "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.4)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (75.1.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.10/dist-packages (from wandb) (4.12.2)\n",
      "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.8.30)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n"
     ]
    }
   ],
   "source": [
    "# Import basic packages\n",
    "!pip install nnunetv2\n",
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 348
    },
    "executionInfo": {
     "elapsed": 429,
     "status": "error",
     "timestamp": 1734494966843,
     "user": {
      "displayName": "Yu Xuan Tang",
      "userId": "11252039200120625827"
     },
     "user_tz": -480
    },
    "id": "bO86sDwTFIq_",
    "outputId": "67e3f2c4-8fd0-499c-ce96-2c8a35725967"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nnunetv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-260554a14254>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Check if nnunet can be imported\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnnunetv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'nnunetv2'",
      "",
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Check if nnunet can be imported\n",
    "import nnunetv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iI1fltW7CTNl"
   },
   "source": [
    "## 1.1 Import Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 11126,
     "status": "ok",
     "timestamp": 1734495100023,
     "user": {
      "displayName": "Yu Xuan Tang",
      "userId": "11252039200120625827"
     },
     "user_tz": -480
    },
    "id": "is9GXRZ9uPsk"
   },
   "outputs": [],
   "source": [
    "# Basic packages\n",
    "import os\n",
    "import shutil\n",
    "import datetime\n",
    "from collections import OrderedDict\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import torch\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "# For visualization\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "from skimage.measure import marching_cubes  # For 3D visualization\n",
    "\n",
    "# For evaluation metrics\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
    "from medpy.metric.binary import hd95\n",
    "\n",
    "# Experiment tracking\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UqrPL6GnCuSb"
   },
   "source": [
    "## 1.2 Setup logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 329,
     "status": "ok",
     "timestamp": 1734495154834,
     "user": {
      "displayName": "Yu Xuan Tang",
      "userId": "11252039200120625827"
     },
     "user_tz": -480
    },
    "id": "2gi0Cz_cCx0Z",
    "outputId": "6b56155b-dcee-4f6f-c083-5937376c183b"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "from datetime import datetime\n",
    "import os\n",
    "import sys\n",
    "from typing import Optional\n",
    "\n",
    "class Logger:\n",
    "    \"\"\"Centralized logging configuration for the project\"\"\"\n",
    "    \n",
    "    def __init__(self, log_dir: str, name: str = \"lung_segmentation\"):\n",
    "        self.log_dir = log_dir\n",
    "        self.name = name\n",
    "        self._initialize_logging()\n",
    "\n",
    "    def _initialize_logging(self):\n",
    "        \"\"\"Initialize logging configuration\"\"\"\n",
    "        # Create logs directory if it doesn't exist\n",
    "        os.makedirs(self.log_dir, exist_ok=True)\n",
    "\n",
    "        # Create timestamped log file\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        log_file = os.path.join(self.log_dir, f\"{self.name}_{timestamp}.log\")\n",
    "\n",
    "        # Configure logging format\n",
    "        formatter = logging.Formatter(\n",
    "            '%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "            datefmt='%Y-%m-%d %H:%M:%S'\n",
    "        )\n",
    "\n",
    "        # File handler\n",
    "        file_handler = logging.FileHandler(log_file)\n",
    "        file_handler.setFormatter(formatter)\n",
    "\n",
    "        # Console handler\n",
    "        console_handler = logging.StreamHandler(sys.stdout)\n",
    "        console_handler.setFormatter(formatter)\n",
    "\n",
    "        # Configure root logger\n",
    "        logger = logging.getLogger()\n",
    "        logger.setLevel(logging.INFO)\n",
    "        logger.addHandler(file_handler)\n",
    "        logger.addHandler(console_handler)\n",
    "\n",
    "        # Log initial message\n",
    "        logging.info(f\"Logging initialized. Log file: {log_file}\")\n",
    "\n",
    "    @staticmethod\n",
    "    def get_logger(name: Optional[str] = None) -> logging.Logger:\n",
    "        \"\"\"Get logger instance\"\"\"\n",
    "        return logging.getLogger(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ffk4jxxlC69a"
   },
   "source": [
    "## 1.3 GPU Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 559,
     "status": "ok",
     "timestamp": 1734495272957,
     "user": {
      "displayName": "Yu Xuan Tang",
      "userId": "11252039200120625827"
     },
     "user_tz": -480
    },
    "id": "4gSqxukcC9k1",
    "outputId": "cf1f2cee-aa01-4ed8-c64b-ca75ba468fcf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA GeForce RTX 4060 Ti\n",
      "GPU Memory: 17.18 GB\n"
     ]
    }
   ],
   "source": [
    "def check_gpu():\n",
    "    \"\"\"Verify GPU availability and print device information\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"No GPU available, using CPU\")\n",
    "    return device\n",
    "\n",
    "# this is the code from original workshop, just in case the check_gpu() is not working\n",
    "# check whether GPU accelerated computing is available\n",
    "# if there is an error here, enable GPU in the Runtime\n",
    "# assert torch.cuda.is_available()\n",
    "\n",
    "device = check_gpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "53AqZcnrDNbr"
   },
   "source": [
    "## 1.4 Initialize Weights & Biases - External library to track GPU Performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 233
    },
    "executionInfo": {
     "elapsed": 71593,
     "status": "ok",
     "timestamp": 1734443148187,
     "user": {
      "displayName": "Yu Xuan Tang",
      "userId": "11252039200120625827"
     },
     "user_tz": -480
    },
    "id": "Xnlla_VjueYs",
    "outputId": "edcf10dc-fdb2-453c-9db5-5ef7e9493967"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ········\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/doodledaron/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/doodledaron/FYP/nnUnet/FYP-file/wandb/run-20241222_234751-fuxp8czd</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/doodleedaronnn03-multimedia-university/FYP%20-%20nnU-Net/runs/fuxp8czd' target=\"_blank\">flowing-dragon-9</a></strong> to <a href='https://wandb.ai/doodleedaronnn03-multimedia-university/FYP%20-%20nnU-Net' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/doodleedaronnn03-multimedia-university/FYP%20-%20nnU-Net' target=\"_blank\">https://wandb.ai/doodleedaronnn03-multimedia-university/FYP%20-%20nnU-Net</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/doodleedaronnn03-multimedia-university/FYP%20-%20nnU-Net/runs/fuxp8czd' target=\"_blank\">https://wandb.ai/doodleedaronnn03-multimedia-university/FYP%20-%20nnU-Net/runs/fuxp8czd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized Weights & Biases tracking\n"
     ]
    }
   ],
   "source": [
    "def init_wandb(project_name: str = \"FYP - nnU-Net\"):\n",
    "    \"\"\"Initialize Weights & Biases tracking\"\"\"\n",
    "    wandb.init(project=project_name)\n",
    "    print(\"Initialized Weights & Biases tracking\")\n",
    "\n",
    "init_wandb()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GzvEQKt6yEqB"
   },
   "source": [
    "## 1.6 Connect to Google Drive and Set Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2640,
     "status": "ok",
     "timestamp": 1734495300285,
     "user": {
      "displayName": "Yu Xuan Tang",
      "userId": "11252039200120625827"
     },
     "user_tz": -480
    },
    "id": "PzNg09UUvKFD",
    "outputId": "0acf410c-7143-438c-8662-e3b949b6918e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "Set nnUNet_raw to /content/drive/MyDrive/FYP/nnUNet_raw\n",
      "Set nnUNet_preprocessed to /content/drive/MyDrive/FYP/nnUNet_preprocessed\n",
      "Set nnUNet_results to /content/drive/MyDrive/FYP/nnUNet_results\n",
      "Set RAW_DATA_PATH to /content/drive/MyDrive/FYP/RawData\n",
      "nnUNet_raw: /content/drive/MyDrive/FYP/nnUNet_raw\n",
      "nnUNet_preprocessed: /content/drive/MyDrive/FYP/nnUNet_preprocessed\n",
      "nnUNet_results: /content/drive/MyDrive/FYP/nnUNet_results\n",
      "RAW_DATA_PATH: /content/drive/MyDrive/FYP/RawData\n"
     ]
    }
   ],
   "source": [
    "class EnvironmentSetup:\n",
    "    def __init__(self, base_path: str = \"home\\doodledaron\\FYP\\nnUnet\"):\n",
    "        # Initialize logging\n",
    "        self.logger = Logger(\n",
    "            log_dir=os.path.join(base_path, \"logs\"),\n",
    "            name=\"environment_setup\"\n",
    "        ).get_logger(__name__)\n",
    "\n",
    "        self.base_path = base_path\n",
    "        self.setup_paths()\n",
    "\n",
    "    def setup_paths(self):\n",
    "        \"\"\"Setup nnU-Net environment paths with logging\"\"\"\n",
    "        self.logger.info(\"Setting up nnU-Net environment paths...\")\n",
    "        \n",
    "        try:\n",
    "            paths = {\n",
    "                \"nnUNet_raw\": os.path.join(self.base_path, \"nnUNet_raw\"),\n",
    "                \"nnUNet_preprocessed\": os.path.join(self.base_path, \"nnUNet_preprocessed\"),\n",
    "                \"nnUNet_results\": os.path.join(self.base_path, \"nnUNet_results\"),\n",
    "                # \"RESULTS_FOLDER\": os.path.join(self.base_path, \"nnUNet_results\"),\n",
    "                \"RAW_DATA_PATH\": os.path.join(self.base_path, \"RawData\")\n",
    "            }\n",
    "\n",
    "            # Create directories\n",
    "            for key, path in paths.items():\n",
    "                os.makedirs(path, exist_ok=True)\n",
    "                self.logger.debug(f\"Created directory: {path}\")\n",
    "\n",
    "            # Set environment variables\n",
    "            for key, path in paths.items():\n",
    "                os.environ[key] = path\n",
    "                self.logger.info(f\"Set {key} to {path}\")\n",
    "\n",
    "            self.logger.info(\"Environment setup completed successfully\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Environment setup failed: {str(e)}\")\n",
    "            raise\n",
    "env = EnvironmentSetup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 544,
     "status": "ok",
     "timestamp": 1734495376126,
     "user": {
      "displayName": "Yu Xuan Tang",
      "userId": "11252039200120625827"
     },
     "user_tz": -480
    },
    "id": "a_K6Los9T834"
   },
   "outputs": [],
   "source": [
    "class ErrorHandler:\n",
    "    @staticmethod\n",
    "    def check_gpu_memory():\n",
    "        \"\"\"Check available GPU memory\"\"\"\n",
    "        if torch.cuda.is_available():\n",
    "            memory = torch.cuda.get_device_properties(0).total_memory\n",
    "            memory_allocated = torch.cuda.memory_allocated(0)\n",
    "            memory_free = memory - memory_allocated\n",
    "            print(f\"GPU Memory: Total={memory/1e9:.2f}GB, \"\n",
    "                  f\"Free={memory_free/1e9:.2f}GB\")\n",
    "            return memory_free\n",
    "        return 0\n",
    "\n",
    "    @staticmethod\n",
    "    def handle_training_error(error: Exception, checkpoint_path: str):\n",
    "        \"\"\"Handle training interruption\"\"\"\n",
    "        print(f\"Training interrupted: {str(error)}\")\n",
    "        if os.path.exists(checkpoint_path):\n",
    "            print(f\"Latest checkpoint saved at: {checkpoint_path}\")\n",
    "        raise error\n",
    "\n",
    "# Create instance\n",
    "error_handler = ErrorHandler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eeLsC4AlVhXe"
   },
   "source": [
    "# Initialize Training Checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 331,
     "status": "ok",
     "timestamp": 1734495632945,
     "user": {
      "displayName": "Yu Xuan Tang",
      "userId": "11252039200120625827"
     },
     "user_tz": -480
    },
    "id": "c0T8wYhFVgKf"
   },
   "outputs": [],
   "source": [
    "class TrainingTracker:\n",
    "    def __init__(self, dataset_id: int = 1):\n",
    "        \"\"\"Initialize training tracker\"\"\"\n",
    "        self.dataset_id = dataset_id\n",
    "        self.results_path = os.path.join(os.environ[\"nnUNet_results\"],\n",
    "                                       f\"Dataset{self.dataset_id:03d}_Lung\")\n",
    "        self.excel_path = os.path.join(self.results_path, \"training_results.xlsx\")\n",
    "\n",
    "        # Create DataFrame to store results\n",
    "        self.columns = [\n",
    "            'date', 'configuration', 'fold', 'epochs',\n",
    "            'train_loss', 'val_loss', 'dice_score', 'hd95',\n",
    "            'training_time', 'gpu_memory_used', 'notes'\n",
    "        ]\n",
    "        self.results_df = pd.DataFrame(columns=self.columns)\n",
    "\n",
    "        # Load existing results if file exists\n",
    "        if os.path.exists(self.excel_path):\n",
    "            try:\n",
    "                self.results_df = pd.read_excel(self.excel_path)\n",
    "                print(\"Loaded existing training results\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading existing results: {str(e)}\")\n",
    "\n",
    "    def add_result(self, config: str, fold: int, metrics: Dict, notes: str = \"\"):\n",
    "        \"\"\"Add a training result\"\"\"\n",
    "        try:\n",
    "            # Get training time from log file\n",
    "            log_path = os.path.join(self.results_path,\n",
    "                                  f\"nnUNetTrainer__{config}/{fold}/progress.txt\")\n",
    "            training_time = self._parse_training_time(log_path)\n",
    "\n",
    "            # Get GPU memory usage\n",
    "            gpu_memory = torch.cuda.max_memory_allocated() / 1e9 if torch.cuda.is_available() else 0\n",
    "\n",
    "            # Create new result row\n",
    "            new_result = {\n",
    "                'date': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                'configuration': config,\n",
    "                'fold': fold,\n",
    "                'epochs': self._get_epochs(config, fold),\n",
    "                'train_loss': metrics.get('train_loss', float('nan')),\n",
    "                'val_loss': metrics.get('val_loss', float('nan')),\n",
    "                'dice_score': metrics.get('dice', float('nan')),\n",
    "                'hd95': metrics.get('hd95', float('nan')),\n",
    "                'training_time': training_time,\n",
    "                'gpu_memory_used': f\"{gpu_memory:.2f} GB\",\n",
    "                'notes': notes\n",
    "            }\n",
    "\n",
    "            # Add to DataFrame\n",
    "            self.results_df = pd.concat([self.results_df,\n",
    "                                       pd.DataFrame([new_result])],\n",
    "                                      ignore_index=True)\n",
    "\n",
    "            # Save to Excel\n",
    "            self.save_results()\n",
    "            print(f\"Added results for {config} fold {fold}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error adding result: {str(e)}\")\n",
    "\n",
    "    def save_results(self):\n",
    "        \"\"\"Save results to Excel file\"\"\"\n",
    "        try:\n",
    "            # Create summary sheet with aggregated metrics\n",
    "            with pd.ExcelWriter(self.excel_path, engine='xlsxwriter') as writer:\n",
    "                # Save detailed results\n",
    "                self.results_df.to_excel(writer, sheet_name='Detailed Results', index=False)\n",
    "\n",
    "                # Create summary by configuration\n",
    "                summary = self.results_df.groupby('configuration').agg({\n",
    "                    'dice_score': ['mean', 'std', 'min', 'max'],\n",
    "                    'hd95': ['mean', 'std', 'min', 'max'],\n",
    "                    'training_time': 'mean'\n",
    "                }).round(4)\n",
    "                summary.to_excel(writer, sheet_name='Summary by Configuration')\n",
    "\n",
    "                # Add formatting\n",
    "                workbook = writer.book\n",
    "                header_format = workbook.add_format({'bold': True, 'bg_color': '#D9D9D9'})\n",
    "\n",
    "                for sheet in writer.sheets.values():\n",
    "                    for col, width in enumerate(self.get_column_widths(sheet)):\n",
    "                        sheet.set_column(col, col, width)\n",
    "                    sheet.set_row(0, None, header_format)\n",
    "\n",
    "            print(f\"Results saved to {self.excel_path}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving results: {str(e)}\")\n",
    "\n",
    "    @staticmethod\n",
    "    def _parse_training_time(log_path: str) -> str:\n",
    "        \"\"\"Parse training time from progress.txt\"\"\"\n",
    "        if not os.path.exists(log_path):\n",
    "            return \"N/A\"\n",
    "\n",
    "        try:\n",
    "            with open(log_path, 'r') as f:\n",
    "                lines = f.readlines()\n",
    "                for line in reversed(lines):\n",
    "                    if \"training done\" in line.lower():\n",
    "                        # Extract time from line\n",
    "                        time_str = line.split(\"training done,\")[1].strip()\n",
    "                        return time_str\n",
    "        except Exception:\n",
    "            pass\n",
    "        return \"N/A\"\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_epochs(config: str, fold: int) -> int:\n",
    "        \"\"\"Get number of epochs trained\"\"\"\n",
    "        try:\n",
    "            checkpoint = torch.load(f\"nnUNetTrainer__{config}/{fold}/checkpoint_final.pth\")\n",
    "            return checkpoint.get('epoch', 0)\n",
    "        except:\n",
    "            return 0\n",
    "\n",
    "    @staticmethod\n",
    "    def get_column_widths(sheet) -> List[int]:\n",
    "        \"\"\"Get appropriate column widths based on content\"\"\"\n",
    "        return [max([len(str(cell)) for cell in column]) + 2\n",
    "                for column in zip(*sheet)]\n",
    "\n",
    "# # Example usage:\n",
    "# tracker = TrainingTracker()\n",
    "\n",
    "# # After training a configuration:\n",
    "# metrics = {\n",
    "#     'train_loss': 0.245,\n",
    "#     'val_loss': 0.189,\n",
    "#     'dice': 0.934,\n",
    "#     'hd95': 4.56\n",
    "# }\n",
    "# tracker.add_result('3d_fullres', 0, metrics, notes=\"Initial training run\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EIiFcDQHGhVL"
   },
   "source": [
    "# 2.Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1owWZTgcG6s4"
   },
   "source": [
    "## 2.1 Dataset Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 756
    },
    "executionInfo": {
     "elapsed": 337087,
     "status": "ok",
     "timestamp": 1734443598114,
     "user": {
      "displayName": "Yu Xuan Tang",
      "userId": "11252039200120625827"
     },
     "user_tz": -480
    },
    "id": "1SQ71LlaG_AO",
    "outputId": "73bed25c-0319-4b85-f690-1dc502912f6d"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#analyze Raw Dataset (Task001) in RawData, havent preprocess\n",
    "class DatasetAnalyzer:\n",
    "    def __init__(self, dataset_id: int = 1):\n",
    "        self.dataset_id = dataset_id\n",
    "        self.logger = Logger(\n",
    "            log_dir=os.path.join(os.environ[\"nnUNet_results\"], \"logs\"),\n",
    "            name=\"dataset_analyzer\"\n",
    "        ).get_logger(__name__)        \n",
    "        self.dataset_path = os.path.join(os.environ[\"RawData\"], f\"Task001_Lung\")\n",
    "\n",
    "    def load_dataset_json(self) -> dict:\n",
    "        \"\"\"Load and return dataset.json\"\"\"\n",
    "        json_path = os.path.join(self.dataset_path, \"dataset.json\")\n",
    "        with open(json_path, 'r') as f:\n",
    "            return json.load(f)\n",
    "\n",
    "    def get_dataset_stats(self) -> Dict:\n",
    "        \"\"\"Calculate basic dataset statistics\"\"\"\n",
    "        stats = {\n",
    "            \"num_training\": 0,\n",
    "            \"image_sizes\": [],\n",
    "            \"spacings\": []\n",
    "        }\n",
    "\n",
    "        images_path = os.path.join(self.dataset_path, \"imagesTr\")\n",
    "        for file in os.listdir(images_path):\n",
    "            if file.endswith(\"_0000.nii.gz\"):  # First channel\n",
    "                img = nib.load(os.path.join(images_path, file))\n",
    "                stats[\"num_training\"] += 1\n",
    "                stats[\"image_sizes\"].append(img.shape)\n",
    "                stats[\"spacings\"].append(img.header.get_zooms())\n",
    "        print(f\"Number of training images: {stats['num_training']}\")\n",
    "        print(f\"Image sizes: {stats['image_sizes']}\")\n",
    "        print(f\"Spacings: {stats['spacings']}\")\n",
    "        return stats\n",
    "\n",
    "    def plot_size_distribution(self):\n",
    "        \"\"\"Plot distribution of image sizes\"\"\"\n",
    "        stats = self.get_dataset_stats()\n",
    "        sizes = np.array(stats[\"image_sizes\"])\n",
    "\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "        for i, dim in enumerate(['X', 'Y', 'Z']):\n",
    "            sns.histplot(sizes[:, i], ax=axes[i])\n",
    "            axes[i].set_title(f'{dim} Dimension Distribution')\n",
    "            axes[i].set_xlabel('Size (voxels)')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def analyze_class_distribution(self):\n",
    "        \"\"\"Analyze distribution of segmentation classes\"\"\"\n",
    "        labels_path = os.path.join(self.dataset_path, \"labelsTr\")\n",
    "        class_pixels = {}\n",
    "\n",
    "        for file in os.listdir(labels_path):\n",
    "            if file.endswith(\".nii.gz\"):\n",
    "                img = nib.load(os.path.join(labels_path, file))\n",
    "                data = img.get_fdata()\n",
    "                unique, counts = np.unique(data, return_counts=True)\n",
    "\n",
    "                for u, c in zip(unique, counts):\n",
    "                    if u not in class_pixels:\n",
    "                        class_pixels[u] = []\n",
    "                    class_pixels[u].append(c)\n",
    "\n",
    "        # Plot class distribution\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        boxplot_data = [np.array(pixels) for pixels in class_pixels.values()]\n",
    "        plt.boxplot(boxplot_data, labels=[f\"Class {int(k)}\" for k in class_pixels.keys()])\n",
    "        plt.title(\"Class Distribution\")\n",
    "        plt.ylabel(\"Number of Voxels\")\n",
    "        plt.yscale('log')\n",
    "        plt.show()\n",
    "\n",
    "    def validate_dataset_format(self):\n",
    "        \"\"\"Validate dataset format compliance\"\"\"\n",
    "        required_folders = ['imagesTr', 'labelsTr']\n",
    "        required_files = ['dataset.json']\n",
    "\n",
    "        # Check folders\n",
    "        for folder in required_folders:\n",
    "            folder_path = os.path.join(self.dataset_path, folder)\n",
    "            if not os.path.exists(folder_path):\n",
    "                raise FileNotFoundError(f\"Missing required folder: {folder}\")\n",
    "\n",
    "        # Check files\n",
    "        for file in required_files:\n",
    "            file_path = os.path.join(self.dataset_path, file)\n",
    "            if not os.path.exists(file_path):\n",
    "                raise FileNotFoundError(f\"Missing required file: {file}\")\n",
    "\n",
    "        print(\"Dataset format validation completed successfully\")\n",
    "\n",
    "    def visualize_sample_cases(self, num_samples: int = 3):\n",
    "        \"\"\"Visualize sample slices from the dataset\"\"\"\n",
    "        images_path = os.path.join(self.dataset_path, \"imagesTr\")\n",
    "        labels_path = os.path.join(self.dataset_path, \"labelsTr\")\n",
    "\n",
    "        # Get list of image files\n",
    "        image_files = [f for f in os.listdir(images_path) if f.endswith('_0000.nii.gz')]\n",
    "        samples = np.random.choice(image_files, min(num_samples, len(image_files)), replace=False)\n",
    "\n",
    "        for image_file in samples:\n",
    "            # Load image and corresponding label\n",
    "            image = nib.load(os.path.join(images_path, image_file)).get_fdata()\n",
    "            label = nib.load(os.path.join(labels_path, image_file.replace('_0000', ''))).get_fdata()\n",
    "\n",
    "            # Get middle slice\n",
    "            slice_idx = image.shape[0] // 2\n",
    "\n",
    "            # Plot\n",
    "            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "            ax1.imshow(image[slice_idx], cmap='gray')\n",
    "            ax1.set_title('Image')\n",
    "            ax2.imshow(image[slice_idx], cmap='gray')\n",
    "            ax2.imshow(label[slice_idx], alpha=0.3, cmap='red')\n",
    "            ax2.set_title('Label Overlay')\n",
    "            plt.suptitle(f'Sample Case: {image_file}')\n",
    "            plt.show()\n",
    "\n",
    "analyzer = DatasetAnalyzer()\n",
    "analyzer.plot_size_distribution()\n",
    "analyzer.analyze_class_distribution()\n",
    "\n",
    "\n",
    "#All images have the same size of 512 voxels (pixels) in both X and Y dimensions\n",
    "#Z dimension shows more variation: ranging from ~100 to 600 voxels\n",
    "\n",
    "\n",
    "#Class 0 (Background):Around 10⁸ voxels (100 million) - Very consistent across images (small box plot range)\n",
    "#Class 1 (Lung lesions/cancer): Around 10³-10⁴ voxels (1,000-10,000) - More variation between images (larger box plot range)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sbvzsDWQHGhQ"
   },
   "source": [
    "# 3.Preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "seaWhwbFHNPB"
   },
   "source": [
    "## 3.1 Run nnUnet Preprocessing\n",
    "The Decathlon datasets are 4D nifti files, for nnU-Net they have to be **converted to 3D nifti** files.\n",
    "\n",
    "For more information about dataset conversion see: [nnU-Net Dataset Formatting Instructions](https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/dataset_format.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "executionInfo": {
     "elapsed": 371,
     "status": "ok",
     "timestamp": 1734495869169,
     "user": {
      "displayName": "Yu Xuan Tang",
      "userId": "11252039200120625827"
     },
     "user_tz": -480
    },
    "id": "mY3HHLuYWO8t"
   },
   "outputs": [],
   "source": [
    "assert os.path.exists(os.path.join(os.environ[\"RAW_DATA_PATH\"], \"Task001_Lung\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 426
    },
    "executionInfo": {
     "elapsed": 751644,
     "status": "error",
     "timestamp": 1734498555530,
     "user": {
      "displayName": "Yu Xuan Tang",
      "userId": "11252039200120625827"
     },
     "user_tz": -480
    },
    "id": "P81DBxesHWdR",
    "outputId": "127d9a65-3565-4750-8685-66f2cf35c75b"
   },
   "outputs": [],
   "source": [
    "def run_preprocessing(dataset_id: int = 1):\n",
    "    \"\"\"Run nnU-Net preprocessing\"\"\"\n",
    "    # Initialize logger\n",
    "    logger = Logger(\n",
    "        log_dir=os.path.join(os.environ[\"nnUNet_results\"], \"logs\"),\n",
    "        name=\"preprocessing\"\n",
    "    ).get_logger(__name__)\n",
    "    \n",
    "    logger.info(\"Starting preprocessing...\")\n",
    "\n",
    "    # Print raw data path for verification \n",
    "    raw_path = os.path.join(os.environ[\"RAW_DATA_PATH\"], \"Task001_Lung\")\n",
    "    logger.info(f\"Raw data path: {raw_path}\")\n",
    "\n",
    "    try:\n",
    "        # Check GPU memory before starting\n",
    "        if torch.cuda.is_available():\n",
    "            available_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "            free_memory = available_memory - (torch.cuda.memory_allocated(0) / 1e9)\n",
    "            logger.info(f\"Available GPU memory: {free_memory:.2f} GB\")\n",
    "            \n",
    "            if free_memory < 2:  # Less than 2GB free\n",
    "                logger.warning(\"Low GPU memory available\")\n",
    "        #add convert dataset if needed\n",
    "\n",
    "\n",
    "        # Run preprocessing\n",
    "        preprocess_command = f'nnUNetv2_plan_and_preprocess -d {dataset_id} -np 2 --verify_dataset_integrity'\n",
    "        logger.info(f\"Running preprocessing command: {preprocess_command}\")\n",
    "        result = os.system(preprocess_command)\n",
    "        if result != 0:\n",
    "            raise RuntimeError(f\"Preprocessing failed with exit code {result}\")\n",
    "        logger.info(\"Preprocessing completed successfully\")\n",
    "\n",
    "        # Verify files were created\n",
    "        preprocessed_path = os.path.join(\n",
    "            os.environ[\"nnUNet_preprocessed\"],\n",
    "            f\"Dataset{dataset_id:03d}_Lung\"\n",
    "        )\n",
    "        logger.info(f\"Checking preprocessed path: {preprocessed_path}\")\n",
    "\n",
    "        required_files = [\"dataset.json\", \"dataset_fingerprint.json\", \"nnUNetPlans.json\"]\n",
    "        missing_files = []\n",
    "        for file in required_files:\n",
    "            file_path = os.path.join(preprocessed_path, file)\n",
    "            if os.path.exists(file_path):\n",
    "                logger.info(f\"Found required file: {file}\")\n",
    "            else:\n",
    "                logger.warning(f\"Missing required file: {file}\")\n",
    "                missing_files.append(file)\n",
    "\n",
    "        if missing_files:\n",
    "            raise FileNotFoundError(f\"Missing required files: {', '.join(missing_files)}\")\n",
    "\n",
    "        logger.info(\"All required files found\")\n",
    "        logger.info(\"Preprocessing pipeline completed successfully\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during preprocessing: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Call the function\n",
    "run_preprocessing()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0sdHL0ukIZtk"
   },
   "source": [
    "## Verify Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 329
    },
    "executionInfo": {
     "elapsed": 523,
     "status": "error",
     "timestamp": 1734446307193,
     "user": {
      "displayName": "Yu Xuan Tang",
      "userId": "11252039200120625827"
     },
     "user_tz": -480
    },
    "id": "zyRpBZ4kIZIi",
    "outputId": "5064a5cc-aaa9-4d8a-e939-d6097242b8fc"
   },
   "outputs": [],
   "source": [
    "def verify_preprocessing(dataset_id: int = 1):\n",
    "    \"\"\"Verify preprocessing results\"\"\"\n",
    "    preprocessed_path = os.path.join(\n",
    "        os.environ[\"nnUNet_raw\"],\n",
    "        f\"Dataset{dataset_id:03d}_Lung\"  # This is correct for preprocessed folder\n",
    "    )\n",
    "\n",
    "    # Check existence of essential files\n",
    "    required_files = [\"dataset.json\", \"dataset_fingerprint.json\", \"nnUNetPlans.json\"]\n",
    "    for file in required_files:\n",
    "        path = os.path.join(preprocessed_path, file)\n",
    "        if not os.path.exists(path):\n",
    "            raise FileNotFoundError(f\"Missing required file: {path}\")\n",
    "\n",
    "\n",
    "verify_preprocessing()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P3pmfSHhTpH9"
   },
   "source": [
    "# 4. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Game plan:\n",
    "1. Train 3d_fullres for 1000 epochs (default) with 3 folds (less fold to save time) -> without npz flag\n",
    "note: first round is to testing round to see how it works\n",
    "\n",
    "next:\n",
    "## Standard Configurations\n",
    "- we add --npz flag so nnUnet can \n",
    "  - find the best configuration\n",
    "  - finding the best model ensemble -> \n",
    "  1. Test individual models (2d, 3d_fullres)\n",
    "  2. Test combinations/ensembles of these models (2d + 3d_fullres together)\n",
    "  3. Choose what works best - either a single model OR an ensemble of multiple models\n",
    "\n",
    "FOLD -> can be left empty, nnUnet trains with 5-fold cross-validation in default\n",
    "1 -> dataset id\n",
    "- nnUNetv2_train 1 2d FOLD -tr nnUNetTrainer_100epochs --npz\n",
    "- nnUNetv2_train 1 3d_fullres FOLD -tr nnUNetTrainer_100epochs --npz \n",
    "- nnUNetv2_train 1 3d_lowres FOLD nnUNetTrainer_100epochs --npz\n",
    "- nnUNetv2_train 1 3d_cascade_fullres FOLD nnUNetTrainer_100epochs --npz\n",
    "\n",
    "## ResEnc M Configuration\n",
    "- nnUNetv2_plan_experiment -d DATASET_ID -pl nnUNetPlannerResEncM\n",
    "- nnUNetv2_train DATASET_ID 3d_fullres 0,1,2,3,4 -p nnUNetResEncUNetMPlans -num_epochs 250 --npz\n",
    "\n",
    "## Find Best Configuration\n",
    "- nnUNetv2_find_best_configuration DATASET_ID -c 2d 3d_fullres 3d_lowres 3d_cascade_fullres 3d_fullres_resenc\n",
    "\n",
    "# Full Training (1000 epochs + 5 folds)\n",
    "- Train best performing model configuration with default settings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UUawRw1mJdXS"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dyTClqNziw5K"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting standard U-Net training: nnUNetv2_train 1 3d_fullres 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages/nnunetv2/training/nnUNetTrainer/nnUNetTrainer.py:164: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.grad_scaler = GradScaler() if self.device.type == 'cuda' else None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "############################\n",
      "INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md\n",
      "############################\n",
      "\n",
      "Using device: cuda:0\n",
      "\n",
      "#######################################################################\n",
      "Please cite the following paper when using nnU-Net:\n",
      "Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.\n",
      "#######################################################################\n",
      "\n",
      "2024-12-22 23:48:24.360729: do_dummy_2d_data_aug: False\n",
      "2024-12-22 23:48:24.361177: Using splits from existing split file: /home/doodledaron/FYP/nnUnet/FYP-file/nnUNet_preprocessed/Dataset001_Lung/splits_final.json\n",
      "2024-12-22 23:48:24.361762: The split file contains 5 splits.\n",
      "2024-12-22 23:48:24.361834: Desired fold for training: 0\n",
      "2024-12-22 23:48:24.361855: This split has 50 training and 13 validation cases.\n",
      "using pin_memory on device 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using pin_memory on device 0\n",
      "2024-12-22 23:48:28.434158: Using torch.compile...\n",
      "\n",
      "This is the configuration used by this training:\n",
      "Configuration name: 3d_fullres\n",
      " {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 2, 'patch_size': [80, 192, 160], 'median_image_size_in_voxels': [252.0, 512.0, 512.0], 'spacing': [1.244979977607727, 0.78515625, 0.78515625], 'normalization_schemes': ['CTNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True} \n",
      "\n",
      "These are the global plan.json settings:\n",
      " {'dataset_name': 'Dataset001_Lung', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.244979977607727, 0.78515625, 0.78515625], 'original_median_shape_after_transp': [252, 512, 512], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 2671.0, 'mean': -273.4598083496094, 'median': -162.0, 'min': -1024.0, 'percentile_00_5': -1024.0, 'percentile_99_5': 311.0, 'std': 346.9495849609375}}} \n",
      "\n",
      "2024-12-22 23:48:29.685398: unpacking dataset...\n",
      "2024-12-22 23:48:34.533162: unpacking done...\n",
      "2024-12-22 23:48:34.534297: Unable to plot network architecture: nnUNet_compile is enabled!\n",
      "2024-12-22 23:48:34.541315: \n",
      "2024-12-22 23:48:34.541383: Epoch 0\n",
      "2024-12-22 23:48:34.541459: Current learning rate: 0.01\n",
      "2024-12-22 23:51:15.377414: train_loss 0.0095\n",
      "2024-12-22 23:51:15.378020: val_loss -0.2249\n",
      "2024-12-22 23:51:15.378094: Pseudo dice [np.float32(0.0)]\n",
      "2024-12-22 23:51:15.378140: Epoch time: 160.84 s\n",
      "2024-12-22 23:51:15.378167: Yayy! New best EMA pseudo Dice: 0.0\n",
      "2024-12-22 23:51:16.620082: \n",
      "2024-12-22 23:51:16.620386: Epoch 1\n",
      "2024-12-22 23:51:16.620510: Current learning rate: 0.00999\n",
      "2024-12-22 23:53:08.288832: train_loss -0.2425\n",
      "2024-12-22 23:53:08.289599: val_loss -0.4553\n",
      "2024-12-22 23:53:08.289673: Pseudo dice [np.float32(0.5466)]\n",
      "2024-12-22 23:53:08.289725: Epoch time: 111.67 s\n",
      "2024-12-22 23:53:08.289750: Yayy! New best EMA pseudo Dice: 0.05469999834895134\n",
      "2024-12-22 23:53:09.138566: \n",
      "2024-12-22 23:53:09.138768: Epoch 2\n",
      "2024-12-22 23:53:09.138854: Current learning rate: 0.00998\n",
      "2024-12-22 23:55:00.904072: train_loss -0.3295\n",
      "2024-12-22 23:55:00.904380: val_loss -0.5347\n",
      "2024-12-22 23:55:00.904418: Pseudo dice [np.float32(0.5658)]\n",
      "2024-12-22 23:55:00.904470: Epoch time: 111.77 s\n",
      "2024-12-22 23:55:00.904501: Yayy! New best EMA pseudo Dice: 0.10580000281333923\n",
      "2024-12-22 23:55:01.752318: \n",
      "2024-12-22 23:55:01.752415: Epoch 3\n",
      "2024-12-22 23:55:01.752491: Current learning rate: 0.00997\n",
      "2024-12-22 23:56:53.423723: train_loss -0.4057\n",
      "2024-12-22 23:56:53.424177: val_loss -0.4961\n",
      "2024-12-22 23:56:53.424244: Pseudo dice [np.float32(0.3654)]\n",
      "2024-12-22 23:56:53.424288: Epoch time: 111.67 s\n",
      "2024-12-22 23:56:53.424313: Yayy! New best EMA pseudo Dice: 0.13169999420642853\n",
      "2024-12-22 23:56:54.231594: \n",
      "2024-12-22 23:56:54.231802: Epoch 4\n",
      "2024-12-22 23:56:54.231913: Current learning rate: 0.00996\n",
      "2024-12-22 23:58:45.819718: train_loss -0.4372\n",
      "2024-12-22 23:58:45.819961: val_loss -0.5379\n",
      "2024-12-22 23:58:45.820001: Pseudo dice [np.float32(0.5486)]\n",
      "2024-12-22 23:58:45.820041: Epoch time: 111.59 s\n",
      "2024-12-22 23:58:45.820063: Yayy! New best EMA pseudo Dice: 0.17339999973773956\n",
      "2024-12-22 23:58:46.611378: \n",
      "2024-12-22 23:58:46.611542: Epoch 5\n",
      "2024-12-22 23:58:46.611618: Current learning rate: 0.00995\n",
      "2024-12-23 00:00:37.943644: train_loss -0.4239\n",
      "2024-12-23 00:00:37.944046: val_loss -0.5469\n",
      "2024-12-23 00:00:37.944135: Pseudo dice [np.float32(0.4904)]\n",
      "2024-12-23 00:00:37.944182: Epoch time: 111.33 s\n",
      "2024-12-23 00:00:37.944205: Yayy! New best EMA pseudo Dice: 0.20509999990463257\n",
      "2024-12-23 00:00:38.700763: \n",
      "2024-12-23 00:00:38.700935: Epoch 6\n",
      "2024-12-23 00:00:38.701007: Current learning rate: 0.00995\n",
      "2024-12-23 00:02:30.038050: train_loss -0.4324\n",
      "2024-12-23 00:02:30.038188: val_loss -0.5727\n",
      "2024-12-23 00:02:30.038219: Pseudo dice [np.float32(0.5777)]\n",
      "2024-12-23 00:02:30.038254: Epoch time: 111.34 s\n",
      "2024-12-23 00:02:30.038277: Yayy! New best EMA pseudo Dice: 0.24240000545978546\n",
      "2024-12-23 00:02:30.878271: \n",
      "2024-12-23 00:02:30.878371: Epoch 7\n",
      "2024-12-23 00:02:30.878443: Current learning rate: 0.00994\n",
      "2024-12-23 00:04:22.257061: train_loss -0.4852\n",
      "2024-12-23 00:04:22.257326: val_loss -0.5767\n",
      "2024-12-23 00:04:22.257391: Pseudo dice [np.float32(0.5911)]\n",
      "2024-12-23 00:04:22.257437: Epoch time: 111.38 s\n",
      "2024-12-23 00:04:22.257460: Yayy! New best EMA pseudo Dice: 0.27720001339912415\n",
      "2024-12-23 00:04:23.048178: \n",
      "2024-12-23 00:04:23.048280: Epoch 8\n",
      "2024-12-23 00:04:23.048354: Current learning rate: 0.00993\n",
      "2024-12-23 00:06:14.450857: train_loss -0.4994\n",
      "2024-12-23 00:06:14.453794: val_loss -0.5923\n",
      "2024-12-23 00:06:14.453916: Pseudo dice [np.float32(0.6158)]\n",
      "2024-12-23 00:06:14.453969: Epoch time: 111.4 s\n",
      "2024-12-23 00:06:14.453992: Yayy! New best EMA pseudo Dice: 0.3111000061035156\n",
      "2024-12-23 00:06:15.252765: \n",
      "2024-12-23 00:06:15.252943: Epoch 9\n",
      "2024-12-23 00:06:15.253026: Current learning rate: 0.00992\n",
      "2024-12-23 00:08:06.690935: train_loss -0.4533\n",
      "2024-12-23 00:08:06.691091: val_loss -0.5287\n",
      "2024-12-23 00:08:06.691127: Pseudo dice [np.float32(0.5301)]\n",
      "2024-12-23 00:08:06.691165: Epoch time: 111.44 s\n",
      "2024-12-23 00:08:06.691187: Yayy! New best EMA pseudo Dice: 0.3330000042915344\n",
      "2024-12-23 00:08:07.465569: \n",
      "2024-12-23 00:08:07.465869: Epoch 10\n",
      "2024-12-23 00:08:07.466114: Current learning rate: 0.00991\n",
      "2024-12-23 00:09:58.871680: train_loss -0.4692\n",
      "2024-12-23 00:09:58.871883: val_loss -0.6333\n",
      "2024-12-23 00:09:58.871928: Pseudo dice [np.float32(0.68)]\n",
      "2024-12-23 00:09:58.871965: Epoch time: 111.41 s\n",
      "2024-12-23 00:09:58.871986: Yayy! New best EMA pseudo Dice: 0.3677000105381012\n",
      "2024-12-23 00:09:59.637576: \n",
      "2024-12-23 00:09:59.637909: Epoch 11\n",
      "2024-12-23 00:09:59.638160: Current learning rate: 0.0099\n",
      "2024-12-23 00:11:51.055575: train_loss -0.468\n",
      "2024-12-23 00:11:51.055985: val_loss -0.6048\n",
      "2024-12-23 00:11:51.056064: Pseudo dice [np.float32(0.5912)]\n",
      "2024-12-23 00:11:51.056117: Epoch time: 111.42 s\n",
      "2024-12-23 00:11:51.056141: Yayy! New best EMA pseudo Dice: 0.38999998569488525\n",
      "2024-12-23 00:11:51.831939: \n",
      "2024-12-23 00:11:51.832040: Epoch 12\n",
      "2024-12-23 00:11:51.832114: Current learning rate: 0.00989\n",
      "2024-12-23 00:13:43.187590: train_loss -0.5438\n",
      "2024-12-23 00:13:43.187736: val_loss -0.6348\n",
      "2024-12-23 00:13:43.187768: Pseudo dice [np.float32(0.704)]\n",
      "2024-12-23 00:13:43.187803: Epoch time: 111.36 s\n",
      "2024-12-23 00:13:43.187823: Yayy! New best EMA pseudo Dice: 0.4214000105857849\n",
      "2024-12-23 00:13:45.924962: \n",
      "2024-12-23 00:13:45.925140: Epoch 13\n",
      "2024-12-23 00:13:45.925311: Current learning rate: 0.00988\n",
      "2024-12-23 00:15:37.622016: train_loss -0.5369\n",
      "2024-12-23 00:15:37.622154: val_loss -0.6174\n",
      "2024-12-23 00:15:37.622336: Pseudo dice [np.float32(0.6826)]\n",
      "2024-12-23 00:15:37.622431: Epoch time: 111.7 s\n",
      "2024-12-23 00:15:37.622466: Yayy! New best EMA pseudo Dice: 0.44760000705718994\n",
      "2024-12-23 00:15:38.360302: \n",
      "2024-12-23 00:15:38.360616: Epoch 14\n",
      "2024-12-23 00:15:38.360728: Current learning rate: 0.00987\n",
      "2024-12-23 00:17:29.714111: train_loss -0.5148\n",
      "2024-12-23 00:17:29.714301: val_loss -0.6173\n",
      "2024-12-23 00:17:29.714337: Pseudo dice [np.float32(0.6492)]\n",
      "2024-12-23 00:17:29.714372: Epoch time: 111.35 s\n",
      "2024-12-23 00:17:29.714393: Yayy! New best EMA pseudo Dice: 0.4677000045776367\n",
      "2024-12-23 00:17:30.472933: \n",
      "2024-12-23 00:17:30.473099: Epoch 15\n",
      "2024-12-23 00:17:30.473180: Current learning rate: 0.00986\n",
      "2024-12-23 00:19:21.770234: train_loss -0.5727\n",
      "2024-12-23 00:19:21.770400: val_loss -0.5901\n",
      "2024-12-23 00:19:21.770438: Pseudo dice [np.float32(0.6462)]\n",
      "2024-12-23 00:19:21.770473: Epoch time: 111.3 s\n",
      "2024-12-23 00:19:21.770493: Yayy! New best EMA pseudo Dice: 0.48559999465942383\n",
      "2024-12-23 00:19:22.533331: \n",
      "2024-12-23 00:19:22.533492: Epoch 16\n",
      "2024-12-23 00:19:22.533571: Current learning rate: 0.00986\n",
      "2024-12-23 00:21:13.927179: train_loss -0.5324\n",
      "2024-12-23 00:21:13.927381: val_loss -0.6051\n",
      "2024-12-23 00:21:13.927420: Pseudo dice [np.float32(0.6845)]\n",
      "2024-12-23 00:21:13.927456: Epoch time: 111.39 s\n",
      "2024-12-23 00:21:13.927476: Yayy! New best EMA pseudo Dice: 0.5055000185966492\n",
      "2024-12-23 00:21:14.709122: \n",
      "2024-12-23 00:21:14.709471: Epoch 17\n",
      "2024-12-23 00:21:14.709614: Current learning rate: 0.00985\n",
      "2024-12-23 00:23:06.945481: train_loss -0.523\n",
      "2024-12-23 00:23:06.945683: val_loss -0.6321\n",
      "2024-12-23 00:23:06.945720: Pseudo dice [np.float32(0.6749)]\n",
      "2024-12-23 00:23:06.945756: Epoch time: 112.24 s\n",
      "2024-12-23 00:23:06.945777: Yayy! New best EMA pseudo Dice: 0.5224000215530396\n",
      "2024-12-23 00:23:07.726222: \n",
      "2024-12-23 00:23:07.726394: Epoch 18\n",
      "2024-12-23 00:23:07.726476: Current learning rate: 0.00984\n",
      "2024-12-23 00:24:59.142023: train_loss -0.4929\n",
      "2024-12-23 00:24:59.142219: val_loss -0.5385\n",
      "2024-12-23 00:24:59.142257: Pseudo dice [np.float32(0.6521)]\n",
      "2024-12-23 00:24:59.142292: Epoch time: 111.42 s\n",
      "2024-12-23 00:24:59.142332: Yayy! New best EMA pseudo Dice: 0.5353999733924866\n",
      "2024-12-23 00:24:59.918192: \n",
      "2024-12-23 00:24:59.918283: Epoch 19\n",
      "2024-12-23 00:24:59.918353: Current learning rate: 0.00983\n",
      "2024-12-23 00:26:51.824436: train_loss -0.5232\n",
      "2024-12-23 00:26:51.824975: val_loss -0.6519\n",
      "2024-12-23 00:26:51.825147: Pseudo dice [np.float32(0.7406)]\n",
      "2024-12-23 00:26:51.825224: Epoch time: 111.91 s\n",
      "Yayy! New best EMA pseudo Dice: 0.555899977684021\n",
      "2024-12-23 00:26:52.608348: \n",
      "2024-12-23 00:26:52.608449: Epoch 20\n",
      "2024-12-23 00:26:52.608521: Current learning rate: 0.00982\n",
      "2024-12-23 00:28:44.182097: train_loss -0.5336\n",
      "2024-12-23 00:28:44.182246: val_loss -0.5822\n",
      "2024-12-23 00:28:44.182285: Pseudo dice [np.float32(0.6167)]\n",
      "2024-12-23 00:28:44.182325: Epoch time: 111.57 s\n",
      "2024-12-23 00:28:44.182347: Yayy! New best EMA pseudo Dice: 0.5619999766349792\n",
      "2024-12-23 00:28:44.950000: \n",
      "2024-12-23 00:28:44.950187: Epoch 21\n",
      "2024-12-23 00:28:44.950265: Current learning rate: 0.00981\n",
      "2024-12-23 00:30:36.295590: train_loss -0.5586\n",
      "2024-12-23 00:30:36.295748: val_loss -0.5844\n",
      "2024-12-23 00:30:36.295815: Pseudo dice [np.float32(0.6142)]\n",
      "2024-12-23 00:30:36.295939: Epoch time: 111.35 s\n",
      "2024-12-23 00:30:36.295992: Yayy! New best EMA pseudo Dice: 0.5672000050544739\n",
      "2024-12-23 00:30:37.090699: \n",
      "2024-12-23 00:30:37.090862: Epoch 22\n",
      "2024-12-23 00:30:37.090942: Current learning rate: 0.0098\n",
      "2024-12-23 00:32:29.399550: train_loss -0.5748\n",
      "2024-12-23 00:32:29.399731: val_loss -0.6007\n",
      "2024-12-23 00:32:29.399802: Pseudo dice [np.float32(0.6489)]\n",
      "2024-12-23 00:32:29.399848: Epoch time: 112.31 s\n",
      "2024-12-23 00:32:29.399872: Yayy! New best EMA pseudo Dice: 0.5753999948501587\n",
      "2024-12-23 00:32:30.146410: \n",
      "2024-12-23 00:32:30.146604: Epoch 23\n",
      "2024-12-23 00:32:30.146682: Current learning rate: 0.00979\n",
      "2024-12-23 00:34:21.706747: train_loss -0.5681\n",
      "2024-12-23 00:34:21.706930: val_loss -0.6069\n",
      "2024-12-23 00:34:21.706971: Pseudo dice [np.float32(0.6583)]\n",
      "2024-12-23 00:34:21.707006: Epoch time: 111.56 s\n",
      "2024-12-23 00:34:21.707026: Yayy! New best EMA pseudo Dice: 0.5837000012397766\n",
      "2024-12-23 00:34:22.466937: \n",
      "2024-12-23 00:34:22.467103: Epoch 24\n",
      "2024-12-23 00:34:22.467175: Current learning rate: 0.00978\n",
      "2024-12-23 00:36:13.764798: train_loss -0.599\n",
      "2024-12-23 00:36:13.764958: val_loss -0.6884\n",
      "2024-12-23 00:36:13.764994: Pseudo dice [np.float32(0.7752)]\n",
      "2024-12-23 00:36:13.765031: Epoch time: 111.3 s\n",
      "2024-12-23 00:36:13.765052: Yayy! New best EMA pseudo Dice: 0.6028000116348267\n",
      "2024-12-23 00:36:14.519457: \n",
      "2024-12-23 00:36:14.519884: Epoch 25\n",
      "2024-12-23 00:36:14.520119: Current learning rate: 0.00977\n",
      "2024-12-23 00:38:06.210206: train_loss -0.591\n",
      "2024-12-23 00:38:06.210437: val_loss -0.5996\n",
      "2024-12-23 00:38:06.210475: Pseudo dice [np.float32(0.6434)]\n",
      "2024-12-23 00:38:06.210512: Epoch time: 111.69 s\n",
      "2024-12-23 00:38:06.210534: Yayy! New best EMA pseudo Dice: 0.6068999767303467\n",
      "2024-12-23 00:38:06.974117: \n",
      "2024-12-23 00:38:06.974294: Epoch 26\n",
      "2024-12-23 00:38:06.974365: Current learning rate: 0.00977\n",
      "2024-12-23 00:39:58.306273: train_loss -0.5705\n",
      "2024-12-23 00:39:58.306486: val_loss -0.6665\n",
      "2024-12-23 00:39:58.306530: Pseudo dice [np.float32(0.6917)]\n",
      "2024-12-23 00:39:58.306568: Epoch time: 111.33 s\n",
      "2024-12-23 00:39:58.306588: Yayy! New best EMA pseudo Dice: 0.6152999997138977\n",
      "2024-12-23 00:39:59.059974: \n",
      "2024-12-23 00:39:59.060273: Epoch 27\n",
      "2024-12-23 00:39:59.060360: Current learning rate: 0.00976\n",
      "2024-12-23 00:41:50.405890: train_loss -0.5682\n",
      "2024-12-23 00:41:50.406050: val_loss -0.6863\n",
      "2024-12-23 00:41:50.406086: Pseudo dice [np.float32(0.6978)]\n",
      "2024-12-23 00:41:50.406152: Epoch time: 111.35 s\n",
      "2024-12-23 00:41:50.406181: Yayy! New best EMA pseudo Dice: 0.6236000061035156\n",
      "2024-12-23 00:41:51.186067: \n",
      "2024-12-23 00:41:51.186271: Epoch 28\n",
      "2024-12-23 00:41:51.186361: Current learning rate: 0.00975\n",
      "2024-12-23 00:43:42.555454: train_loss -0.5958\n",
      "2024-12-23 00:43:42.555618: val_loss -0.5687\n",
      "2024-12-23 00:43:42.555654: Pseudo dice [np.float32(0.6256)]\n",
      "2024-12-23 00:43:42.555692: Epoch time: 111.37 s\n",
      "2024-12-23 00:43:42.555715: Yayy! New best EMA pseudo Dice: 0.6237999796867371\n",
      "2024-12-23 00:43:43.312482: \n",
      "2024-12-23 00:43:43.312643: Epoch 29\n",
      "2024-12-23 00:43:43.312716: Current learning rate: 0.00974\n",
      "2024-12-23 00:45:34.675826: train_loss -0.6003\n",
      "2024-12-23 00:45:34.676285: val_loss -0.6016\n",
      "2024-12-23 00:45:34.676437: Pseudo dice [np.float32(0.6563)]\n",
      "2024-12-23 00:45:34.676485: Epoch time: 111.36 s\n",
      "2024-12-23 00:45:34.676509: Yayy! New best EMA pseudo Dice: 0.6269999742507935\n",
      "2024-12-23 00:45:35.880887: \n",
      "2024-12-23 00:45:35.881013: Epoch 30\n",
      "2024-12-23 00:45:35.881084: Current learning rate: 0.00973\n",
      "2024-12-23 00:47:27.319727: train_loss -0.591\n",
      "2024-12-23 00:47:27.319891: val_loss -0.6071\n",
      "2024-12-23 00:47:27.319927: Pseudo dice [np.float32(0.6194)]\n",
      "2024-12-23 00:47:27.319963: Epoch time: 111.44 s\n",
      "2024-12-23 00:47:27.871812: \n",
      "2024-12-23 00:47:27.872275: Epoch 31\n",
      "2024-12-23 00:47:27.872386: Current learning rate: 0.00972\n",
      "2024-12-23 00:49:18.912373: train_loss -0.5742\n",
      "2024-12-23 00:49:18.912524: val_loss -0.6291\n",
      "2024-12-23 00:49:18.912558: Pseudo dice [np.float32(0.6899)]\n",
      "2024-12-23 00:49:18.912604: Epoch time: 111.04 s\n",
      "2024-12-23 00:49:18.912626: Yayy! New best EMA pseudo Dice: 0.6326000094413757\n",
      "2024-12-23 00:49:19.679261: \n",
      "2024-12-23 00:49:19.679461: Epoch 32\n",
      "2024-12-23 00:49:19.679589: Current learning rate: 0.00971\n",
      "2024-12-23 00:51:10.705303: train_loss -0.6119\n",
      "2024-12-23 00:51:10.705459: val_loss -0.6515\n",
      "2024-12-23 00:51:10.705587: Pseudo dice [np.float32(0.7052)]\n",
      "2024-12-23 00:51:10.705633: Epoch time: 111.03 s\n",
      "2024-12-23 00:51:10.705656: Yayy! New best EMA pseudo Dice: 0.6399000287055969\n",
      "2024-12-23 00:51:11.469664: \n",
      "2024-12-23 00:51:11.469818: Epoch 33\n",
      "2024-12-23 00:51:11.469889: Current learning rate: 0.0097\n",
      "2024-12-23 00:53:02.510331: train_loss -0.6374\n",
      "2024-12-23 00:53:02.510474: val_loss -0.5931\n",
      "2024-12-23 00:53:02.510509: Pseudo dice [np.float32(0.6115)]\n",
      "2024-12-23 00:53:02.510544: Epoch time: 111.04 s\n",
      "2024-12-23 00:53:03.057425: \n",
      "2024-12-23 00:53:03.057525: Epoch 34\n",
      "2024-12-23 00:53:03.057598: Current learning rate: 0.00969\n",
      "2024-12-23 00:54:54.091353: train_loss -0.601\n",
      "2024-12-23 00:54:54.091496: val_loss -0.6375\n",
      "2024-12-23 00:54:54.091528: Pseudo dice [np.float32(0.686)]\n",
      "2024-12-23 00:54:54.091563: Epoch time: 111.03 s\n",
      "2024-12-23 00:54:54.091585: Yayy! New best EMA pseudo Dice: 0.6420000195503235\n",
      "2024-12-23 00:54:54.878935: \n",
      "2024-12-23 00:54:54.879108: Epoch 35\n",
      "2024-12-23 00:54:54.879187: Current learning rate: 0.00968\n",
      "2024-12-23 00:56:45.908199: train_loss -0.6197\n",
      "2024-12-23 00:56:45.908340: val_loss -0.6641\n",
      "2024-12-23 00:56:45.908374: Pseudo dice [np.float32(0.6846)]\n",
      "2024-12-23 00:56:45.908412: Epoch time: 111.03 s\n",
      "2024-12-23 00:56:45.908540: Yayy! New best EMA pseudo Dice: 0.6462000012397766\n",
      "2024-12-23 00:56:46.669385: \n",
      "2024-12-23 00:56:46.669674: Epoch 36\n",
      "2024-12-23 00:56:46.669814: Current learning rate: 0.00968\n",
      "2024-12-23 00:58:37.730489: train_loss -0.6316\n",
      "2024-12-23 00:58:37.730788: val_loss -0.7217\n",
      "2024-12-23 00:58:37.730828: Pseudo dice [np.float32(0.7626)]\n",
      "2024-12-23 00:58:37.730864: Epoch time: 111.06 s\n",
      "2024-12-23 00:58:37.730884: Yayy! New best EMA pseudo Dice: 0.6578999757766724\n",
      "2024-12-23 00:58:38.493502: \n",
      "2024-12-23 00:58:38.493898: Epoch 37\n",
      "2024-12-23 00:58:38.493990: Current learning rate: 0.00967\n",
      "2024-12-23 01:00:29.544978: train_loss -0.6472\n",
      "2024-12-23 01:00:29.545113: val_loss -0.685\n",
      "2024-12-23 01:00:29.545148: Pseudo dice [np.float32(0.7679)]\n",
      "2024-12-23 01:00:29.545185: Epoch time: 111.05 s\n",
      "2024-12-23 01:00:29.545207: Yayy! New best EMA pseudo Dice: 0.6689000129699707\n",
      "2024-12-23 01:00:30.317719: \n",
      "2024-12-23 01:00:30.317908: Epoch 38\n",
      "2024-12-23 01:00:30.317992: Current learning rate: 0.00966\n",
      "2024-12-23 01:02:21.358009: train_loss -0.6047\n",
      "2024-12-23 01:02:21.358149: val_loss -0.6908\n",
      "2024-12-23 01:02:21.358256: Pseudo dice [np.float32(0.7771)]\n",
      "2024-12-23 01:02:21.358321: Epoch time: 111.04 s\n",
      "2024-12-23 01:02:21.358350: Yayy! New best EMA pseudo Dice: 0.6797000169754028\n",
      "2024-12-23 01:02:22.130298: \n",
      "2024-12-23 01:02:22.130392: Epoch 39\n",
      "2024-12-23 01:02:22.130461: Current learning rate: 0.00965\n",
      "2024-12-23 01:04:13.172857: train_loss -0.6111\n",
      "2024-12-23 01:04:13.172992: val_loss -0.6431\n",
      "2024-12-23 01:04:13.173029: Pseudo dice [np.float32(0.7002)]\n",
      "2024-12-23 01:04:13.173066: Epoch time: 111.04 s\n",
      "2024-12-23 01:04:13.173088: Yayy! New best EMA pseudo Dice: 0.6816999912261963\n",
      "2024-12-23 01:04:13.946523: \n",
      "2024-12-23 01:04:13.946631: Epoch 40\n",
      "2024-12-23 01:04:13.946702: Current learning rate: 0.00964\n",
      "2024-12-23 01:06:05.008985: train_loss -0.5737\n",
      "2024-12-23 01:06:05.009130: val_loss -0.7386\n",
      "2024-12-23 01:06:05.009225: Pseudo dice [np.float32(0.7619)]\n",
      "2024-12-23 01:06:05.009282: Epoch time: 111.06 s\n",
      "2024-12-23 01:06:05.009310: Yayy! New best EMA pseudo Dice: 0.6898000240325928\n",
      "2024-12-23 01:06:05.792231: \n",
      "2024-12-23 01:06:05.792396: Epoch 41\n",
      "2024-12-23 01:06:05.792483: Current learning rate: 0.00963\n",
      "2024-12-23 01:07:56.855325: train_loss -0.5949\n",
      "2024-12-23 01:07:56.855471: val_loss -0.6808\n",
      "2024-12-23 01:07:56.855506: Pseudo dice [np.float32(0.7592)]\n",
      "2024-12-23 01:07:56.855548: Epoch time: 111.06 s\n",
      "2024-12-23 01:07:56.855568: Yayy! New best EMA pseudo Dice: 0.6966999769210815\n",
      "2024-12-23 01:07:57.610021: \n",
      "2024-12-23 01:07:57.610110: Epoch 42\n",
      "2024-12-23 01:07:57.610180: Current learning rate: 0.00962\n",
      "2024-12-23 01:09:48.664117: train_loss -0.6181\n",
      "2024-12-23 01:09:48.664257: val_loss -0.6672\n",
      "2024-12-23 01:09:48.664292: Pseudo dice [np.float32(0.701)]\n",
      "2024-12-23 01:09:48.664327: Epoch time: 111.05 s\n",
      "2024-12-23 01:09:48.664348: Yayy! New best EMA pseudo Dice: 0.6970999836921692\n",
      "2024-12-23 01:09:49.414940: \n",
      "2024-12-23 01:09:49.415139: Epoch 43\n",
      "2024-12-23 01:09:49.415224: Current learning rate: 0.00961\n",
      "2024-12-23 01:11:40.439614: train_loss -0.5919\n",
      "2024-12-23 01:11:40.439760: val_loss -0.6938\n",
      "2024-12-23 01:11:40.439794: Pseudo dice [np.float32(0.72)]\n",
      "2024-12-23 01:11:40.439831: Epoch time: 111.03 s\n",
      "2024-12-23 01:11:40.439852: Yayy! New best EMA pseudo Dice: 0.699400007724762\n",
      "2024-12-23 01:11:41.197041: \n",
      "2024-12-23 01:11:41.197191: Epoch 44\n",
      "2024-12-23 01:11:41.197265: Current learning rate: 0.0096\n",
      "2024-12-23 01:13:32.272159: train_loss -0.601\n",
      "2024-12-23 01:13:32.272322: val_loss -0.6628\n",
      "2024-12-23 01:13:32.272356: Pseudo dice [np.float32(0.7431)]\n",
      "2024-12-23 01:13:32.272393: Epoch time: 111.08 s\n",
      "2024-12-23 01:13:32.272415: Yayy! New best EMA pseudo Dice: 0.7038000226020813\n",
      "2024-12-23 01:13:33.123180: \n",
      "2024-12-23 01:13:33.123328: Epoch 45\n",
      "2024-12-23 01:13:33.123403: Current learning rate: 0.00959\n",
      "2024-12-23 01:15:24.188631: train_loss -0.6247\n",
      "2024-12-23 01:15:24.188841: val_loss -0.6259\n",
      "2024-12-23 01:15:24.188876: Pseudo dice [np.float32(0.7129)]\n",
      "2024-12-23 01:15:24.188912: Epoch time: 111.07 s\n",
      "2024-12-23 01:15:24.188932: Yayy! New best EMA pseudo Dice: 0.7046999931335449\n",
      "2024-12-23 01:15:25.396267: \n",
      "2024-12-23 01:15:25.396560: Epoch 46\n",
      "2024-12-23 01:15:25.396718: Current learning rate: 0.00959\n",
      "2024-12-23 01:17:16.552672: train_loss -0.6436\n",
      "2024-12-23 01:17:16.552928: val_loss -0.5887\n",
      "2024-12-23 01:17:16.553217: Pseudo dice [np.float32(0.6433)]\n",
      "2024-12-23 01:17:16.553327: Epoch time: 111.16 s\n",
      "2024-12-23 01:17:17.096372: \n",
      "2024-12-23 01:17:17.096553: Epoch 47\n",
      "2024-12-23 01:17:17.096639: Current learning rate: 0.00958\n",
      "2024-12-23 01:19:08.263361: train_loss -0.674\n",
      "2024-12-23 01:19:08.263808: val_loss -0.6686\n",
      "2024-12-23 01:19:08.263849: Pseudo dice [np.float32(0.7548)]\n",
      "2024-12-23 01:19:08.263896: Epoch time: 111.17 s\n",
      "2024-12-23 01:19:08.808458: \n",
      "2024-12-23 01:19:08.808682: Epoch 48\n",
      "2024-12-23 01:19:08.808767: Current learning rate: 0.00957\n",
      "2024-12-23 01:20:59.993251: train_loss -0.6189\n",
      "2024-12-23 01:20:59.993455: val_loss -0.6877\n",
      "2024-12-23 01:20:59.993490: Pseudo dice [np.float32(0.7658)]\n",
      "2024-12-23 01:20:59.993526: Epoch time: 111.19 s\n",
      "2024-12-23 01:20:59.993549: Yayy! New best EMA pseudo Dice: 0.7103000283241272\n",
      "2024-12-23 01:21:00.779711: \n",
      "2024-12-23 01:21:00.780059: Epoch 49\n",
      "2024-12-23 01:21:00.780195: Current learning rate: 0.00956\n",
      "2024-12-23 01:22:51.972187: train_loss -0.6332\n",
      "2024-12-23 01:22:51.972327: val_loss -0.6533\n",
      "2024-12-23 01:22:51.972361: Pseudo dice [np.float32(0.683)]\n",
      "2024-12-23 01:22:51.972398: Epoch time: 111.19 s\n",
      "2024-12-23 01:22:52.718254: \n",
      "2024-12-23 01:22:52.718376: Epoch 50\n",
      "2024-12-23 01:22:52.718449: Current learning rate: 0.00955\n",
      "2024-12-23 01:24:43.907484: train_loss -0.6384\n",
      "2024-12-23 01:24:43.907758: val_loss -0.6655\n",
      "2024-12-23 01:24:43.907805: Pseudo dice [np.float32(0.741)]\n",
      "2024-12-23 01:24:43.907840: Epoch time: 111.19 s\n",
      "2024-12-23 01:24:43.907861: Yayy! New best EMA pseudo Dice: 0.7110000252723694\n",
      "2024-12-23 01:24:44.683884: \n",
      "2024-12-23 01:24:44.684091: Epoch 51\n",
      "2024-12-23 01:24:44.684163: Current learning rate: 0.00954\n",
      "2024-12-23 01:26:35.865653: train_loss -0.5681\n",
      "2024-12-23 01:26:35.865799: val_loss -0.6251\n",
      "2024-12-23 01:26:35.865837: Pseudo dice [np.float32(0.7166)]\n",
      "2024-12-23 01:26:35.865874: Epoch time: 111.18 s\n",
      "2024-12-23 01:26:35.865896: Yayy! New best EMA pseudo Dice: 0.7114999890327454\n",
      "2024-12-23 01:26:36.679883: \n",
      "2024-12-23 01:26:36.680237: Epoch 52\n",
      "2024-12-23 01:26:36.680393: Current learning rate: 0.00953\n",
      "2024-12-23 01:28:27.949443: train_loss -0.6529\n",
      "2024-12-23 01:28:27.949641: val_loss -0.6408\n",
      "2024-12-23 01:28:27.949677: Pseudo dice [np.float32(0.6883)]\n",
      "2024-12-23 01:28:27.949714: Epoch time: 111.27 s\n",
      "2024-12-23 01:28:28.514319: \n",
      "2024-12-23 01:28:28.514425: Epoch 53\n",
      "2024-12-23 01:28:28.514519: Current learning rate: 0.00952\n",
      "2024-12-23 01:30:19.657430: train_loss -0.5993\n",
      "2024-12-23 01:30:19.657596: val_loss -0.6617\n",
      "2024-12-23 01:30:19.657636: Pseudo dice [np.float32(0.7168)]\n",
      "2024-12-23 01:30:19.657672: Epoch time: 111.14 s\n",
      "2024-12-23 01:30:20.227068: \n",
      "2024-12-23 01:30:20.227307: Epoch 54\n",
      "2024-12-23 01:30:20.227384: Current learning rate: 0.00951\n",
      "2024-12-23 01:32:11.414770: train_loss -0.673\n",
      "2024-12-23 01:32:11.414966: val_loss -0.6771\n",
      "2024-12-23 01:32:11.415008: Pseudo dice [np.float32(0.7598)]\n",
      "2024-12-23 01:32:11.415046: Epoch time: 111.19 s\n",
      "2024-12-23 01:32:11.415068: Yayy! New best EMA pseudo Dice: 0.714900016784668\n",
      "2024-12-23 01:32:12.264601: \n",
      "2024-12-23 01:32:12.264719: Epoch 55\n",
      "2024-12-23 01:32:12.264789: Current learning rate: 0.0095\n",
      "2024-12-23 01:34:03.488391: train_loss -0.6302\n",
      "2024-12-23 01:34:03.488773: val_loss -0.6665\n",
      "2024-12-23 01:34:03.488843: Pseudo dice [np.float32(0.7152)]\n",
      "2024-12-23 01:34:03.488884: Epoch time: 111.22 s\n",
      "2024-12-23 01:34:03.488906: Yayy! New best EMA pseudo Dice: 0.7149999737739563\n",
      "2024-12-23 01:34:04.289776: \n",
      "2024-12-23 01:34:04.290015: Epoch 56\n",
      "2024-12-23 01:34:04.290103: Current learning rate: 0.00949\n",
      "2024-12-23 01:35:55.446786: train_loss -0.6641\n",
      "2024-12-23 01:35:55.446939: val_loss -0.6618\n",
      "2024-12-23 01:35:55.446974: Pseudo dice [np.float32(0.7353)]\n",
      "2024-12-23 01:35:55.447010: Epoch time: 111.16 s\n",
      "2024-12-23 01:35:55.447030: Yayy! New best EMA pseudo Dice: 0.7170000076293945\n",
      "2024-12-23 01:35:56.244486: \n",
      "2024-12-23 01:35:56.244911: Epoch 57\n",
      "2024-12-23 01:35:56.245031: Current learning rate: 0.00949\n",
      "2024-12-23 01:37:47.444095: train_loss -0.6417\n",
      "2024-12-23 01:37:47.444306: val_loss -0.6511\n",
      "2024-12-23 01:37:47.444343: Pseudo dice [np.float32(0.6879)]\n",
      "2024-12-23 01:37:47.444380: Epoch time: 111.2 s\n",
      "2024-12-23 01:37:47.997207: \n",
      "2024-12-23 01:37:47.997307: Epoch 58\n",
      "2024-12-23 01:37:47.997379: Current learning rate: 0.00948\n",
      "2024-12-23 01:39:39.230090: train_loss -0.6801\n",
      "2024-12-23 01:39:39.230247: val_loss -0.7043\n",
      "2024-12-23 01:39:39.230282: Pseudo dice [np.float32(0.7892)]\n",
      "2024-12-23 01:39:39.230317: Epoch time: 111.23 s\n",
      "2024-12-23 01:39:39.230340: Yayy! New best EMA pseudo Dice: 0.7215999960899353\n",
      "2024-12-23 01:39:40.030351: \n",
      "2024-12-23 01:39:40.030457: Epoch 59\n",
      "2024-12-23 01:39:40.030553: Current learning rate: 0.00947\n",
      "2024-12-23 01:41:31.225621: train_loss -0.6544\n",
      "2024-12-23 01:41:31.225794: val_loss -0.6206\n",
      "2024-12-23 01:41:31.225832: Pseudo dice [np.float32(0.7114)]\n",
      "2024-12-23 01:41:31.225867: Epoch time: 111.2 s\n",
      "2024-12-23 01:41:31.792880: \n",
      "2024-12-23 01:41:31.793226: Epoch 60\n",
      "2024-12-23 01:41:31.793430: Current learning rate: 0.00946\n",
      "2024-12-23 01:43:22.998255: train_loss -0.6263\n",
      "2024-12-23 01:43:22.998499: val_loss -0.4926\n",
      "2024-12-23 01:43:22.998547: Pseudo dice [np.float32(0.5125)]\n",
      "2024-12-23 01:43:22.998586: Epoch time: 111.21 s\n",
      "2024-12-23 01:43:23.568307: \n",
      "2024-12-23 01:43:23.568731: Epoch 61\n",
      "2024-12-23 01:43:23.568842: Current learning rate: 0.00945\n",
      "2024-12-23 01:45:14.781746: train_loss -0.6557\n",
      "2024-12-23 01:45:14.781901: val_loss -0.6443\n",
      "2024-12-23 01:45:14.781936: Pseudo dice [np.float32(0.7612)]\n",
      "2024-12-23 01:45:14.781973: Epoch time: 111.21 s\n",
      "2024-12-23 01:45:15.344492: \n",
      "2024-12-23 01:45:15.344593: Epoch 62\n",
      "2024-12-23 01:45:15.344666: Current learning rate: 0.00944\n",
      "train_loss -0.67966.519660: \n",
      "2024-12-23 01:47:06.520008: val_loss -0.6046\n",
      "2024-12-23 01:47:06.520190: Pseudo dice [np.float32(0.6895)]\n",
      "2024-12-23 01:47:06.520277: Epoch time: 111.18 s\n",
      "2024-12-23 01:47:07.541831: \n",
      "2024-12-23 01:47:07.541941: Epoch 63\n",
      "2024-12-23 01:47:07.542031: Current learning rate: 0.00943\n",
      "2024-12-23 01:48:58.701400: train_loss -0.6446\n",
      "2024-12-23 01:48:58.701550: val_loss -0.7086\n",
      "2024-12-23 01:48:58.701588: Pseudo dice [np.float32(0.7888)]\n",
      "2024-12-23 01:48:58.701624: Epoch time: 111.16 s\n",
      "2024-12-23 01:48:59.273122: \n",
      "2024-12-23 01:48:59.273520: Epoch 64\n",
      "2024-12-23 01:48:59.273680: Current learning rate: 0.00942\n",
      "2024-12-23 01:50:50.468760: train_loss -0.6821\n",
      "2024-12-23 01:50:50.468920: val_loss -0.7351\n",
      "2024-12-23 01:50:50.468968: Pseudo dice [np.float32(0.8144)]\n",
      "2024-12-23 01:50:50.469012: Epoch time: 111.2 s\n",
      "2024-12-23 01:50:50.469036: Yayy! New best EMA pseudo Dice: 0.7228999733924866\n",
      "2024-12-23 01:50:51.262911: \n",
      "2024-12-23 01:50:51.263119: Epoch 65\n",
      "2024-12-23 01:50:51.263220: Current learning rate: 0.00941\n",
      "2024-12-23 01:52:42.473247: train_loss -0.6799\n",
      "2024-12-23 01:52:42.473619: val_loss -0.6819\n",
      "2024-12-23 01:52:42.473694: Pseudo dice [np.float32(0.7649)]\n",
      "2024-12-23 01:52:42.473738: Epoch time: 111.21 s\n",
      "2024-12-23 01:52:42.473763: Yayy! New best EMA pseudo Dice: 0.7271000146865845\n",
      "2024-12-23 01:52:43.288687: \n",
      "2024-12-23 01:52:43.288891: Epoch 66\n",
      "2024-12-23 01:52:43.288980: Current learning rate: 0.0094\n",
      "2024-12-23 01:54:34.518371: train_loss -0.6353\n",
      "2024-12-23 01:54:34.518923: val_loss -0.6156\n",
      "2024-12-23 01:54:34.519068: Pseudo dice [np.float32(0.6814)]\n",
      "2024-12-23 01:54:34.519141: Epoch time: 111.23 s\n",
      "2024-12-23 01:54:35.086364: \n",
      "2024-12-23 01:54:35.086490: Epoch 67\n",
      "2024-12-23 01:54:35.086564: Current learning rate: 0.00939\n",
      "2024-12-23 01:56:26.329760: train_loss -0.6274\n",
      "2024-12-23 01:56:26.329948: val_loss -0.7092\n",
      "2024-12-23 01:56:26.329982: Pseudo dice [np.float32(0.762)]\n",
      "2024-12-23 01:56:26.330017: Epoch time: 111.24 s\n",
      "2024-12-23 01:56:26.893260: \n",
      "2024-12-23 01:56:26.893694: Epoch 68\n",
      "2024-12-23 01:56:26.893876: Current learning rate: 0.00939\n",
      "2024-12-23 01:58:18.126074: train_loss -0.6504\n",
      "2024-12-23 01:58:18.126224: val_loss -0.6952\n",
      "2024-12-23 01:58:18.126258: Pseudo dice [np.float32(0.7329)]\n",
      "2024-12-23 01:58:18.126296: Epoch time: 111.23 s\n",
      "2024-12-23 01:58:18.126318: Yayy! New best EMA pseudo Dice: 0.7271000146865845\n",
      "2024-12-23 01:58:18.927269: \n",
      "2024-12-23 01:58:18.927397: Epoch 69\n",
      "2024-12-23 01:58:18.927471: Current learning rate: 0.00938\n",
      "2024-12-23 02:00:10.149198: train_loss -0.6501\n",
      "2024-12-23 02:00:10.149424: val_loss -0.7315\n",
      "2024-12-23 02:00:10.149460: Pseudo dice [np.float32(0.8066)]\n",
      "2024-12-23 02:00:10.149497: Epoch time: 111.22 s\n",
      "2024-12-23 02:00:10.149521: Yayy! New best EMA pseudo Dice: 0.7350999712944031\n",
      "2024-12-23 02:00:10.974174: \n",
      "2024-12-23 02:00:10.974453: Epoch 70\n",
      "2024-12-23 02:00:10.974599: Current learning rate: 0.00937\n",
      "2024-12-23 02:02:02.219907: train_loss -0.6612\n",
      "2024-12-23 02:02:02.220344: val_loss -0.6978\n",
      "2024-12-23 02:02:02.220390: Pseudo dice [np.float32(0.7563)]\n",
      "2024-12-23 02:02:02.220429: Epoch time: 111.25 s\n",
      "2024-12-23 02:02:02.220451: Yayy! New best EMA pseudo Dice: 0.7372000217437744\n",
      "2024-12-23 02:02:03.027824: \n",
      "2024-12-23 02:02:03.027937: Epoch 71\n",
      "2024-12-23 02:02:03.028010: Current learning rate: 0.00936\n",
      "2024-12-23 02:03:54.307117: train_loss -0.665\n",
      "2024-12-23 02:03:54.307513: val_loss -0.6257\n",
      "2024-12-23 02:03:54.307663: Pseudo dice [np.float32(0.7067)]\n",
      "2024-12-23 02:03:54.307712: Epoch time: 111.28 s\n",
      "2024-12-23 02:03:54.877537: \n",
      "2024-12-23 02:03:54.877884: Epoch 72\n",
      "2024-12-23 02:03:54.877969: Current learning rate: 0.00935\n",
      "2024-12-23 02:05:46.076249: train_loss -0.6627\n",
      "2024-12-23 02:05:46.076482: val_loss -0.6825\n",
      "2024-12-23 02:05:46.076606: Pseudo dice [np.float32(0.7379)]\n",
      "2024-12-23 02:05:46.076652: Epoch time: 111.2 s\n",
      "2024-12-23 02:05:46.651235: \n",
      "2024-12-23 02:05:46.651341: Epoch 73\n",
      "2024-12-23 02:05:46.651414: Current learning rate: 0.00934\n",
      "2024-12-23 02:07:37.886284: train_loss -0.6206\n",
      "2024-12-23 02:07:37.886434: val_loss -0.6141\n",
      "2024-12-23 02:07:37.886468: Pseudo dice [np.float32(0.6661)]\n",
      "2024-12-23 02:07:37.886504: Epoch time: 111.24 s\n",
      "2024-12-23 02:07:38.448014: \n",
      "2024-12-23 02:07:38.448122: Epoch 74\n",
      "2024-12-23 02:07:38.448193: Current learning rate: 0.00933\n",
      "2024-12-23 02:09:29.696597: train_loss -0.6423\n",
      "2024-12-23 02:09:29.696749: val_loss -0.7359\n",
      "2024-12-23 02:09:29.696793: Pseudo dice [np.float32(0.8119)]\n",
      "2024-12-23 02:09:29.696829: Epoch time: 111.25 s\n",
      "2024-12-23 02:09:30.268241: \n",
      "2024-12-23 02:09:30.268402: Epoch 75\n",
      "2024-12-23 02:09:30.268488: Current learning rate: 0.00932\n",
      "2024-12-23 02:11:21.488472: train_loss -0.6462\n",
      "2024-12-23 02:11:21.488629: val_loss -0.7049\n",
      "2024-12-23 02:11:21.488760: Pseudo dice [np.float32(0.7741)]\n",
      "2024-12-23 02:11:21.488813: Epoch time: 111.22 s\n",
      "2024-12-23 02:11:21.488854: Yayy! New best EMA pseudo Dice: 0.7398999929428101\n",
      "2024-12-23 02:11:22.304788: \n",
      "2024-12-23 02:11:22.305346: Epoch 76\n",
      "2024-12-23 02:11:22.305439: Current learning rate: 0.00931\n",
      "2024-12-23 02:13:13.499152: train_loss -0.6729\n",
      "2024-12-23 02:13:13.499367: val_loss -0.674\n",
      "2024-12-23 02:13:13.499534: Pseudo dice [np.float32(0.7612)]\n",
      "2024-12-23 02:13:13.499607: Epoch time: 111.19 s\n",
      "2024-12-23 02:13:13.499638: Yayy! New best EMA pseudo Dice: 0.7419999837875366\n",
      "2024-12-23 02:13:14.318045: \n",
      "2024-12-23 02:13:14.318430: Epoch 77\n",
      "2024-12-23 02:13:14.318595: Current learning rate: 0.0093\n",
      "2024-12-23 02:15:05.526827: train_loss -0.6644\n",
      "2024-12-23 02:15:05.526999: val_loss -0.7233\n",
      "2024-12-23 02:15:05.527067: Pseudo dice [np.float32(0.7731)]\n",
      "2024-12-23 02:15:05.527112: Epoch time: 111.21 s\n",
      "2024-12-23 02:15:05.527135: Yayy! New best EMA pseudo Dice: 0.7451000213623047\n",
      "2024-12-23 02:15:06.330828: \n",
      "2024-12-23 02:15:06.330989: Epoch 78\n",
      "2024-12-23 02:15:06.331065: Current learning rate: 0.0093\n",
      "2024-12-23 02:16:57.581774: train_loss -0.6889\n",
      "2024-12-23 02:16:57.581916: val_loss -0.7104\n",
      "2024-12-23 02:16:57.581953: Pseudo dice [np.float32(0.7755)]\n",
      "2024-12-23 02:16:57.581990: Epoch time: 111.25 s\n",
      "2024-12-23 02:16:57.582015: Yayy! New best EMA pseudo Dice: 0.748199999332428\n",
      "2024-12-23 02:16:58.789045: \n",
      "2024-12-23 02:16:58.789598: Epoch 79\n",
      "2024-12-23 02:16:58.789730: Current learning rate: 0.00929\n",
      "2024-12-23 02:18:50.060761: train_loss -0.6779\n",
      "2024-12-23 02:18:50.061106: val_loss -0.6851\n",
      "2024-12-23 02:18:50.061148: Pseudo dice [np.float32(0.6932)]\n",
      "2024-12-23 02:18:50.061183: Epoch time: 111.27 s\n",
      "2024-12-23 02:18:50.643959: \n",
      "2024-12-23 02:18:50.644091: Epoch 80\n",
      "2024-12-23 02:18:50.644166: Current learning rate: 0.00928\n",
      "2024-12-23 02:20:41.893363: train_loss -0.6479\n",
      "2024-12-23 02:20:41.893697: val_loss -0.6527\n",
      "2024-12-23 02:20:41.893753: Pseudo dice [np.float32(0.7134)]\n",
      "2024-12-23 02:20:41.893796: Epoch time: 111.25 s\n",
      "2024-12-23 02:20:42.478747: \n",
      "2024-12-23 02:20:42.479115: Epoch 81\n",
      "2024-12-23 02:20:42.479193: Current learning rate: 0.00927\n",
      "2024-12-23 02:22:33.752207: train_loss -0.6836\n",
      "2024-12-23 02:22:33.752361: val_loss -0.6286\n",
      "2024-12-23 02:22:33.752395: Pseudo dice [np.float32(0.6749)]\n",
      "2024-12-23 02:22:33.752432: Epoch time: 111.27 s\n",
      "2024-12-23 02:22:34.337202: \n",
      "2024-12-23 02:22:34.337335: Epoch 82\n",
      "2024-12-23 02:22:34.337406: Current learning rate: 0.00926\n",
      "2024-12-23 02:24:25.621879: train_loss -0.7048\n",
      "2024-12-23 02:24:25.622011: val_loss -0.6879\n",
      "2024-12-23 02:24:25.622045: Pseudo dice [np.float32(0.7459)]\n",
      "2024-12-23 02:24:25.622082: Epoch time: 111.29 s\n",
      "2024-12-23 02:24:26.179794: \n",
      "2024-12-23 02:24:26.180192: Epoch 83\n",
      "2024-12-23 02:24:26.180355: Current learning rate: 0.00925\n",
      "2024-12-23 02:26:17.460340: train_loss -0.7093\n",
      "2024-12-23 02:26:17.460525: val_loss -0.6996\n",
      "2024-12-23 02:26:17.460604: Pseudo dice [np.float32(0.7664)]\n",
      "2024-12-23 02:26:17.460652: Epoch time: 111.28 s\n",
      "2024-12-23 02:26:18.013505: \n",
      "2024-12-23 02:26:18.013716: Epoch 84\n",
      "2024-12-23 02:26:18.013793: Current learning rate: 0.00924\n",
      "2024-12-23 02:28:09.379818: train_loss -0.6684\n",
      "2024-12-23 02:28:09.379956: val_loss -0.6599\n",
      "2024-12-23 02:28:09.379991: Pseudo dice [np.float32(0.7409)]\n",
      "2024-12-23 02:28:09.380027: Epoch time: 111.37 s\n",
      "2024-12-23 02:28:09.945477: \n",
      "2024-12-23 02:28:09.945980: Epoch 85\n",
      "Current learning rate: 0.00923\n",
      "2024-12-23 02:30:01.242743: train_loss -0.677\n",
      "2024-12-23 02:30:01.242900: val_loss -0.7112\n",
      "2024-12-23 02:30:01.242935: Pseudo dice [np.float32(0.7582)]\n",
      "2024-12-23 02:30:01.242973: Epoch time: 111.3 s\n",
      "2024-12-23 02:30:01.806587: \n",
      "2024-12-23 02:30:01.806714: Epoch 86\n",
      "2024-12-23 02:30:01.806787: Current learning rate: 0.00922\n",
      "2024-12-23 02:31:53.100464: train_loss -0.6924\n",
      "2024-12-23 02:31:53.100617: val_loss -0.6375\n",
      "2024-12-23 02:31:53.100652: Pseudo dice [np.float32(0.7373)]\n",
      "2024-12-23 02:31:53.100690: Epoch time: 111.29 s\n",
      "2024-12-23 02:31:53.658877: \n",
      "2024-12-23 02:31:53.659091: Epoch 87\n",
      "2024-12-23 02:31:53.659181: Current learning rate: 0.00921\n",
      "2024-12-23 02:33:44.926638: train_loss -0.7083\n",
      "2024-12-23 02:33:44.926780: val_loss -0.6894\n",
      "2024-12-23 02:33:44.926815: Pseudo dice [np.float32(0.7535)]\n",
      "2024-12-23 02:33:44.926852: Epoch time: 111.27 s\n",
      "2024-12-23 02:33:45.497207: \n",
      "2024-12-23 02:33:45.497316: Epoch 88\n",
      "2024-12-23 02:33:45.497390: Current learning rate: 0.0092\n",
      "2024-12-23 02:35:36.726936: train_loss -0.6932\n",
      "2024-12-23 02:35:36.727082: val_loss -0.6027\n",
      "2024-12-23 02:35:36.727117: Pseudo dice [np.float32(0.6782)]\n",
      "2024-12-23 02:35:36.727153: Epoch time: 111.23 s\n",
      "2024-12-23 02:35:37.289992: \n",
      "2024-12-23 02:35:37.290107: Epoch 89\n",
      "2024-12-23 02:35:37.290178: Current learning rate: 0.0092\n",
      "2024-12-23 02:37:28.534975: train_loss -0.612\n",
      "2024-12-23 02:37:28.535109: val_loss -0.646\n",
      "2024-12-23 02:37:28.535141: Pseudo dice [np.float32(0.7349)]\n",
      "2024-12-23 02:37:28.535177: Epoch time: 111.25 s\n",
      "2024-12-23 02:37:29.097261: \n",
      "2024-12-23 02:37:29.097568: Epoch 90\n",
      "2024-12-23 02:37:29.097730: Current learning rate: 0.00919\n",
      "2024-12-23 02:39:20.398995: train_loss -0.6814\n",
      "2024-12-23 02:39:20.399131: val_loss -0.6672\n",
      "2024-12-23 02:39:20.399164: Pseudo dice [np.float32(0.712)]\n",
      "2024-12-23 02:39:20.399201: Epoch time: 111.3 s\n",
      "2024-12-23 02:39:20.953424: \n",
      "2024-12-23 02:39:20.953846: Epoch 91\n",
      "2024-12-23 02:39:20.953956: Current learning rate: 0.00918\n",
      "2024-12-23 02:41:12.201742: train_loss -0.6791\n",
      "2024-12-23 02:41:12.201885: val_loss -0.7141\n",
      "2024-12-23 02:41:12.201921: Pseudo dice [np.float32(0.8154)]\n",
      "2024-12-23 02:41:12.201959: Epoch time: 111.25 s\n",
      "2024-12-23 02:41:12.761954: \n",
      "2024-12-23 02:41:12.762055: Epoch 92\n",
      "2024-12-23 02:41:12.762127: Current learning rate: 0.00917\n",
      "2024-12-23 02:43:03.998024: train_loss -0.6542\n",
      "2024-12-23 02:43:03.998235: val_loss -0.6264\n",
      "2024-12-23 02:43:03.998271: Pseudo dice [np.float32(0.6714)]\n",
      "2024-12-23 02:43:03.998307: Epoch time: 111.24 s\n",
      "2024-12-23 02:43:04.547811: \n",
      "2024-12-23 02:43:04.547908: Epoch 93\n",
      "2024-12-23 02:43:04.547981: Current learning rate: 0.00916\n",
      "2024-12-23 02:44:55.807213: train_loss -0.6477\n",
      "2024-12-23 02:44:55.807364: val_loss -0.6293\n",
      "2024-12-23 02:44:55.807399: Pseudo dice [np.float32(0.7205)]\n",
      "2024-12-23 02:44:55.807435: Epoch time: 111.26 s\n",
      "2024-12-23 02:44:56.357654: \n",
      "2024-12-23 02:44:56.357961: Epoch 94\n",
      "2024-12-23 02:44:56.358125: Current learning rate: 0.00915\n",
      "2024-12-23 02:46:47.609394: train_loss -0.6622\n",
      "2024-12-23 02:46:47.609535: val_loss -0.6516\n",
      "2024-12-23 02:46:47.609569: Pseudo dice [np.float32(0.5717)]\n",
      "2024-12-23 02:46:47.609605: Epoch time: 111.25 s\n",
      "2024-12-23 02:46:48.163665: \n",
      "2024-12-23 02:46:48.163772: Epoch 95\n",
      "2024-12-23 02:46:48.163844: Current learning rate: 0.00914\n",
      "2024-12-23 02:48:39.406134: train_loss -0.6418\n",
      "2024-12-23 02:48:39.406297: val_loss -0.5875\n",
      "2024-12-23 02:48:39.406332: Pseudo dice [np.float32(0.5896)]\n",
      "2024-12-23 02:48:39.406379: Epoch time: 111.24 s\n",
      "2024-12-23 02:48:39.957567: \n",
      "2024-12-23 02:48:39.957774: Epoch 96\n",
      "2024-12-23 02:48:39.957885: Current learning rate: 0.00913\n",
      "2024-12-23 02:50:31.220068: train_loss -0.6633\n",
      "2024-12-23 02:50:31.220218: val_loss -0.6363\n",
      "2024-12-23 02:50:31.220254: Pseudo dice [np.float32(0.7019)]\n",
      "2024-12-23 02:50:31.220291: Epoch time: 111.26 s\n",
      "2024-12-23 02:50:32.219774: \n",
      "2024-12-23 02:50:32.220000: Epoch 97\n",
      "2024-12-23 02:50:32.220093: Current learning rate: 0.00912\n",
      "2024-12-23 02:52:23.496926: train_loss -0.645\n",
      "2024-12-23 02:52:23.497151: val_loss -0.6379\n",
      "2024-12-23 02:52:23.497193: Pseudo dice [np.float32(0.7342)]\n",
      "2024-12-23 02:52:23.497231: Epoch time: 111.28 s\n",
      "2024-12-23 02:52:24.056261: \n",
      "2024-12-23 02:52:24.056448: Epoch 98\n",
      "2024-12-23 02:52:24.056539: Current learning rate: 0.00911\n",
      "2024-12-23 02:54:15.249296: train_loss -0.6815\n",
      "2024-12-23 02:54:15.249444: val_loss -0.681\n",
      "2024-12-23 02:54:15.249480: Pseudo dice [np.float32(0.7521)]\n",
      "2024-12-23 02:54:15.249517: Epoch time: 111.19 s\n",
      "2024-12-23 02:54:15.815154: \n",
      "2024-12-23 02:54:15.815375: Epoch 99\n",
      "2024-12-23 02:54:15.815591: Current learning rate: 0.0091\n",
      "2024-12-23 02:56:07.071901: train_loss -0.6613\n",
      "2024-12-23 02:56:07.072097: val_loss -0.6828\n",
      "2024-12-23 02:56:07.072135: Pseudo dice [np.float32(0.7427)]\n",
      "2024-12-23 02:56:07.072171: Epoch time: 111.26 s\n",
      "2024-12-23 02:56:07.857275: \n",
      "2024-12-23 02:56:07.857472: Epoch 100\n",
      "2024-12-23 02:56:07.857546: Current learning rate: 0.0091\n",
      "2024-12-23 02:57:59.032185: train_loss -0.6769\n",
      "2024-12-23 02:57:59.032439: val_loss -0.6253\n",
      "2024-12-23 02:57:59.032476: Pseudo dice [np.float32(0.6886)]\n",
      "2024-12-23 02:57:59.032514: Epoch time: 111.18 s\n",
      "2024-12-23 02:57:59.591256: \n",
      "2024-12-23 02:57:59.591382: Epoch 101\n",
      "2024-12-23 02:57:59.591456: Current learning rate: 0.00909\n",
      "2024-12-23 02:59:50.858119: train_loss -0.6139\n",
      "2024-12-23 02:59:50.858321: val_loss -0.6212\n",
      "2024-12-23 02:59:50.858358: Pseudo dice [np.float32(0.6592)]\n",
      "2024-12-23 02:59:50.858394: Epoch time: 111.27 s\n",
      "2024-12-23 02:59:51.416173: \n",
      "2024-12-23 02:59:51.416390: Epoch 102\n",
      "2024-12-23 02:59:51.416463: Current learning rate: 0.00908\n",
      "2024-12-23 03:01:42.646210: train_loss -0.6749\n",
      "2024-12-23 03:01:42.646361: val_loss -0.6472\n",
      "2024-12-23 03:01:42.646398: Pseudo dice [np.float32(0.748)]\n",
      "2024-12-23 03:01:42.646437: Epoch time: 111.23 s\n",
      "2024-12-23 03:01:43.216018: \n",
      "2024-12-23 03:01:43.216135: Epoch 103\n",
      "2024-12-23 03:01:43.216204: Current learning rate: 0.00907\n",
      "2024-12-23 03:03:34.401323: train_loss -0.7163\n",
      "2024-12-23 03:03:34.401695: val_loss -0.7144\n",
      "2024-12-23 03:03:34.401746: Pseudo dice [np.float32(0.7448)]\n",
      "2024-12-23 03:03:34.401783: Epoch time: 111.19 s\n",
      "2024-12-23 03:03:34.961382: \n",
      "2024-12-23 03:03:34.961745: Epoch 104\n",
      "2024-12-23 03:03:34.961832: Current learning rate: 0.00906\n",
      "2024-12-23 03:05:26.193763: train_loss -0.674\n",
      "2024-12-23 03:05:26.193905: val_loss -0.6877\n",
      "2024-12-23 03:05:26.193937: Pseudo dice [np.float32(0.7308)]\n",
      "2024-12-23 03:05:26.193970: Epoch time: 111.23 s\n",
      "2024-12-23 03:05:26.749105: \n",
      "2024-12-23 03:05:26.749541: Epoch 105\n",
      "2024-12-23 03:05:26.749680: Current learning rate: 0.00905\n",
      "2024-12-23 03:07:17.930139: train_loss -0.6806\n",
      "2024-12-23 03:07:17.930286: val_loss -0.7442\n",
      "2024-12-23 03:07:17.930320: Pseudo dice [np.float32(0.8206)]\n",
      "2024-12-23 03:07:17.930357: Epoch time: 111.18 s\n",
      "2024-12-23 03:07:18.482108: \n",
      "2024-12-23 03:07:18.482235: Epoch 106\n",
      "2024-12-23 03:07:18.482311: Current learning rate: 0.00904\n",
      "2024-12-23 03:09:09.711056: train_loss -0.7024\n",
      "2024-12-23 03:09:09.711195: val_loss -0.6666\n",
      "2024-12-23 03:09:09.711228: Pseudo dice [np.float32(0.7662)]\n",
      "2024-12-23 03:09:09.711263: Epoch time: 111.23 s\n",
      "2024-12-23 03:09:10.266432: \n",
      "2024-12-23 03:09:10.266545: Epoch 107\n",
      "2024-12-23 03:09:10.266639: Current learning rate: 0.00903\n",
      "2024-12-23 03:11:01.478542: train_loss -0.6882\n",
      "2024-12-23 03:11:01.478705: val_loss -0.6425\n",
      "2024-12-23 03:11:01.478740: Pseudo dice [np.float32(0.6884)]\n",
      "2024-12-23 03:11:01.478777: Epoch time: 111.21 s\n",
      "2024-12-23 03:11:02.030049: \n",
      "2024-12-23 03:11:02.030160: Epoch 108\n",
      "2024-12-23 03:11:02.030234: Current learning rate: 0.00902\n",
      "2024-12-23 03:12:53.293339: train_loss -0.6961\n",
      "2024-12-23 03:12:53.293526: val_loss -0.6879\n",
      "2024-12-23 03:12:53.293644: Pseudo dice [np.float32(0.7336)]\n",
      "2024-12-23 03:12:53.293773: Epoch time: 111.26 s\n",
      "2024-12-23 03:12:53.846153: \n",
      "2024-12-23 03:12:53.846553: Epoch 109\n",
      "2024-12-23 03:12:53.846690: Current learning rate: 0.00901\n",
      "2024-12-23 03:14:45.077771: train_loss -0.678\n",
      "2024-12-23 03:14:45.077918: val_loss -0.7351\n",
      "2024-12-23 03:14:45.077953: Pseudo dice [np.float32(0.8176)]\n",
      "2024-12-23 03:14:45.077989: Epoch time: 111.23 s\n",
      "2024-12-23 03:14:45.642299: \n",
      "2024-12-23 03:14:45.642394: Epoch 110\n",
      "Current learning rate: 0.009\n",
      "2024-12-23 03:16:36.862652: train_loss -0.6843\n",
      "2024-12-23 03:16:36.862899: val_loss -0.5646\n",
      "2024-12-23 03:16:36.863120: Pseudo dice [np.float32(0.6541)]\n",
      "2024-12-23 03:16:36.863194: Epoch time: 111.22 s\n",
      "2024-12-23 03:16:37.417493: \n",
      "2024-12-23 03:16:37.417602: Epoch 111\n",
      "2024-12-23 03:16:37.417675: Current learning rate: 0.009\n",
      "2024-12-23 03:18:28.674741: train_loss -0.618\n",
      "2024-12-23 03:18:28.674888: val_loss -0.6141\n",
      "2024-12-23 03:18:28.674922: Pseudo dice [np.float32(0.6316)]\n",
      "2024-12-23 03:18:28.674958: Epoch time: 111.26 s\n",
      "2024-12-23 03:18:29.225208: \n",
      "2024-12-23 03:18:29.225302: Epoch 112\n",
      "2024-12-23 03:18:29.225373: Current learning rate: 0.00899\n",
      "2024-12-23 03:20:20.456568: train_loss -0.6145\n",
      "2024-12-23 03:20:20.456713: val_loss -0.5819\n",
      "2024-12-23 03:20:20.456751: Pseudo dice [np.float32(0.6617)]\n",
      "2024-12-23 03:20:20.456787: Epoch time: 111.23 s\n",
      "2024-12-23 03:20:21.014580: \n",
      "2024-12-23 03:20:21.014977: Epoch 113\n",
      "2024-12-23 03:20:21.015233: Current learning rate: 0.00898\n",
      "2024-12-23 03:22:12.277871: train_loss -0.6676\n",
      "2024-12-23 03:22:12.278146: val_loss -0.654\n",
      "2024-12-23 03:22:12.278254: Pseudo dice [np.float32(0.721)]\n",
      "2024-12-23 03:22:12.278350: Epoch time: 111.26 s\n",
      "2024-12-23 03:22:12.830734: \n",
      "2024-12-23 03:22:12.830831: Epoch 114\n",
      "2024-12-23 03:22:12.830904: Current learning rate: 0.00897\n",
      "2024-12-23 03:24:04.020686: train_loss -0.6821\n",
      "2024-12-23 03:24:04.020831: val_loss -0.7283\n",
      "2024-12-23 03:24:04.020867: Pseudo dice [np.float32(0.7975)]\n",
      "2024-12-23 03:24:04.020904: Epoch time: 111.19 s\n",
      "2024-12-23 03:24:05.012458: \n",
      "2024-12-23 03:24:05.012691: Epoch 115\n",
      "2024-12-23 03:24:05.012820: Current learning rate: 0.00896\n",
      "2024-12-23 03:25:56.271935: train_loss -0.6496\n",
      "2024-12-23 03:25:56.272109: val_loss -0.665\n",
      "2024-12-23 03:25:56.272145: Pseudo dice [np.float32(0.718)]\n",
      "2024-12-23 03:25:56.272180: Epoch time: 111.26 s\n",
      "2024-12-23 03:25:56.834810: \n",
      "2024-12-23 03:25:56.834949: Epoch 116\n",
      "2024-12-23 03:25:56.835025: Current learning rate: 0.00895\n",
      "2024-12-23 03:27:48.180959: train_loss -0.6613\n",
      "2024-12-23 03:27:48.181103: val_loss -0.6384\n",
      "2024-12-23 03:27:48.181139: Pseudo dice [np.float32(0.7004)]\n",
      "2024-12-23 03:27:48.181178: Epoch time: 111.35 s\n",
      "2024-12-23 03:27:48.747322: \n",
      "2024-12-23 03:27:48.747464: Epoch 117\n",
      "2024-12-23 03:27:48.747546: Current learning rate: 0.00894\n",
      "2024-12-23 03:29:39.970405: train_loss -0.7017\n",
      "2024-12-23 03:29:39.970564: val_loss -0.5841\n",
      "2024-12-23 03:29:39.970599: Pseudo dice [np.float32(0.6777)]\n",
      "2024-12-23 03:29:39.970635: Epoch time: 111.22 s\n",
      "2024-12-23 03:29:40.531308: \n",
      "2024-12-23 03:29:40.531722: Epoch 118\n",
      "2024-12-23 03:29:40.531809: Current learning rate: 0.00893\n",
      "2024-12-23 03:31:31.800411: train_loss -0.6497\n",
      "2024-12-23 03:31:31.800647: val_loss -0.6937\n",
      "2024-12-23 03:31:31.800684: Pseudo dice [np.float32(0.7493)]\n",
      "2024-12-23 03:31:31.800722: Epoch time: 111.27 s\n",
      "2024-12-23 03:31:32.362520: \n",
      "2024-12-23 03:31:32.362647: Epoch 119\n",
      "2024-12-23 03:31:32.362758: Current learning rate: 0.00892\n",
      "2024-12-23 03:33:23.647115: train_loss -0.7072\n",
      "2024-12-23 03:33:23.647339: val_loss -0.7359\n",
      "2024-12-23 03:33:23.647377: Pseudo dice [np.float32(0.721)]\n",
      "2024-12-23 03:33:23.647413: Epoch time: 111.29 s\n",
      "2024-12-23 03:33:24.214546: \n",
      "2024-12-23 03:33:24.214675: Epoch 120\n",
      "2024-12-23 03:33:24.214756: Current learning rate: 0.00891\n",
      "2024-12-23 03:35:15.491036: train_loss -0.7225\n",
      "2024-12-23 03:35:15.491443: val_loss -0.6181\n",
      "2024-12-23 03:35:15.491508: Pseudo dice [np.float32(0.7164)]\n",
      "2024-12-23 03:35:15.491677: Epoch time: 111.28 s\n",
      "2024-12-23 03:35:16.056727: \n",
      "2024-12-23 03:35:16.057074: Epoch 121\n",
      "2024-12-23 03:35:16.057317: Current learning rate: 0.0089\n",
      "2024-12-23 03:37:07.268614: train_loss -0.691\n",
      "2024-12-23 03:37:07.268817: val_loss -0.7197\n",
      "2024-12-23 03:37:07.268853: Pseudo dice [np.float32(0.7803)]\n",
      "2024-12-23 03:37:07.268890: Epoch time: 111.21 s\n",
      "2024-12-23 03:37:07.836665: \n",
      "2024-12-23 03:37:07.837111: Epoch 122\n",
      "2024-12-23 03:37:07.837310: Current learning rate: 0.00889\n",
      "2024-12-23 03:38:59.111592: train_loss -0.6735\n",
      "2024-12-23 03:38:59.112072: val_loss -0.7133\n",
      "2024-12-23 03:38:59.112136: Pseudo dice [np.float32(0.7885)]\n",
      "2024-12-23 03:38:59.112187: Epoch time: 111.28 s\n",
      "2024-12-23 03:38:59.677067: \n",
      "2024-12-23 03:38:59.677463: Epoch 123\n",
      "2024-12-23 03:38:59.677577: Current learning rate: 0.00889\n",
      "2024-12-23 03:40:50.923469: train_loss -0.7052\n",
      "2024-12-23 03:40:50.923953: val_loss -0.6226\n",
      "2024-12-23 03:40:50.924025: Pseudo dice [np.float32(0.7341)]\n",
      "2024-12-23 03:40:50.924073: Epoch time: 111.25 s\n",
      "2024-12-23 03:40:51.486956: \n",
      "2024-12-23 03:40:51.487235: Epoch 124\n",
      "2024-12-23 03:40:51.487344: Current learning rate: 0.00888\n",
      "2024-12-23 03:42:42.722232: train_loss -0.6956\n",
      "2024-12-23 03:42:42.722372: val_loss -0.666\n",
      "2024-12-23 03:42:42.722406: Pseudo dice [np.float32(0.718)]\n",
      "2024-12-23 03:42:42.722445: Epoch time: 111.24 s\n",
      "2024-12-23 03:42:43.297097: \n",
      "2024-12-23 03:42:43.297210: Epoch 125\n",
      "2024-12-23 03:42:43.297283: Current learning rate: 0.00887\n",
      "2024-12-23 03:44:34.541118: train_loss -0.7121\n",
      "2024-12-23 03:44:34.541327: val_loss -0.6863\n",
      "2024-12-23 03:44:34.541364: Pseudo dice [np.float32(0.7883)]\n",
      "2024-12-23 03:44:34.541436: Epoch time: 111.24 s\n",
      "2024-12-23 03:44:35.101466: \n",
      "2024-12-23 03:44:35.101701: Epoch 126\n",
      "2024-12-23 03:44:35.101786: Current learning rate: 0.00886\n",
      "2024-12-23 03:46:26.311693: train_loss -0.6955\n",
      "2024-12-23 03:46:26.311839: val_loss -0.7037\n",
      "2024-12-23 03:46:26.311937: Pseudo dice [np.float32(0.7643)]\n",
      "2024-12-23 03:46:26.311983: Epoch time: 111.21 s\n",
      "2024-12-23 03:46:26.876523: \n",
      "2024-12-23 03:46:26.876664: Epoch 127\n",
      "2024-12-23 03:46:26.876734: Current learning rate: 0.00885\n",
      "2024-12-23 03:48:18.127904: train_loss -0.6747\n",
      "2024-12-23 03:48:18.128159: val_loss -0.6866\n",
      "2024-12-23 03:48:18.128336: Pseudo dice [np.float32(0.7672)]\n",
      "2024-12-23 03:48:18.128410: Epoch time: 111.25 s\n",
      "2024-12-23 03:48:18.697811: \n",
      "2024-12-23 03:48:18.697994: Epoch 128\n",
      "2024-12-23 03:48:18.698071: Current learning rate: 0.00884\n",
      "2024-12-23 03:50:09.976309: train_loss -0.6369\n",
      "2024-12-23 03:50:09.976703: val_loss -0.6206\n",
      "2024-12-23 03:50:09.976769: Pseudo dice [np.float32(0.7043)]\n",
      "2024-12-23 03:50:09.976814: Epoch time: 111.28 s\n",
      "2024-12-23 03:50:10.544959: \n",
      "2024-12-23 03:50:10.545055: Epoch 129\n",
      "2024-12-23 03:50:10.545128: Current learning rate: 0.00883\n",
      "2024-12-23 03:52:01.832043: train_loss -0.6916\n",
      "2024-12-23 03:52:01.832196: val_loss -0.6773\n",
      "2024-12-23 03:52:01.832229: Pseudo dice [np.float32(0.7189)]\n",
      "2024-12-23 03:52:01.832266: Epoch time: 111.29 s\n",
      "2024-12-23 03:52:02.389833: \n",
      "2024-12-23 03:52:02.389988: Epoch 130\n",
      "2024-12-23 03:52:02.390062: Current learning rate: 0.00882\n",
      "2024-12-23 03:53:53.617230: train_loss -0.6877\n",
      "2024-12-23 03:53:53.617373: val_loss -0.6104\n",
      "2024-12-23 03:53:53.617406: Pseudo dice [np.float32(0.6591)]\n",
      "2024-12-23 03:53:53.617442: Epoch time: 111.23 s\n",
      "2024-12-23 03:53:54.186317: \n",
      "2024-12-23 03:53:54.186415: Epoch 131\n",
      "2024-12-23 03:53:54.186485: Current learning rate: 0.00881\n",
      "2024-12-23 03:55:45.462034: train_loss -0.6877\n",
      "2024-12-23 03:55:45.462181: val_loss -0.6696\n",
      "2024-12-23 03:55:45.462217: Pseudo dice [np.float32(0.7221)]\n",
      "2024-12-23 03:55:45.462253: Epoch time: 111.28 s\n",
      "2024-12-23 03:55:46.469687: \n",
      "2024-12-23 03:55:46.469864: Epoch 132\n",
      "2024-12-23 03:55:46.469958: Current learning rate: 0.0088\n",
      "2024-12-23 03:57:37.777592: train_loss -0.7365\n",
      "2024-12-23 03:57:37.777874: val_loss -0.7116\n",
      "2024-12-23 03:57:37.778003: Pseudo dice [np.float32(0.7797)]\n",
      "2024-12-23 03:57:37.778074: Epoch time: 111.31 s\n",
      "2024-12-23 03:57:38.346173: \n",
      "2024-12-23 03:57:38.346365: Epoch 133\n",
      "2024-12-23 03:57:38.346441: Current learning rate: 0.00879\n",
      "2024-12-23 03:59:29.605574: train_loss -0.7308\n",
      "2024-12-23 03:59:29.605723: val_loss -0.6163\n",
      "2024-12-23 03:59:29.605759: Pseudo dice [np.float32(0.7284)]\n",
      "2024-12-23 03:59:29.605794: Epoch time: 111.26 s\n",
      "2024-12-23 03:59:30.175929: \n",
      "2024-12-23 03:59:30.176050: Epoch 134\n",
      "2024-12-23 03:59:30.176120: Current learning rate: 0.00879\n",
      "2024-12-23 04:01:21.468719: train_loss -0.6841\n",
      "2024-12-23 04:01:21.468877: val_loss -0.6348\n",
      "2024-12-23 04:01:21.468915: Pseudo dice [np.float32(0.7086)]\n",
      "2024-12-23 04:01:21.468954: Epoch time: 111.29 s\n",
      "2024-12-23 04:01:22.041434: \n",
      "2024-12-23 04:01:22.041632: Epoch 135\n",
      "Current learning rate: 0.00878\n",
      "2024-12-23 04:03:13.361664: train_loss -0.6737\n",
      "2024-12-23 04:03:13.362145: val_loss -0.6306\n",
      "2024-12-23 04:03:13.362203: Pseudo dice [np.float32(0.6946)]\n",
      "2024-12-23 04:03:13.362248: Epoch time: 111.32 s\n",
      "2024-12-23 04:03:13.940971: \n",
      "2024-12-23 04:03:13.941201: Epoch 136\n",
      "2024-12-23 04:03:13.941279: Current learning rate: 0.00877\n",
      "2024-12-23 04:05:05.233600: train_loss -0.7025\n",
      "2024-12-23 04:05:05.233761: val_loss -0.6621\n",
      "2024-12-23 04:05:05.233816: Pseudo dice [np.float32(0.7358)]\n",
      "2024-12-23 04:05:05.233855: Epoch time: 111.29 s\n",
      "2024-12-23 04:05:05.816705: \n",
      "2024-12-23 04:05:05.816831: Epoch 137\n",
      "2024-12-23 04:05:05.816901: Current learning rate: 0.00876\n",
      "2024-12-23 04:06:57.144730: train_loss -0.6661\n",
      "2024-12-23 04:06:57.144882: val_loss -0.6821\n",
      "2024-12-23 04:06:57.144920: Pseudo dice [np.float32(0.744)]\n",
      "2024-12-23 04:06:57.144957: Epoch time: 111.33 s\n",
      "2024-12-23 04:06:57.720613: \n",
      "2024-12-23 04:06:57.720738: Epoch 138\n",
      "2024-12-23 04:06:57.720812: Current learning rate: 0.00875\n",
      "2024-12-23 04:08:49.052404: train_loss -0.6625\n",
      "2024-12-23 04:08:49.052709: val_loss -0.6013\n",
      "2024-12-23 04:08:49.052942: Pseudo dice [np.float32(0.6363)]\n",
      "2024-12-23 04:08:49.053059: Epoch time: 111.33 s\n",
      "2024-12-23 04:08:49.639874: \n",
      "2024-12-23 04:08:49.640002: Epoch 139\n",
      "2024-12-23 04:08:49.640075: Current learning rate: 0.00874\n",
      "2024-12-23 04:10:40.936015: train_loss -0.7129\n",
      "2024-12-23 04:10:40.936170: val_loss -0.6917\n",
      "2024-12-23 04:10:40.936207: Pseudo dice [np.float32(0.783)]\n",
      "2024-12-23 04:10:40.936245: Epoch time: 111.3 s\n",
      "2024-12-23 04:10:41.509496: \n",
      "2024-12-23 04:10:41.509943: Epoch 140\n",
      "2024-12-23 04:10:41.510095: Current learning rate: 0.00873\n",
      "2024-12-23 04:12:32.778431: train_loss -0.7083\n",
      "2024-12-23 04:12:32.778575: val_loss -0.6728\n",
      "2024-12-23 04:12:32.778610: Pseudo dice [np.float32(0.7283)]\n",
      "2024-12-23 04:12:32.778646: Epoch time: 111.27 s\n",
      "2024-12-23 04:12:33.354876: \n",
      "2024-12-23 04:12:33.354993: Epoch 141\n",
      "2024-12-23 04:12:33.355064: Current learning rate: 0.00872\n",
      "2024-12-23 04:14:24.598121: train_loss -0.7258\n",
      "2024-12-23 04:14:24.598278: val_loss -0.7193\n",
      "2024-12-23 04:14:24.598314: Pseudo dice [np.float32(0.789)]\n",
      "2024-12-23 04:14:24.598353: Epoch time: 111.24 s\n",
      "2024-12-23 04:14:25.178227: \n",
      "2024-12-23 04:14:25.178344: Epoch 142\n",
      "2024-12-23 04:14:25.178419: Current learning rate: 0.00871\n",
      "2024-12-23 04:16:16.461360: train_loss -0.7241\n",
      "2024-12-23 04:16:16.461601: val_loss -0.6943\n",
      "2024-12-23 04:16:16.461637: Pseudo dice [np.float32(0.7464)]\n",
      "2024-12-23 04:16:16.461675: Epoch time: 111.28 s\n",
      "2024-12-23 04:16:17.038953: \n",
      "2024-12-23 04:16:17.039296: Epoch 143\n",
      "2024-12-23 04:16:17.039382: Current learning rate: 0.0087\n",
      "2024-12-23 04:18:08.330699: train_loss -0.7319\n",
      "2024-12-23 04:18:08.330847: val_loss -0.7185\n",
      "2024-12-23 04:18:08.330885: Pseudo dice [np.float32(0.7514)]\n",
      "2024-12-23 04:18:08.330923: Epoch time: 111.29 s\n",
      "2024-12-23 04:18:08.903148: \n",
      "2024-12-23 04:18:08.903321: Epoch 144\n",
      "2024-12-23 04:18:08.903392: Current learning rate: 0.00869\n",
      "2024-12-23 04:20:00.178992: train_loss -0.6821\n",
      "2024-12-23 04:20:00.179151: val_loss -0.6981\n",
      "2024-12-23 04:20:00.179192: Pseudo dice [np.float32(0.7284)]\n",
      "2024-12-23 04:20:00.179235: Epoch time: 111.28 s\n",
      "2024-12-23 04:20:00.752835: \n",
      "2024-12-23 04:20:00.752933: Epoch 145\n",
      "2024-12-23 04:20:00.753006: Current learning rate: 0.00868\n",
      "2024-12-23 04:21:52.013377: train_loss -0.7015\n",
      "2024-12-23 04:21:52.013519: val_loss -0.6895\n",
      "2024-12-23 04:21:52.013554: Pseudo dice [np.float32(0.7778)]\n",
      "2024-12-23 04:21:52.013591: Epoch time: 111.26 s\n",
      "2024-12-23 04:21:52.649389: \n",
      "2024-12-23 04:21:52.649508: Epoch 146\n",
      "2024-12-23 04:21:52.649587: Current learning rate: 0.00868\n",
      "2024-12-23 04:23:43.910983: train_loss -0.6954\n",
      "2024-12-23 04:23:43.911116: val_loss -0.6363\n",
      "2024-12-23 04:23:43.911149: Pseudo dice [np.float32(0.6596)]\n",
      "2024-12-23 04:23:43.911183: Epoch time: 111.26 s\n",
      "2024-12-23 04:23:44.483460: \n",
      "2024-12-23 04:23:44.483553: Epoch 147\n",
      "2024-12-23 04:23:44.483629: Current learning rate: 0.00867\n",
      "2024-12-23 04:25:35.738400: train_loss -0.7174\n",
      "2024-12-23 04:25:35.738541: val_loss -0.6999\n",
      "2024-12-23 04:25:35.738574: Pseudo dice [np.float32(0.7066)]\n",
      "2024-12-23 04:25:35.738611: Epoch time: 111.26 s\n",
      "2024-12-23 04:25:36.313540: \n",
      "2024-12-23 04:25:36.313639: Epoch 148\n",
      "2024-12-23 04:25:36.313708: Current learning rate: 0.00866\n",
      "2024-12-23 04:27:27.592797: train_loss -0.68\n",
      "2024-12-23 04:27:27.592929: val_loss -0.6803\n",
      "2024-12-23 04:27:27.592962: Pseudo dice [np.float32(0.7227)]\n",
      "2024-12-23 04:27:27.593000: Epoch time: 111.28 s\n",
      "2024-12-23 04:27:28.176556: \n",
      "2024-12-23 04:27:28.176905: Epoch 149\n",
      "2024-12-23 04:27:28.177088: Current learning rate: 0.00865\n",
      "2024-12-23 04:29:19.451250: train_loss -0.6752\n",
      "2024-12-23 04:29:19.451392: val_loss -0.6502\n",
      "2024-12-23 04:29:19.451428: Pseudo dice [np.float32(0.707)]\n",
      "2024-12-23 04:29:19.451464: Epoch time: 111.28 s\n",
      "2024-12-23 04:29:20.700115: \n",
      "2024-12-23 04:29:20.700323: Epoch 150\n",
      "2024-12-23 04:29:20.700411: Current learning rate: 0.00864\n",
      "2024-12-23 04:31:11.942581: train_loss -0.7108\n",
      "2024-12-23 04:31:11.942807: val_loss -0.7174\n",
      "2024-12-23 04:31:11.942842: Pseudo dice [np.float32(0.7987)]\n",
      "2024-12-23 04:31:11.942877: Epoch time: 111.24 s\n",
      "2024-12-23 04:31:12.530636: \n",
      "2024-12-23 04:31:12.530782: Epoch 151\n",
      "2024-12-23 04:31:12.530855: Current learning rate: 0.00863\n",
      "2024-12-23 04:33:03.843313: train_loss -0.6882\n",
      "2024-12-23 04:33:03.843477: val_loss -0.6478\n",
      "2024-12-23 04:33:03.843511: Pseudo dice [np.float32(0.7425)]\n",
      "2024-12-23 04:33:03.843548: Epoch time: 111.31 s\n",
      "2024-12-23 04:33:04.423751: \n",
      "2024-12-23 04:33:04.424001: Epoch 152\n",
      "2024-12-23 04:33:04.424089: Current learning rate: 0.00862\n",
      "2024-12-23 04:34:55.698412: train_loss -0.6518\n",
      "2024-12-23 04:34:55.698568: val_loss -0.6366\n",
      "2024-12-23 04:34:55.698603: Pseudo dice [np.float32(0.7488)]\n",
      "2024-12-23 04:34:55.698639: Epoch time: 111.28 s\n",
      "2024-12-23 04:34:56.281690: \n",
      "2024-12-23 04:34:56.281817: Epoch 153\n",
      "2024-12-23 04:34:56.281974: Current learning rate: 0.00861\n",
      "2024-12-23 04:36:47.533346: train_loss -0.6669\n",
      "2024-12-23 04:36:47.533502: val_loss -0.6892\n",
      "2024-12-23 04:36:47.533536: Pseudo dice [np.float32(0.7528)]\n",
      "2024-12-23 04:36:47.533573: Epoch time: 111.25 s\n",
      "2024-12-23 04:36:48.131262: \n",
      "2024-12-23 04:36:48.131467: Epoch 154\n",
      "2024-12-23 04:36:48.131624: Current learning rate: 0.0086\n",
      "2024-12-23 04:38:39.364535: train_loss -0.7069\n",
      "2024-12-23 04:38:39.364742: val_loss -0.6788\n",
      "2024-12-23 04:38:39.364774: Pseudo dice [np.float32(0.7206)]\n",
      "2024-12-23 04:38:39.364811: Epoch time: 111.23 s\n",
      "2024-12-23 04:38:39.947367: \n",
      "2024-12-23 04:38:39.947894: Epoch 155\n",
      "2024-12-23 04:38:39.948000: Current learning rate: 0.00859\n",
      "2024-12-23 04:40:31.211102: train_loss -0.7006\n",
      "2024-12-23 04:40:31.211255: val_loss -0.6507\n",
      "2024-12-23 04:40:31.211288: Pseudo dice [np.float32(0.7339)]\n",
      "2024-12-23 04:40:31.211324: Epoch time: 111.26 s\n",
      "2024-12-23 04:40:31.806146: \n",
      "2024-12-23 04:40:31.806283: Epoch 156\n",
      "2024-12-23 04:40:31.806352: Current learning rate: 0.00858\n",
      "2024-12-23 04:42:23.054393: train_loss -0.6947\n",
      "2024-12-23 04:42:23.054719: val_loss -0.6925\n",
      "2024-12-23 04:42:23.054781: Pseudo dice [np.float32(0.729)]\n",
      "2024-12-23 04:42:23.054823: Epoch time: 111.25 s\n",
      "2024-12-23 04:42:23.641410: \n",
      "2024-12-23 04:42:23.641813: Epoch 157\n",
      "2024-12-23 04:42:23.641930: Current learning rate: 0.00858\n",
      "2024-12-23 04:44:14.882164: train_loss -0.6929\n",
      "2024-12-23 04:44:14.882374: val_loss -0.7286\n",
      "2024-12-23 04:44:14.882417: Pseudo dice [np.float32(0.7723)]\n",
      "2024-12-23 04:44:14.882455: Epoch time: 111.24 s\n",
      "2024-12-23 04:44:15.463006: \n",
      "2024-12-23 04:44:15.463403: Epoch 158\n",
      "2024-12-23 04:44:15.463612: Current learning rate: 0.00857\n",
      "2024-12-23 04:46:06.677771: train_loss -0.6914\n",
      "2024-12-23 04:46:06.678000: val_loss -0.5305\n",
      "2024-12-23 04:46:06.678037: Pseudo dice [np.float32(0.6238)]\n",
      "2024-12-23 04:46:06.678074: Epoch time: 111.22 s\n",
      "2024-12-23 04:46:07.269198: \n",
      "2024-12-23 04:46:07.269315: Epoch 159\n",
      "2024-12-23 04:46:07.269389: Current learning rate: 0.00856\n",
      "2024-12-23 04:47:58.532209: train_loss -0.6919\n",
      "2024-12-23 04:47:58.532359: val_loss -0.565\n",
      "2024-12-23 04:47:58.532392: Pseudo dice [np.float32(0.6585)]\n",
      "2024-12-23 04:47:58.532429: Epoch time: 111.26 s\n",
      "2024-12-23 04:47:59.119445: \n",
      "2024-12-23 04:47:59.119550: Epoch 160\n",
      "Current learning rate: 0.00855\n",
      "2024-12-23 04:49:50.375276: train_loss -0.7095\n",
      "2024-12-23 04:49:50.375431: val_loss -0.6734\n",
      "2024-12-23 04:49:50.375468: Pseudo dice [np.float32(0.7711)]\n",
      "2024-12-23 04:49:50.375505: Epoch time: 111.26 s\n",
      "2024-12-23 04:49:50.971025: \n",
      "2024-12-23 04:49:50.971129: Epoch 161\n",
      "2024-12-23 04:49:50.971201: Current learning rate: 0.00854\n",
      "2024-12-23 04:51:42.234936: train_loss -0.7157\n",
      "2024-12-23 04:51:42.235085: val_loss -0.6507\n",
      "2024-12-23 04:51:42.235121: Pseudo dice [np.float32(0.6916)]\n",
      "2024-12-23 04:51:42.235242: Epoch time: 111.26 s\n",
      "2024-12-23 04:51:42.817468: \n",
      "2024-12-23 04:51:42.817660: Epoch 162\n",
      "2024-12-23 04:51:42.817740: Current learning rate: 0.00853\n",
      "2024-12-23 04:53:34.077110: train_loss -0.7334\n",
      "2024-12-23 04:53:34.077302: val_loss -0.712\n",
      "2024-12-23 04:53:34.077338: Pseudo dice [np.float32(0.7261)]\n",
      "2024-12-23 04:53:34.077374: Epoch time: 111.26 s\n",
      "2024-12-23 04:53:34.671696: \n",
      "2024-12-23 04:53:34.671794: Epoch 163\n",
      "2024-12-23 04:53:34.671868: Current learning rate: 0.00852\n",
      "2024-12-23 04:55:25.902411: train_loss -0.7248\n",
      "2024-12-23 04:55:25.902566: val_loss -0.6039\n",
      "2024-12-23 04:55:25.902601: Pseudo dice [np.float32(0.7182)]\n",
      "2024-12-23 04:55:25.902637: Epoch time: 111.23 s\n",
      "2024-12-23 04:55:26.495946: \n",
      "2024-12-23 04:55:26.496045: Epoch 164\n",
      "2024-12-23 04:55:26.496118: Current learning rate: 0.00851\n",
      "2024-12-23 04:57:17.730632: train_loss -0.7197\n",
      "2024-12-23 04:57:17.730798: val_loss -0.7256\n",
      "2024-12-23 04:57:17.730835: Pseudo dice [np.float32(0.7877)]\n",
      "2024-12-23 04:57:17.730871: Epoch time: 111.24 s\n",
      "2024-12-23 04:57:18.314128: \n",
      "2024-12-23 04:57:18.314437: Epoch 165\n",
      "2024-12-23 04:57:18.314625: Current learning rate: 0.0085\n",
      "2024-12-23 04:59:09.517253: train_loss -0.6868\n",
      "2024-12-23 04:59:09.517404: val_loss -0.6534\n",
      "2024-12-23 04:59:09.517444: Pseudo dice [np.float32(0.6655)]\n",
      "2024-12-23 04:59:09.517479: Epoch time: 111.2 s\n",
      "2024-12-23 04:59:10.527719: \n",
      "2024-12-23 04:59:10.528029: Epoch 166\n",
      "2024-12-23 04:59:10.528116: Current learning rate: 0.00849\n",
      "2024-12-23 05:01:01.817230: train_loss -0.6817\n",
      "2024-12-23 05:01:01.817424: val_loss -0.5878\n",
      "2024-12-23 05:01:01.817566: Pseudo dice [np.float32(0.6694)]\n",
      "2024-12-23 05:01:01.817721: Epoch time: 111.29 s\n",
      "2024-12-23 05:01:02.400097: \n",
      "2024-12-23 05:01:02.400236: Epoch 167\n",
      "2024-12-23 05:01:02.400310: Current learning rate: 0.00848\n",
      "2024-12-23 05:02:53.676846: train_loss -0.6446\n",
      "2024-12-23 05:02:53.677001: val_loss -0.6887\n",
      "2024-12-23 05:02:53.677037: Pseudo dice [np.float32(0.7428)]\n",
      "2024-12-23 05:02:53.677073: Epoch time: 111.28 s\n",
      "2024-12-23 05:02:54.273491: \n",
      "2024-12-23 05:02:54.273622: Epoch 168\n",
      "2024-12-23 05:02:54.273694: Current learning rate: 0.00847\n",
      "2024-12-23 05:04:45.505933: train_loss -0.6769\n",
      "2024-12-23 05:04:45.506083: val_loss -0.6975\n",
      "2024-12-23 05:04:45.506118: Pseudo dice [np.float32(0.7797)]\n",
      "2024-12-23 05:04:45.506154: Epoch time: 111.23 s\n",
      "2024-12-23 05:04:46.097665: \n",
      "2024-12-23 05:04:46.097897: Epoch 169\n",
      "2024-12-23 05:04:46.098165: Current learning rate: 0.00847\n",
      "2024-12-23 05:06:37.351722: train_loss -0.6872\n",
      "2024-12-23 05:06:37.351983: val_loss -0.6977\n",
      "2024-12-23 05:06:37.352021: Pseudo dice [np.float32(0.727)]\n",
      "2024-12-23 05:06:37.352057: Epoch time: 111.25 s\n",
      "2024-12-23 05:06:37.940026: \n",
      "2024-12-23 05:06:37.940410: Epoch 170\n",
      "2024-12-23 05:06:37.940587: Current learning rate: 0.00846\n",
      "2024-12-23 05:08:29.184415: train_loss -0.6927\n",
      "2024-12-23 05:08:29.185040: val_loss -0.6461\n",
      "2024-12-23 05:08:29.185208: Pseudo dice [np.float32(0.7339)]\n",
      "2024-12-23 05:08:29.185287: Epoch time: 111.24 s\n",
      "2024-12-23 05:08:29.773094: \n",
      "2024-12-23 05:08:29.773249: Epoch 171\n",
      "2024-12-23 05:08:29.773323: Current learning rate: 0.00845\n",
      "2024-12-23 05:10:20.972832: train_loss -0.7103\n",
      "2024-12-23 05:10:20.972976: val_loss -0.7094\n",
      "2024-12-23 05:10:20.973018: Pseudo dice [np.float32(0.7833)]\n",
      "2024-12-23 05:10:20.973054: Epoch time: 111.2 s\n",
      "2024-12-23 05:10:21.570923: \n",
      "2024-12-23 05:10:21.571305: Epoch 172\n",
      "2024-12-23 05:10:21.571493: Current learning rate: 0.00844\n",
      "2024-12-23 05:12:12.773964: train_loss -0.6522\n",
      "2024-12-23 05:12:12.774090: val_loss -0.7148\n",
      "2024-12-23 05:12:12.774123: Pseudo dice [np.float32(0.7984)]\n",
      "2024-12-23 05:12:12.774161: Epoch time: 111.2 s\n",
      "2024-12-23 05:12:13.369456: \n",
      "2024-12-23 05:12:13.369875: Epoch 173\n",
      "2024-12-23 05:12:13.369968: Current learning rate: 0.00843\n",
      "2024-12-23 05:14:04.639144: train_loss -0.695\n",
      "2024-12-23 05:14:04.639410: val_loss -0.6166\n",
      "2024-12-23 05:14:04.639459: Pseudo dice [np.float32(0.6754)]\n",
      "2024-12-23 05:14:04.639500: Epoch time: 111.27 s\n",
      "2024-12-23 05:14:05.220200: \n",
      "2024-12-23 05:14:05.220608: Epoch 174\n",
      "2024-12-23 05:14:05.220822: Current learning rate: 0.00842\n",
      "2024-12-23 05:15:56.466420: train_loss -0.6882\n",
      "2024-12-23 05:15:56.466560: val_loss -0.6514\n",
      "2024-12-23 05:15:56.466594: Pseudo dice [np.float32(0.6992)]\n",
      "2024-12-23 05:15:56.466629: Epoch time: 111.25 s\n",
      "2024-12-23 05:15:57.061621: \n",
      "2024-12-23 05:15:57.061856: Epoch 175\n",
      "2024-12-23 05:15:57.061941: Current learning rate: 0.00841\n",
      "2024-12-23 05:17:48.286553: train_loss -0.6859\n",
      "2024-12-23 05:17:48.286694: val_loss -0.6028\n",
      "2024-12-23 05:17:48.286728: Pseudo dice [np.float32(0.6527)]\n",
      "2024-12-23 05:17:48.286856: Epoch time: 111.23 s\n",
      "2024-12-23 05:17:48.879874: \n",
      "2024-12-23 05:17:48.880002: Epoch 176\n",
      "2024-12-23 05:17:48.880077: Current learning rate: 0.0084\n",
      "2024-12-23 05:19:40.114203: train_loss -0.7026\n",
      "2024-12-23 05:19:40.114356: val_loss -0.6114\n",
      "2024-12-23 05:19:40.114408: Pseudo dice [np.float32(0.6951)]\n",
      "2024-12-23 05:19:40.114448: Epoch time: 111.23 s\n",
      "2024-12-23 05:19:40.698906: \n",
      "2024-12-23 05:19:40.699016: Epoch 177\n",
      "2024-12-23 05:19:40.699096: Current learning rate: 0.00839\n",
      "2024-12-23 05:21:31.994244: train_loss -0.7032\n",
      "2024-12-23 05:21:31.994392: val_loss -0.6208\n",
      "2024-12-23 05:21:31.994427: Pseudo dice [np.float32(0.679)]\n",
      "2024-12-23 05:21:31.994465: Epoch time: 111.3 s\n",
      "2024-12-23 05:21:32.583927: \n",
      "2024-12-23 05:21:32.584142: Epoch 178\n",
      "2024-12-23 05:21:32.584267: Current learning rate: 0.00838\n",
      "2024-12-23 05:23:23.846227: train_loss -0.7092\n",
      "2024-12-23 05:23:23.846476: val_loss -0.7719\n",
      "2024-12-23 05:23:23.846656: Pseudo dice [np.float32(0.8124)]\n",
      "2024-12-23 05:23:23.846731: Epoch time: 111.26 s\n",
      "2024-12-23 05:23:24.438986: \n",
      "2024-12-23 05:23:24.439469: Epoch 179\n",
      "2024-12-23 05:23:24.439577: Current learning rate: 0.00837\n",
      "2024-12-23 05:25:15.638590: train_loss -0.6785\n",
      "2024-12-23 05:25:15.638800: val_loss -0.6434\n",
      "2024-12-23 05:25:15.638834: Pseudo dice [np.float32(0.7519)]\n",
      "2024-12-23 05:25:15.638870: Epoch time: 111.2 s\n",
      "2024-12-23 05:25:16.232714: \n",
      "2024-12-23 05:25:16.233107: Epoch 180\n",
      "2024-12-23 05:25:16.233322: Current learning rate: 0.00836\n",
      "2024-12-23 05:27:07.468194: train_loss -0.706\n",
      "2024-12-23 05:27:07.468352: val_loss -0.7404\n",
      "2024-12-23 05:27:07.468388: Pseudo dice [np.float32(0.7835)]\n",
      "2024-12-23 05:27:07.468426: Epoch time: 111.24 s\n",
      "2024-12-23 05:27:08.058465: \n",
      "2024-12-23 05:27:08.058570: Epoch 181\n",
      "2024-12-23 05:27:08.058643: Current learning rate: 0.00836\n",
      "2024-12-23 05:28:59.385477: train_loss -0.699\n",
      "2024-12-23 05:28:59.385686: val_loss -0.6896\n",
      "2024-12-23 05:28:59.385720: Pseudo dice [np.float32(0.7569)]\n",
      "2024-12-23 05:28:59.385755: Epoch time: 111.33 s\n",
      "2024-12-23 05:28:59.978099: \n",
      "2024-12-23 05:28:59.978199: Epoch 182\n",
      "2024-12-23 05:28:59.978271: Current learning rate: 0.00835\n",
      "2024-12-23 05:30:51.174687: train_loss -0.7281\n",
      "2024-12-23 05:30:51.174937: val_loss -0.7564\n",
      "2024-12-23 05:30:51.174984: Pseudo dice [np.float32(0.8285)]\n",
      "2024-12-23 05:30:51.175023: Epoch time: 111.2 s\n",
      "2024-12-23 05:30:52.195568: \n",
      "2024-12-23 05:30:52.195894: Epoch 183\n",
      "2024-12-23 05:30:52.196069: Current learning rate: 0.00834\n",
      "2024-12-23 05:32:43.438461: train_loss -0.6526\n",
      "2024-12-23 05:32:43.438626: val_loss -0.6563\n",
      "2024-12-23 05:32:43.438721: Pseudo dice [np.float32(0.7424)]\n",
      "2024-12-23 05:32:43.438870: Epoch time: 111.24 s\n",
      "2024-12-23 05:32:44.028987: \n",
      "2024-12-23 05:32:44.029367: Epoch 184\n",
      "2024-12-23 05:32:44.029606: Current learning rate: 0.00833\n",
      "2024-12-23 05:34:35.300153: train_loss -0.6961\n",
      "2024-12-23 05:34:35.300490: val_loss -0.6945\n",
      "2024-12-23 05:34:35.300822: Pseudo dice [np.float32(0.766)]\n",
      "2024-12-23 05:34:35.300898: Epoch time: 111.27 s\n",
      "2024-12-23 05:34:35.894881: \n",
      "2024-12-23 05:34:35.895286: Epoch 185\n",
      "Current learning rate: 0.00832\n",
      "2024-12-23 05:36:27.139239: train_loss -0.7042\n",
      "2024-12-23 05:36:27.139398: val_loss -0.7037\n",
      "2024-12-23 05:36:27.139433: Pseudo dice [np.float32(0.745)]\n",
      "2024-12-23 05:36:27.139472: Epoch time: 111.24 s\n",
      "2024-12-23 05:36:27.732652: \n",
      "2024-12-23 05:36:27.732920: Epoch 186\n",
      "2024-12-23 05:36:27.733049: Current learning rate: 0.00831\n",
      "2024-12-23 05:38:18.967124: train_loss -0.7002\n",
      "2024-12-23 05:38:18.967266: val_loss -0.6475\n",
      "2024-12-23 05:38:18.967301: Pseudo dice [np.float32(0.6275)]\n",
      "2024-12-23 05:38:18.967338: Epoch time: 111.24 s\n",
      "2024-12-23 05:38:19.564349: \n",
      "2024-12-23 05:38:19.564505: Epoch 187\n",
      "2024-12-23 05:38:19.564613: Current learning rate: 0.0083\n",
      "2024-12-23 05:40:10.845794: train_loss -0.7267\n",
      "2024-12-23 05:40:10.845940: val_loss -0.6541\n",
      "2024-12-23 05:40:10.846075: Pseudo dice [np.float32(0.7163)]\n",
      "2024-12-23 05:40:10.846122: Epoch time: 111.28 s\n",
      "2024-12-23 05:40:11.443879: \n",
      "2024-12-23 05:40:11.444029: Epoch 188\n",
      "2024-12-23 05:40:11.444106: Current learning rate: 0.00829\n",
      "2024-12-23 05:42:02.730435: train_loss -0.7278\n",
      "2024-12-23 05:42:02.730592: val_loss -0.7429\n",
      "2024-12-23 05:42:02.730626: Pseudo dice [np.float32(0.8077)]\n",
      "2024-12-23 05:42:02.730664: Epoch time: 111.29 s\n",
      "2024-12-23 05:42:03.337566: \n",
      "2024-12-23 05:42:03.337954: Epoch 189\n",
      "2024-12-23 05:42:03.338117: Current learning rate: 0.00828\n",
      "2024-12-23 05:43:54.558320: train_loss -0.7377\n",
      "2024-12-23 05:43:54.558564: val_loss -0.7143\n",
      "2024-12-23 05:43:54.558600: Pseudo dice [np.float32(0.7616)]\n",
      "2024-12-23 05:43:54.558636: Epoch time: 111.22 s\n",
      "2024-12-23 05:43:55.163226: \n",
      "2024-12-23 05:43:55.163356: Epoch 190\n",
      "2024-12-23 05:43:55.163431: Current learning rate: 0.00827\n",
      "2024-12-23 05:45:46.403671: train_loss -0.7355\n",
      "2024-12-23 05:45:46.403903: val_loss -0.6538\n",
      "2024-12-23 05:45:46.403944: Pseudo dice [np.float32(0.7232)]\n",
      "2024-12-23 05:45:46.403984: Epoch time: 111.24 s\n",
      "2024-12-23 05:45:46.995767: \n",
      "2024-12-23 05:45:46.996199: Epoch 191\n",
      "2024-12-23 05:45:46.996309: Current learning rate: 0.00826\n",
      "2024-12-23 05:47:38.265968: train_loss -0.7599\n",
      "2024-12-23 05:47:38.266161: val_loss -0.7064\n",
      "2024-12-23 05:47:38.266238: Pseudo dice [np.float32(0.7864)]\n",
      "2024-12-23 05:47:38.266283: Epoch time: 111.27 s\n",
      "2024-12-23 05:47:38.874934: \n",
      "2024-12-23 05:47:38.875254: Epoch 192\n",
      "2024-12-23 05:47:38.875392: Current learning rate: 0.00825\n",
      "2024-12-23 05:49:30.124295: train_loss -0.7475\n",
      "2024-12-23 05:49:30.124433: val_loss -0.7389\n",
      "2024-12-23 05:49:30.124465: Pseudo dice [np.float32(0.801)]\n",
      "2024-12-23 05:49:30.124500: Epoch time: 111.25 s\n",
      "2024-12-23 05:49:30.124521: Yayy! New best EMA pseudo Dice: 0.7505999803543091\n",
      "2024-12-23 05:49:30.937350: \n",
      "2024-12-23 05:49:30.937570: Epoch 193\n",
      "2024-12-23 05:49:30.937653: Current learning rate: 0.00824\n",
      "2024-12-23 05:51:22.191089: train_loss -0.716\n",
      "2024-12-23 05:51:22.191227: val_loss -0.604\n",
      "2024-12-23 05:51:22.191260: Pseudo dice [np.float32(0.65)]\n",
      "2024-12-23 05:51:22.191296: Epoch time: 111.25 s\n",
      "2024-12-23 05:51:22.791531: \n",
      "2024-12-23 05:51:22.791927: Epoch 194\n",
      "2024-12-23 05:51:22.792086: Current learning rate: 0.00824\n",
      "2024-12-23 05:53:14.047139: train_loss -0.6864\n",
      "2024-12-23 05:53:14.047287: val_loss -0.6782\n",
      "2024-12-23 05:53:14.047321: Pseudo dice [np.float32(0.651)]\n",
      "2024-12-23 05:53:14.047357: Epoch time: 111.26 s\n",
      "2024-12-23 05:53:14.638022: \n",
      "2024-12-23 05:53:14.638144: Epoch 195\n",
      "2024-12-23 05:53:14.638228: Current learning rate: 0.00823\n",
      "2024-12-23 05:55:05.905580: train_loss -0.685\n",
      "2024-12-23 05:55:05.905718: val_loss -0.6869\n",
      "2024-12-23 05:55:05.905752: Pseudo dice [np.float32(0.7244)]\n",
      "2024-12-23 05:55:05.905788: Epoch time: 111.27 s\n",
      "2024-12-23 05:55:06.501297: \n",
      "2024-12-23 05:55:06.501405: Epoch 196\n",
      "2024-12-23 05:55:06.501475: Current learning rate: 0.00822\n",
      "2024-12-23 05:56:57.754192: train_loss -0.69\n",
      "2024-12-23 05:56:57.754392: val_loss -0.7087\n",
      "2024-12-23 05:56:57.754425: Pseudo dice [np.float32(0.7682)]\n",
      "2024-12-23 05:56:57.754462: Epoch time: 111.25 s\n",
      "2024-12-23 05:56:58.357592: \n",
      "2024-12-23 05:56:58.357800: Epoch 197\n",
      "2024-12-23 05:56:58.357880: Current learning rate: 0.00821\n",
      "2024-12-23 05:58:49.641590: train_loss -0.7396\n",
      "2024-12-23 05:58:49.641848: val_loss -0.7172\n",
      "2024-12-23 05:58:49.641896: Pseudo dice [np.float32(0.7822)]\n",
      "2024-12-23 05:58:49.641940: Epoch time: 111.28 s\n",
      "2024-12-23 05:58:50.244656: \n",
      "2024-12-23 05:58:50.244996: Epoch 198\n",
      "2024-12-23 05:58:50.245337: Current learning rate: 0.0082\n",
      "2024-12-23 06:00:41.461912: train_loss -0.7366\n",
      "2024-12-23 06:00:41.462178: val_loss -0.629\n",
      "2024-12-23 06:00:41.462268: Pseudo dice [np.float32(0.6943)]\n",
      "2024-12-23 06:00:41.462313: Epoch time: 111.22 s\n",
      "2024-12-23 06:00:42.062475: \n",
      "2024-12-23 06:00:42.062566: Epoch 199\n",
      "2024-12-23 06:00:42.062635: Current learning rate: 0.00819\n",
      "2024-12-23 06:02:33.359205: train_loss -0.733\n",
      "2024-12-23 06:02:33.359600: val_loss -0.6544\n",
      "2024-12-23 06:02:33.359661: Pseudo dice [np.float32(0.7147)]\n",
      "2024-12-23 06:02:33.359703: Epoch time: 111.3 s\n",
      "2024-12-23 06:02:34.662364: \n",
      "2024-12-23 06:02:34.662747: Epoch 200\n",
      "2024-12-23 06:02:34.662844: Current learning rate: 0.00818\n",
      "2024-12-23 06:04:25.908714: train_loss -0.6788\n",
      "2024-12-23 06:04:25.908967: val_loss -0.6954\n",
      "2024-12-23 06:04:25.909003: Pseudo dice [np.float32(0.7376)]\n",
      "2024-12-23 06:04:25.909038: Epoch time: 111.25 s\n",
      "2024-12-23 06:04:26.512764: \n",
      "2024-12-23 06:04:26.513217: Epoch 201\n",
      "2024-12-23 06:04:26.513470: Current learning rate: 0.00817\n",
      "2024-12-23 06:06:17.764286: train_loss -0.7243\n",
      "2024-12-23 06:06:17.764504: val_loss -0.6423\n",
      "2024-12-23 06:06:17.764690: Pseudo dice [np.float32(0.742)]\n",
      "2024-12-23 06:06:17.764788: Epoch time: 111.25 s\n",
      "2024-12-23 06:06:18.358150: \n",
      "2024-12-23 06:06:18.358435: Epoch 202\n",
      "2024-12-23 06:06:18.358686: Current learning rate: 0.00816\n",
      "2024-12-23 06:08:09.671821: train_loss -0.6778\n",
      "2024-12-23 06:08:09.671967: val_loss -0.7273\n",
      "2024-12-23 06:08:09.672003: Pseudo dice [np.float32(0.7975)]\n",
      "2024-12-23 06:08:09.672041: Epoch time: 111.31 s\n",
      "2024-12-23 06:08:10.260145: \n",
      "2024-12-23 06:08:10.260544: Epoch 203\n",
      "2024-12-23 06:08:10.260738: Current learning rate: 0.00815\n",
      "2024-12-23 06:10:01.567364: train_loss -0.7247\n",
      "2024-12-23 06:10:01.567515: val_loss -0.743\n",
      "2024-12-23 06:10:01.567927: Pseudo dice [np.float32(0.7818)]\n",
      "2024-12-23 06:10:01.568123: Epoch time: 111.31 s\n",
      "2024-12-23 06:10:02.156449: \n",
      "2024-12-23 06:10:02.156810: Epoch 204\n",
      "2024-12-23 06:10:02.156896: Current learning rate: 0.00814\n",
      "2024-12-23 06:11:53.434482: train_loss -0.6875\n",
      "2024-12-23 06:11:53.434629: val_loss -0.652\n",
      "2024-12-23 06:11:53.434664: Pseudo dice [np.float32(0.7211)]\n",
      "2024-12-23 06:11:53.434715: Epoch time: 111.28 s\n",
      "2024-12-23 06:11:54.035218: \n",
      "2024-12-23 06:11:54.035364: Epoch 205\n",
      "2024-12-23 06:11:54.035439: Current learning rate: 0.00813\n",
      "2024-12-23 06:13:45.314481: train_loss -0.7134\n",
      "2024-12-23 06:13:45.314630: val_loss -0.7141\n",
      "2024-12-23 06:13:45.314666: Pseudo dice [np.float32(0.7728)]\n",
      "2024-12-23 06:13:45.314778: Epoch time: 111.28 s\n",
      "2024-12-23 06:13:45.882312: \n",
      "2024-12-23 06:13:45.882448: Epoch 206\n",
      "2024-12-23 06:13:45.882521: Current learning rate: 0.00813\n",
      "2024-12-23 06:15:37.223341: train_loss -0.6674\n",
      "2024-12-23 06:15:37.223498: val_loss -0.6322\n",
      "2024-12-23 06:15:37.223533: Pseudo dice [np.float32(0.7361)]\n",
      "2024-12-23 06:15:37.223572: Epoch time: 111.34 s\n",
      "2024-12-23 06:15:37.801465: \n",
      "2024-12-23 06:15:37.801690: Epoch 207\n",
      "2024-12-23 06:15:37.801999: Current learning rate: 0.00812\n",
      "2024-12-23 06:17:29.101318: train_loss -0.7032\n",
      "2024-12-23 06:17:29.101827: val_loss -0.7062\n",
      "2024-12-23 06:17:29.101872: Pseudo dice [np.float32(0.8042)]\n",
      "2024-12-23 06:17:29.101911: Epoch time: 111.3 s\n",
      "2024-12-23 06:17:29.680014: \n",
      "2024-12-23 06:17:29.680139: Epoch 208\n",
      "2024-12-23 06:17:29.680268: Current learning rate: 0.00811\n",
      "2024-12-23 06:19:20.974780: train_loss -0.7139\n",
      "2024-12-23 06:19:20.974977: val_loss -0.7292\n",
      "2024-12-23 06:19:20.975048: Pseudo dice [np.float32(0.7775)]\n",
      "2024-12-23 06:19:20.975093: Epoch time: 111.3 s\n",
      "2024-12-23 06:19:20.975118: Yayy! New best EMA pseudo Dice: 0.7530999779701233\n",
      "2024-12-23 06:19:21.771065: \n",
      "2024-12-23 06:19:21.771205: Epoch 209\n",
      "2024-12-23 06:19:21.771280: Current learning rate: 0.0081\n",
      "2024-12-23 06:21:13.033149: train_loss -0.7252\n",
      "2024-12-23 06:21:13.033366: val_loss -0.6411\n",
      "2024-12-23 06:21:13.033430: Pseudo dice [np.float32(0.707)]\n",
      "2024-12-23 06:21:13.033539: Epoch time: 111.26 s\n",
      "2024-12-23 06:21:13.609211: \n",
      "2024-12-23 06:21:13.609341: Epoch 210\n",
      "2024-12-23 06:21:13.609421: Current learning rate: 0.00809\n",
      "2024-12-23 06:23:04.831779: train_loss -0.7462\n",
      "2024-12-23 06:23:04.831924: val_loss -0.7081\n",
      "2024-12-23 06:23:04.831959: Pseudo dice [np.float32(0.7692)]\n",
      "2024-12-23 06:23:04.831995: Epoch time: 111.22 s\n",
      "2024-12-23 06:23:05.412621: \n",
      "2024-12-23 06:23:05.412745: Epoch 211\n",
      "2024-12-23 06:23:05.412820: Current learning rate: 0.00808\n",
      "2024-12-23 06:24:56.655666: train_loss -0.7326\n",
      "2024-12-23 06:24:56.655809: val_loss -0.6694\n",
      "2024-12-23 06:24:56.655843: Pseudo dice [np.float32(0.739)]\n",
      "2024-12-23 06:24:56.655880: Epoch time: 111.24 s\n",
      "2024-12-23 06:24:57.234807: \n",
      "2024-12-23 06:24:57.235198: Epoch 212\n",
      "2024-12-23 06:24:57.235286: Current learning rate: 0.00807\n",
      "2024-12-23 06:26:48.495395: train_loss -0.7209\n",
      "2024-12-23 06:26:48.495544: val_loss -0.6595\n",
      "2024-12-23 06:26:48.495581: Pseudo dice [np.float32(0.733)]\n",
      "2024-12-23 06:26:48.495619: Epoch time: 111.26 s\n",
      "2024-12-23 06:26:49.069139: \n",
      "2024-12-23 06:26:49.069542: Epoch 213\n",
      "2024-12-23 06:26:49.069628: Current learning rate: 0.00806\n",
      "2024-12-23 06:28:40.401737: train_loss -0.7251\n",
      "2024-12-23 06:28:40.401934: val_loss -0.7406\n",
      "2024-12-23 06:28:40.402015: Pseudo dice [np.float32(0.8144)]\n",
      "2024-12-23 06:28:40.402060: Epoch time: 111.33 s\n",
      "2024-12-23 06:28:40.402084: Yayy! New best EMA pseudo Dice: 0.7544000148773193\n",
      "2024-12-23 06:28:41.254193: \n",
      "2024-12-23 06:28:41.254355: Epoch 214\n",
      "2024-12-23 06:28:41.254433: Current learning rate: 0.00805\n",
      "2024-12-23 06:30:32.500954: train_loss -0.7121\n",
      "2024-12-23 06:30:32.501272: val_loss -0.6519\n",
      "2024-12-23 06:30:32.501329: Pseudo dice [np.float32(0.7595)]\n",
      "2024-12-23 06:30:32.501374: Epoch time: 111.25 s\n",
      "2024-12-23 06:30:32.501397: Yayy! New best EMA pseudo Dice: 0.7548999786376953\n",
      "2024-12-23 06:30:33.317466: \n",
      "2024-12-23 06:30:33.317772: Epoch 215\n",
      "2024-12-23 06:30:33.317849: Current learning rate: 0.00804\n",
      "2024-12-23 06:32:24.568242: train_loss -0.7358\n",
      "2024-12-23 06:32:24.568382: val_loss -0.6906\n",
      "2024-12-23 06:32:24.568414: Pseudo dice [np.float32(0.7513)]\n",
      "2024-12-23 06:32:24.568450: Epoch time: 111.25 s\n",
      "2024-12-23 06:32:25.137123: \n",
      "2024-12-23 06:32:25.137237: Epoch 216\n",
      "2024-12-23 06:32:25.137316: Current learning rate: 0.00803\n",
      "2024-12-23 06:34:16.407831: train_loss -0.7244\n",
      "2024-12-23 06:34:16.408015: val_loss -0.7127\n",
      "2024-12-23 06:34:16.408067: Pseudo dice [np.float32(0.7894)]\n",
      "2024-12-23 06:34:16.408105: Epoch time: 111.27 s\n",
      "2024-12-23 06:34:16.408126: Yayy! New best EMA pseudo Dice: 0.7580999732017517\n",
      "2024-12-23 06:34:17.673402: \n",
      "2024-12-23 06:34:17.673546: Epoch 217\n",
      "2024-12-23 06:34:17.673647: Current learning rate: 0.00802\n",
      "2024-12-23 06:36:08.934532: train_loss -0.7483\n",
      "2024-12-23 06:36:08.934704: val_loss -0.6976\n",
      "2024-12-23 06:36:08.934812: Pseudo dice [np.float32(0.7645)]\n",
      "2024-12-23 06:36:08.934876: Epoch time: 111.26 s\n",
      "2024-12-23 06:36:08.934906: Yayy! New best EMA pseudo Dice: 0.7587000131607056\n",
      "2024-12-23 06:36:09.741319: \n",
      "2024-12-23 06:36:09.741469: Epoch 218\n",
      "2024-12-23 06:36:09.741539: Current learning rate: 0.00801\n",
      "2024-12-23 06:38:01.006624: train_loss -0.7373\n",
      "2024-12-23 06:38:01.006776: val_loss -0.6777\n",
      "2024-12-23 06:38:01.006809: Pseudo dice [np.float32(0.7336)]\n",
      "2024-12-23 06:38:01.006847: Epoch time: 111.27 s\n",
      "2024-12-23 06:38:01.580470: \n",
      "2024-12-23 06:38:01.580932: Epoch 219\n",
      "2024-12-23 06:38:01.581106: Current learning rate: 0.00801\n",
      "2024-12-23 06:39:52.841699: train_loss -0.7336\n",
      "2024-12-23 06:39:52.841845: val_loss -0.6916\n",
      "2024-12-23 06:39:52.841886: Pseudo dice [np.float32(0.7261)]\n",
      "2024-12-23 06:39:52.841923: Epoch time: 111.26 s\n",
      "2024-12-23 06:39:53.421168: \n",
      "2024-12-23 06:39:53.421425: Epoch 220\n",
      "2024-12-23 06:39:53.421513: Current learning rate: 0.008\n",
      "2024-12-23 06:41:44.672530: train_loss -0.7537\n",
      "2024-12-23 06:41:44.672682: val_loss -0.7442\n",
      "2024-12-23 06:41:44.672721: Pseudo dice [np.float32(0.7786)]\n",
      "2024-12-23 06:41:44.672758: Epoch time: 111.25 s\n",
      "2024-12-23 06:41:45.254542: \n",
      "2024-12-23 06:41:45.254771: Epoch 221\n",
      "2024-12-23 06:41:45.254988: Current learning rate: 0.00799\n",
      "2024-12-23 06:43:36.518556: train_loss -0.7405\n",
      "2024-12-23 06:43:36.518714: val_loss -0.7009\n",
      "2024-12-23 06:43:36.518752: Pseudo dice [np.float32(0.7622)]\n",
      "2024-12-23 06:43:36.518789: Epoch time: 111.26 s\n",
      "2024-12-23 06:43:37.105848: \n",
      "2024-12-23 06:43:37.105989: Epoch 222\n",
      "2024-12-23 06:43:37.106063: Current learning rate: 0.00798\n",
      "2024-12-23 06:45:28.377843: train_loss -0.7442\n",
      "2024-12-23 06:45:28.378299: val_loss -0.7657\n",
      "2024-12-23 06:45:28.378370: Pseudo dice [np.float32(0.8252)]\n",
      "2024-12-23 06:45:28.378418: Epoch time: 111.27 s\n",
      "2024-12-23 06:45:28.378444: Yayy! New best EMA pseudo Dice: 0.7633000016212463\n",
      "2024-12-23 06:45:29.222558: \n",
      "2024-12-23 06:45:29.222992: Epoch 223\n",
      "2024-12-23 06:45:29.223156: Current learning rate: 0.00797\n",
      "2024-12-23 06:47:20.502645: train_loss -0.749\n",
      "2024-12-23 06:47:20.502818: val_loss -0.6885\n",
      "2024-12-23 06:47:20.502854: Pseudo dice [np.float32(0.7677)]\n",
      "2024-12-23 06:47:20.502892: Epoch time: 111.28 s\n",
      "2024-12-23 06:47:20.502916: Yayy! New best EMA pseudo Dice: 0.763700008392334\n",
      "2024-12-23 06:47:21.319307: \n",
      "2024-12-23 06:47:21.319708: Epoch 224\n",
      "2024-12-23 06:47:21.319789: Current learning rate: 0.00796\n",
      "2024-12-23 06:49:12.536964: train_loss -0.7333\n",
      "2024-12-23 06:49:12.537115: val_loss -0.7132\n",
      "2024-12-23 06:49:12.537147: Pseudo dice [np.float32(0.7789)]\n",
      "2024-12-23 06:49:12.537184: Epoch time: 111.22 s\n",
      "2024-12-23 06:49:12.537206: Yayy! New best EMA pseudo Dice: 0.7652000188827515\n",
      "2024-12-23 06:49:13.375459: \n",
      "2024-12-23 06:49:13.375590: Epoch 225\n",
      "2024-12-23 06:49:13.375663: Current learning rate: 0.00795\n",
      "2024-12-23 06:51:04.635246: train_loss -0.7536\n",
      "2024-12-23 06:51:04.635388: val_loss -0.6923\n",
      "2024-12-23 06:51:04.635422: Pseudo dice [np.float32(0.7484)]\n",
      "2024-12-23 06:51:04.635458: Epoch time: 111.26 s\n",
      "2024-12-23 06:51:05.202730: \n",
      "2024-12-23 06:51:05.203128: Epoch 226\n",
      "2024-12-23 06:51:05.203216: Current learning rate: 0.00794\n",
      "2024-12-23 06:52:56.446049: train_loss -0.7664\n",
      "2024-12-23 06:52:56.446478: val_loss -0.7577\n",
      "2024-12-23 06:52:56.446527: Pseudo dice [np.float32(0.8101)]\n",
      "2024-12-23 06:52:56.446564: Epoch time: 111.24 s\n",
      "2024-12-23 06:52:56.446586: Yayy! New best EMA pseudo Dice: 0.7681999802589417\n",
      "2024-12-23 06:52:57.242578: \n",
      "2024-12-23 06:52:57.242710: Epoch 227\n",
      "2024-12-23 06:52:57.242783: Current learning rate: 0.00793\n",
      "2024-12-23 06:54:48.440559: train_loss -0.7726\n",
      "2024-12-23 06:54:48.440705: val_loss -0.7212\n",
      "2024-12-23 06:54:48.440738: Pseudo dice [np.float32(0.7845)]\n",
      "2024-12-23 06:54:48.440775: Epoch time: 111.2 s\n",
      "2024-12-23 06:54:48.440797: Yayy! New best EMA pseudo Dice: 0.7698000073432922\n",
      "2024-12-23 06:54:49.239895: \n",
      "2024-12-23 06:54:49.240271: Epoch 228\n",
      "2024-12-23 06:54:49.240443: Current learning rate: 0.00792\n",
      "2024-12-23 06:56:40.469579: train_loss -0.754\n",
      "2024-12-23 06:56:40.469732: val_loss -0.6586\n",
      "2024-12-23 06:56:40.469769: Pseudo dice [np.float32(0.7467)]\n",
      "2024-12-23 06:56:40.469805: Epoch time: 111.23 s\n",
      "2024-12-23 06:56:41.041123: \n",
      "2024-12-23 06:56:41.041312: Epoch 229\n",
      "2024-12-23 06:56:41.041389: Current learning rate: 0.00791\n",
      "2024-12-23 06:58:32.233997: train_loss -0.7762\n",
      "2024-12-23 06:58:32.234160: val_loss -0.6815\n",
      "2024-12-23 06:58:32.234195: Pseudo dice [np.float32(0.7487)]\n",
      "2024-12-23 06:58:32.234231: Epoch time: 111.19 s\n",
      "2024-12-23 06:58:32.800291: \n",
      "2024-12-23 06:58:32.800694: Epoch 230\n",
      "2024-12-23 06:58:32.800898: Current learning rate: 0.0079\n",
      "2024-12-23 07:00:24.069654: train_loss -0.7009\n",
      "2024-12-23 07:00:24.069796: val_loss -0.7124\n",
      "2024-12-23 07:00:24.069829: Pseudo dice [np.float32(0.7611)]\n",
      "2024-12-23 07:00:24.069865: Epoch time: 111.27 s\n",
      "2024-12-23 07:00:24.643602: \n",
      "2024-12-23 07:00:24.644032: Epoch 231\n",
      "2024-12-23 07:00:24.644150: Current learning rate: 0.00789\n",
      "2024-12-23 07:02:15.907438: train_loss -0.6891\n",
      "2024-12-23 07:02:15.907578: val_loss -0.694\n",
      "2024-12-23 07:02:15.907612: Pseudo dice [np.float32(0.7602)]\n",
      "2024-12-23 07:02:15.907650: Epoch time: 111.26 s\n",
      "2024-12-23 07:02:16.474727: \n",
      "2024-12-23 07:02:16.475054: Epoch 232\n",
      "2024-12-23 07:02:16.475141: Current learning rate: 0.00789\n",
      "2024-12-23 07:04:07.722117: train_loss -0.7739\n",
      "2024-12-23 07:04:07.722366: val_loss -0.5802\n",
      "2024-12-23 07:04:07.722432: Pseudo dice [np.float32(0.5932)]\n",
      "2024-12-23 07:04:07.722547: Epoch time: 111.25 s\n",
      "2024-12-23 07:04:08.288308: \n",
      "2024-12-23 07:04:08.288409: Epoch 233\n",
      "2024-12-23 07:04:08.288482: Current learning rate: 0.00788\n",
      "2024-12-23 07:05:59.494133: train_loss -0.7684\n",
      "2024-12-23 07:05:59.494368: val_loss -0.7378\n",
      "2024-12-23 07:05:59.494407: Pseudo dice [np.float32(0.8097)]\n",
      "2024-12-23 07:05:59.494443: Epoch time: 111.21 s\n",
      "2024-12-23 07:06:00.067856: \n",
      "2024-12-23 07:06:00.068013: Epoch 234\n",
      "2024-12-23 07:06:00.068092: Current learning rate: 0.00787\n",
      "2024-12-23 07:07:51.264460: train_loss -0.7272\n",
      "2024-12-23 07:07:51.264759: val_loss -0.687\n",
      "2024-12-23 07:07:51.264852: Pseudo dice [np.float32(0.7575)]\n",
      "2024-12-23 07:07:51.264896: Epoch time: 111.2 s\n",
      "2024-12-23 07:07:52.274208: \n",
      "2024-12-23 07:07:52.274488: Epoch 235\n",
      "2024-12-23 07:07:52.274605: Current learning rate: 0.00786\n",
      "2024-12-23 07:09:43.520496: train_loss -0.7368\n",
      "2024-12-23 07:09:43.520712: val_loss -0.6854\n",
      "2024-12-23 07:09:43.520871: Pseudo dice [np.float32(0.795)]\n",
      "2024-12-23 07:09:43.520945: Epoch time: 111.25 s\n",
      "2024-12-23 07:09:44.096377: \n",
      "2024-12-23 07:09:44.096725: Epoch 236\n",
      "2024-12-23 07:09:44.096889: Current learning rate: 0.00785\n",
      "2024-12-23 07:11:35.344122: train_loss -0.7382\n",
      "2024-12-23 07:11:35.344340: val_loss -0.7126\n",
      "2024-12-23 07:11:35.344378: Pseudo dice [np.float32(0.7877)]\n",
      "2024-12-23 07:11:35.344524: Epoch time: 111.25 s\n",
      "2024-12-23 07:11:35.937046: \n",
      "2024-12-23 07:11:35.937194: Epoch 237\n",
      "2024-12-23 07:11:35.937315: Current learning rate: 0.00784\n",
      "2024-12-23 07:13:27.123907: train_loss -0.7285\n",
      "2024-12-23 07:13:27.124075: val_loss -0.7192\n",
      "2024-12-23 07:13:27.124114: Pseudo dice [np.float32(0.7997)]\n",
      "2024-12-23 07:13:27.124152: Epoch time: 111.19 s\n",
      "2024-12-23 07:13:27.701044: \n",
      "2024-12-23 07:13:27.701182: Epoch 238\n",
      "2024-12-23 07:13:27.701257: Current learning rate: 0.00783\n",
      "2024-12-23 07:15:18.956797: train_loss -0.7316\n",
      "2024-12-23 07:15:18.956959: val_loss -0.6347\n",
      "2024-12-23 07:15:18.956995: Pseudo dice [np.float32(0.5643)]\n",
      "2024-12-23 07:15:18.957032: Epoch time: 111.26 s\n",
      "2024-12-23 07:15:19.521057: \n",
      "2024-12-23 07:15:19.521322: Epoch 239\n",
      "2024-12-23 07:15:19.521403: Current learning rate: 0.00782\n",
      "2024-12-23 07:17:10.808763: train_loss -0.6955\n",
      "2024-12-23 07:17:10.808913: val_loss -0.6345\n",
      "2024-12-23 07:17:10.808952: Pseudo dice [np.float32(0.6378)]\n",
      "2024-12-23 07:17:10.808993: Epoch time: 111.29 s\n",
      "2024-12-23 07:17:11.388568: \n",
      "2024-12-23 07:17:11.388698: Epoch 240\n",
      "2024-12-23 07:17:11.388769: Current learning rate: 0.00781\n",
      "2024-12-23 07:19:02.715059: train_loss -0.6733\n",
      "2024-12-23 07:19:02.715274: val_loss -0.6069\n",
      "2024-12-23 07:19:02.715324: Pseudo dice [np.float32(0.6973)]\n",
      "2024-12-23 07:19:02.715368: Epoch time: 111.33 s\n",
      "2024-12-23 07:19:03.296946: \n",
      "2024-12-23 07:19:03.297073: Epoch 241\n",
      "2024-12-23 07:19:03.297146: Current learning rate: 0.0078\n",
      "2024-12-23 07:20:54.618512: train_loss -0.7157\n",
      "2024-12-23 07:20:54.618667: val_loss -0.6859\n",
      "2024-12-23 07:20:54.618702: Pseudo dice [np.float32(0.7635)]\n",
      "2024-12-23 07:20:54.618740: Epoch time: 111.32 s\n",
      "2024-12-23 07:20:55.192422: \n",
      "2024-12-23 07:20:55.192562: Epoch 242\n",
      "2024-12-23 07:20:55.192699: Current learning rate: 0.00779\n",
      "2024-12-23 07:22:46.404838: train_loss -0.6986\n",
      "2024-12-23 07:22:46.404986: val_loss -0.6984\n",
      "2024-12-23 07:22:46.405027: Pseudo dice [np.float32(0.7546)]\n",
      "2024-12-23 07:22:46.405062: Epoch time: 111.21 s\n",
      "2024-12-23 07:22:46.986517: \n",
      "2024-12-23 07:22:46.986887: Epoch 243\n",
      "2024-12-23 07:22:46.987036: Current learning rate: 0.00778\n",
      "2024-12-23 07:24:38.205199: train_loss -0.7363\n",
      "2024-12-23 07:24:38.205346: val_loss -0.6609\n",
      "2024-12-23 07:24:38.205380: Pseudo dice [np.float32(0.6726)]\n",
      "2024-12-23 07:24:38.205417: Epoch time: 111.22 s\n",
      "2024-12-23 07:24:38.785074: \n",
      "2024-12-23 07:24:38.785197: Epoch 244\n",
      "2024-12-23 07:24:38.785269: Current learning rate: 0.00777\n",
      "2024-12-23 07:26:30.110218: train_loss -0.7101\n",
      "2024-12-23 07:26:30.110372: val_loss -0.6635\n",
      "2024-12-23 07:26:30.110411: Pseudo dice [np.float32(0.7575)]\n",
      "2024-12-23 07:26:30.110448: Epoch time: 111.33 s\n",
      "2024-12-23 07:26:30.727873: \n",
      "2024-12-23 07:26:30.727989: Epoch 245\n",
      "2024-12-23 07:26:30.728057: Current learning rate: 0.00777\n",
      "2024-12-23 07:28:22.074139: train_loss -0.6893\n",
      "2024-12-23 07:28:22.074626: val_loss -0.6193\n",
      "2024-12-23 07:28:22.074669: Pseudo dice [np.float32(0.6723)]\n",
      "2024-12-23 07:28:22.074704: Epoch time: 111.35 s\n",
      "2024-12-23 07:28:22.655093: \n",
      "2024-12-23 07:28:22.655324: Epoch 246\n",
      "2024-12-23 07:28:22.655421: Current learning rate: 0.00776\n",
      "2024-12-23 07:30:13.974332: train_loss -0.7373\n",
      "2024-12-23 07:30:13.974485: val_loss -0.7198\n",
      "2024-12-23 07:30:13.974520: Pseudo dice [np.float32(0.7568)]\n",
      "2024-12-23 07:30:13.974558: Epoch time: 111.32 s\n",
      "2024-12-23 07:30:14.547500: \n",
      "2024-12-23 07:30:14.547620: Epoch 247\n",
      "2024-12-23 07:30:14.547692: Current learning rate: 0.00775\n",
      "2024-12-23 07:32:05.762139: train_loss -0.7198\n",
      "2024-12-23 07:32:05.762295: val_loss -0.7184\n",
      "2024-12-23 07:32:05.762331: Pseudo dice [np.float32(0.7506)]\n",
      "2024-12-23 07:32:05.762368: Epoch time: 111.22 s\n",
      "2024-12-23 07:32:06.338447: \n",
      "2024-12-23 07:32:06.338554: Epoch 248\n",
      "2024-12-23 07:32:06.338628: Current learning rate: 0.00774\n",
      "2024-12-23 07:33:57.579826: train_loss -0.7252\n",
      "2024-12-23 07:33:57.579974: val_loss -0.657\n",
      "2024-12-23 07:33:57.580036: Pseudo dice [np.float32(0.7435)]\n",
      "2024-12-23 07:33:57.580072: Epoch time: 111.24 s\n",
      "2024-12-23 07:33:58.143285: \n",
      "2024-12-23 07:33:58.143667: Epoch 249\n",
      "2024-12-23 07:33:58.143807: Current learning rate: 0.00773\n",
      "2024-12-23 07:35:49.405199: train_loss -0.7693\n",
      "2024-12-23 07:35:49.405468: val_loss -0.6218\n",
      "2024-12-23 07:35:49.405580: Pseudo dice [np.float32(0.8131)]\n",
      "2024-12-23 07:35:49.405623: Epoch time: 111.26 s\n",
      "2024-12-23 07:35:50.230465: \n",
      "2024-12-23 07:35:50.230903: Epoch 250\n",
      "2024-12-23 07:35:50.231216: Current learning rate: 0.00772\n",
      "2024-12-23 07:37:41.461819: train_loss -0.728\n",
      "2024-12-23 07:37:41.461957: val_loss -0.682\n",
      "2024-12-23 07:37:41.461989: Pseudo dice [np.float32(0.7521)]\n",
      "2024-12-23 07:37:41.462024: Epoch time: 111.23 s\n",
      "2024-12-23 07:37:42.035338: \n",
      "2024-12-23 07:37:42.035508: Epoch 251\n",
      "2024-12-23 07:37:42.035581: Current learning rate: 0.00771\n",
      "2024-12-23 07:39:33.327266: train_loss -0.7335\n",
      "2024-12-23 07:39:33.327405: val_loss -0.7111\n",
      "2024-12-23 07:39:33.327441: Pseudo dice [np.float32(0.7696)]\n",
      "2024-12-23 07:39:33.327477: Epoch time: 111.29 s\n",
      "2024-12-23 07:39:33.909262: \n",
      "2024-12-23 07:39:33.909498: Epoch 252\n",
      "2024-12-23 07:39:33.909582: Current learning rate: 0.0077\n",
      "2024-12-23 07:41:25.158512: train_loss -0.7177\n",
      "2024-12-23 07:41:25.158703: val_loss -0.7307\n",
      "2024-12-23 07:41:25.158817: Pseudo dice [np.float32(0.7905)]\n",
      "2024-12-23 07:41:25.158863: Epoch time: 111.25 s\n",
      "2024-12-23 07:41:26.167131: \n",
      "2024-12-23 07:41:26.167575: Epoch 253\n",
      "2024-12-23 07:41:26.167674: Current learning rate: 0.00769\n",
      "2024-12-23 07:43:17.442840: train_loss -0.747\n",
      "2024-12-23 07:43:17.443004: val_loss -0.712\n",
      "2024-12-23 07:43:17.443038: Pseudo dice [np.float32(0.8125)]\n",
      "2024-12-23 07:43:17.443074: Epoch time: 111.28 s\n",
      "2024-12-23 07:43:18.009976: \n",
      "2024-12-23 07:43:18.010490: Epoch 254\n",
      "2024-12-23 07:43:18.010661: Current learning rate: 0.00768\n",
      "2024-12-23 07:45:09.254217: train_loss -0.7307\n",
      "2024-12-23 07:45:09.254635: val_loss -0.6273\n",
      "2024-12-23 07:45:09.254740: Pseudo dice [np.float32(0.6939)]\n",
      "2024-12-23 07:45:09.254791: Epoch time: 111.24 s\n",
      "2024-12-23 07:45:09.833858: \n",
      "2024-12-23 07:45:09.833999: Epoch 255\n",
      "2024-12-23 07:45:09.834074: Current learning rate: 0.00767\n",
      "2024-12-23 07:47:01.118188: train_loss -0.6862\n",
      "2024-12-23 07:47:01.118398: val_loss -0.7239\n",
      "2024-12-23 07:47:01.118433: Pseudo dice [np.float32(0.7413)]\n",
      "2024-12-23 07:47:01.118469: Epoch time: 111.28 s\n",
      "2024-12-23 07:47:01.690247: \n",
      "2024-12-23 07:47:01.690456: Epoch 256\n",
      "2024-12-23 07:47:01.690609: Current learning rate: 0.00766\n",
      "2024-12-23 07:48:52.916969: train_loss -0.7094\n",
      "2024-12-23 07:48:52.917123: val_loss -0.754\n",
      "2024-12-23 07:48:52.917158: Pseudo dice [np.float32(0.8116)]\n",
      "2024-12-23 07:48:52.917196: Epoch time: 111.23 s\n",
      "2024-12-23 07:48:53.499838: \n",
      "2024-12-23 07:48:53.499974: Epoch 257\n",
      "2024-12-23 07:48:53.500046: Current learning rate: 0.00765\n",
      "2024-12-23 07:50:44.741801: train_loss -0.6894\n",
      "2024-12-23 07:50:44.742091: val_loss -0.6487\n",
      "2024-12-23 07:50:44.742152: Pseudo dice [np.float32(0.7669)]\n",
      "2024-12-23 07:50:44.742294: Epoch time: 111.24 s\n",
      "2024-12-23 07:50:45.327340: \n",
      "2024-12-23 07:50:45.327486: Epoch 258\n",
      "2024-12-23 07:50:45.327562: Current learning rate: 0.00764\n",
      "2024-12-23 07:52:36.623861: train_loss -0.6514\n",
      "2024-12-23 07:52:36.624012: val_loss -0.6779\n",
      "2024-12-23 07:52:36.624045: Pseudo dice [np.float32(0.7217)]\n",
      "2024-12-23 07:52:36.624080: Epoch time: 111.3 s\n",
      "2024-12-23 07:52:37.205966: \n",
      "2024-12-23 07:52:37.206206: Epoch 259\n",
      "2024-12-23 07:52:37.206444: Current learning rate: 0.00764\n",
      "2024-12-23 07:54:28.444476: train_loss -0.6997\n",
      "2024-12-23 07:54:28.444622: val_loss -0.7047\n",
      "2024-12-23 07:54:28.444656: Pseudo dice [np.float32(0.7546)]\n",
      "2024-12-23 07:54:28.444691: Epoch time: 111.24 s\n",
      "2024-12-23 07:54:29.010216: \n",
      "2024-12-23 07:54:29.010348: Epoch 260\n",
      "2024-12-23 07:54:29.010422: Current learning rate: 0.00763\n",
      "2024-12-23 07:56:20.303638: train_loss -0.7235\n",
      "2024-12-23 07:56:20.303825: val_loss -0.6794\n",
      "2024-12-23 07:56:20.303869: Pseudo dice [np.float32(0.7194)]\n",
      "2024-12-23 07:56:20.303908: Epoch time: 111.29 s\n",
      "2024-12-23 07:56:20.881460: \n",
      "2024-12-23 07:56:20.881586: Epoch 261\n",
      "2024-12-23 07:56:20.881660: Current learning rate: 0.00762\n",
      "2024-12-23 07:58:12.180678: train_loss -0.7485\n",
      "2024-12-23 07:58:12.180868: val_loss -0.6722\n",
      "2024-12-23 07:58:12.181033: Pseudo dice [np.float32(0.6343)]\n",
      "2024-12-23 07:58:12.181118: Epoch time: 111.3 s\n",
      "2024-12-23 07:58:12.770233: \n",
      "2024-12-23 07:58:12.770363: Epoch 262\n",
      "2024-12-23 07:58:12.770436: Current learning rate: 0.00761\n",
      "2024-12-23 08:00:04.015561: train_loss -0.7359\n",
      "2024-12-23 08:00:04.015787: val_loss -0.6945\n",
      "2024-12-23 08:00:04.015821: Pseudo dice [np.float32(0.7888)]\n",
      "2024-12-23 08:00:04.015858: Epoch time: 111.25 s\n",
      "2024-12-23 08:00:04.598195: \n",
      "2024-12-23 08:00:04.598324: Epoch 263\n",
      "2024-12-23 08:00:04.598398: Current learning rate: 0.0076\n",
      "2024-12-23 08:01:55.856348: train_loss -0.734\n",
      "2024-12-23 08:01:55.856501: val_loss -0.6907\n",
      "2024-12-23 08:01:55.856543: Pseudo dice [np.float32(0.7566)]\n",
      "2024-12-23 08:01:55.856638: Epoch time: 111.26 s\n",
      "2024-12-23 08:01:56.416896: \n",
      "2024-12-23 08:01:56.417385: Epoch 264\n",
      "2024-12-23 08:01:56.417514: Current learning rate: 0.00759\n",
      "2024-12-23 08:03:47.697318: train_loss -0.7484\n",
      "2024-12-23 08:03:47.697560: val_loss -0.6718\n",
      "2024-12-23 08:03:47.697623: Pseudo dice [np.float32(0.7854)]\n",
      "2024-12-23 08:03:47.697666: Epoch time: 111.28 s\n",
      "2024-12-23 08:03:48.274295: \n",
      "2024-12-23 08:03:48.274416: Epoch 265\n",
      "2024-12-23 08:03:48.274494: Current learning rate: 0.00758\n",
      "2024-12-23 08:05:39.495110: train_loss -0.7146\n",
      "2024-12-23 08:05:39.495263: val_loss -0.7059\n",
      "2024-12-23 08:05:39.495299: Pseudo dice [np.float32(0.747)]\n",
      "2024-12-23 08:05:39.495334: Epoch time: 111.22 s\n",
      "2024-12-23 08:05:40.069347: \n",
      "2024-12-23 08:05:40.069733: Epoch 266\n",
      "2024-12-23 08:05:40.069874: Current learning rate: 0.00757\n",
      "2024-12-23 08:07:31.338263: train_loss -0.7572\n",
      "2024-12-23 08:07:31.338469: val_loss -0.6946\n",
      "2024-12-23 08:07:31.338516: Pseudo dice [np.float32(0.729)]\n",
      "2024-12-23 08:07:31.338556: Epoch time: 111.27 s\n",
      "2024-12-23 08:07:31.917939: \n",
      "2024-12-23 08:07:31.918039: Epoch 267\n",
      "2024-12-23 08:07:31.918110: Current learning rate: 0.00756\n",
      "2024-12-23 08:09:23.215304: train_loss -0.7504\n",
      "2024-12-23 08:09:23.215456: val_loss -0.6294\n",
      "2024-12-23 08:09:23.215491: Pseudo dice [np.float32(0.6496)]\n",
      "2024-12-23 08:09:23.215528: Epoch time: 111.3 s\n",
      "2024-12-23 08:09:23.794326: \n",
      "2024-12-23 08:09:23.794430: Epoch 268\n",
      "2024-12-23 08:09:23.794506: Current learning rate: 0.00755\n",
      "2024-12-23 08:11:15.141577: train_loss -0.6938\n",
      "2024-12-23 08:11:15.141797: val_loss -0.6484\n",
      "2024-12-23 08:11:15.141834: Pseudo dice [np.float32(0.6597)]\n",
      "2024-12-23 08:11:15.141874: Epoch time: 111.35 s\n",
      "2024-12-23 08:11:15.725296: \n",
      "2024-12-23 08:11:15.725694: Epoch 269\n",
      "2024-12-23 08:11:15.725793: Current learning rate: 0.00754\n",
      "2024-12-23 08:13:07.002035: train_loss -0.7131\n",
      "2024-12-23 08:13:07.002226: val_loss -0.6489\n",
      "2024-12-23 08:13:07.002259: Pseudo dice [np.float32(0.6959)]\n",
      "2024-12-23 08:13:07.002293: Epoch time: 111.28 s\n",
      "2024-12-23 08:13:08.004970: \n",
      "2024-12-23 08:13:08.005103: Epoch 270\n",
      "2024-12-23 08:13:08.005221: Current learning rate: 0.00753\n",
      "2024-12-23 08:14:59.334393: train_loss -0.7179\n",
      "2024-12-23 08:14:59.334559: val_loss -0.7382\n",
      "2024-12-23 08:14:59.334596: Pseudo dice [np.float32(0.8111)]\n",
      "2024-12-23 08:14:59.334635: Epoch time: 111.33 s\n",
      "2024-12-23 08:14:59.905095: \n",
      "2024-12-23 08:14:59.905228: Epoch 271\n",
      "2024-12-23 08:14:59.905298: Current learning rate: 0.00752\n",
      "2024-12-23 08:16:51.124080: train_loss -0.7564\n",
      "2024-12-23 08:16:51.124218: val_loss -0.6922\n",
      "2024-12-23 08:16:51.124252: Pseudo dice [np.float32(0.7978)]\n",
      "2024-12-23 08:16:51.124299: Epoch time: 111.22 s\n",
      "2024-12-23 08:16:51.707584: \n",
      "2024-12-23 08:16:51.708019: Epoch 272\n",
      "2024-12-23 08:16:51.708222: Current learning rate: 0.00751\n",
      "2024-12-23 08:18:43.000015: train_loss -0.7659\n",
      "2024-12-23 08:18:43.000152: val_loss -0.7376\n",
      "2024-12-23 08:18:43.000191: Pseudo dice [np.float32(0.7873)]\n",
      "2024-12-23 08:18:43.000229: Epoch time: 111.29 s\n",
      "2024-12-23 08:18:43.574924: \n",
      "2024-12-23 08:18:43.575096: Epoch 273\n",
      "2024-12-23 08:18:43.575173: Current learning rate: 0.00751\n",
      "2024-12-23 08:20:34.876722: train_loss -0.731\n",
      "2024-12-23 08:20:34.876867: val_loss -0.6913\n",
      "2024-12-23 08:20:34.876902: Pseudo dice [np.float32(0.7346)]\n",
      "2024-12-23 08:20:34.876938: Epoch time: 111.3 s\n",
      "2024-12-23 08:20:35.449339: \n",
      "2024-12-23 08:20:35.449495: Epoch 274\n",
      "2024-12-23 08:20:35.449569: Current learning rate: 0.0075\n",
      "2024-12-23 08:22:26.734376: train_loss -0.7213\n",
      "2024-12-23 08:22:26.734530: val_loss -0.71\n",
      "2024-12-23 08:22:26.734564: Pseudo dice [np.float32(0.7445)]\n",
      "2024-12-23 08:22:26.734599: Epoch time: 111.29 s\n",
      "2024-12-23 08:22:27.317620: \n",
      "2024-12-23 08:22:27.317973: Epoch 275\n",
      "2024-12-23 08:22:27.318060: Current learning rate: 0.00749\n",
      "2024-12-23 08:24:18.611500: train_loss -0.755\n",
      "2024-12-23 08:24:18.611646: val_loss -0.6601\n",
      "2024-12-23 08:24:18.611682: Pseudo dice [np.float32(0.7199)]\n",
      "2024-12-23 08:24:18.611718: Epoch time: 111.29 s\n",
      "2024-12-23 08:24:19.192551: \n",
      "2024-12-23 08:24:19.192908: Epoch 276\n",
      "2024-12-23 08:24:19.192987: Current learning rate: 0.00748\n",
      "2024-12-23 08:26:10.502538: train_loss -0.7706\n",
      "2024-12-23 08:26:10.502690: val_loss -0.6804\n",
      "2024-12-23 08:26:10.502727: Pseudo dice [np.float32(0.7527)]\n",
      "2024-12-23 08:26:10.502764: Epoch time: 111.31 s\n",
      "2024-12-23 08:26:11.090812: \n",
      "2024-12-23 08:26:11.090994: Epoch 277\n",
      "2024-12-23 08:26:11.091165: Current learning rate: 0.00747\n",
      "2024-12-23 08:28:02.353816: train_loss -0.7619\n",
      "2024-12-23 08:28:02.354269: val_loss -0.766\n",
      "2024-12-23 08:28:02.354376: Pseudo dice [np.float32(0.8227)]\n",
      "2024-12-23 08:28:02.354420: Epoch time: 111.26 s\n",
      "2024-12-23 08:28:02.940647: \n",
      "2024-12-23 08:28:02.941139: Epoch 278\n",
      "2024-12-23 08:28:02.941246: Current learning rate: 0.00746\n",
      "2024-12-23 08:29:54.267226: train_loss -0.7882\n",
      "2024-12-23 08:29:54.267516: val_loss -0.6435\n",
      "2024-12-23 08:29:54.267652: Pseudo dice [np.float32(0.7262)]\n",
      "2024-12-23 08:29:54.267703: Epoch time: 111.33 s\n",
      "2024-12-23 08:29:54.843783: \n",
      "2024-12-23 08:29:54.843997: Epoch 279\n",
      "2024-12-23 08:29:54.844076: Current learning rate: 0.00745\n",
      "2024-12-23 08:31:46.116352: train_loss -0.7544\n",
      "2024-12-23 08:31:46.116500: val_loss -0.6008\n",
      "2024-12-23 08:31:46.116534: Pseudo dice [np.float32(0.7091)]\n",
      "2024-12-23 08:31:46.116570: Epoch time: 111.27 s\n",
      "2024-12-23 08:31:46.705291: \n",
      "2024-12-23 08:31:46.705486: Epoch 280\n",
      "2024-12-23 08:31:46.705570: Current learning rate: 0.00744\n",
      "2024-12-23 08:33:37.953071: train_loss -0.7506\n",
      "2024-12-23 08:33:37.953298: val_loss -0.6445\n",
      "2024-12-23 08:33:37.953336: Pseudo dice [np.float32(0.7445)]\n",
      "2024-12-23 08:33:37.953372: Epoch time: 111.25 s\n",
      "2024-12-23 08:33:38.530219: \n",
      "2024-12-23 08:33:38.530328: Epoch 281\n",
      "2024-12-23 08:33:38.530398: Current learning rate: 0.00743\n",
      "2024-12-23 08:35:29.798945: train_loss -0.7565\n",
      "2024-12-23 08:35:29.799096: val_loss -0.6101\n",
      "2024-12-23 08:35:29.799129: Pseudo dice [np.float32(0.6442)]\n",
      "2024-12-23 08:35:29.799166: Epoch time: 111.27 s\n",
      "2024-12-23 08:35:30.388065: \n",
      "2024-12-23 08:35:30.388181: Epoch 282\n",
      "2024-12-23 08:35:30.388256: Current learning rate: 0.00742\n",
      "2024-12-23 08:37:21.623969: train_loss -0.7531\n",
      "2024-12-23 08:37:21.624383: val_loss -0.6753\n",
      "2024-12-23 08:37:21.624443: Pseudo dice [np.float32(0.6558)]\n",
      "2024-12-23 08:37:21.624557: Epoch time: 111.24 s\n",
      "2024-12-23 08:37:22.208521: \n",
      "2024-12-23 08:37:22.208631: Epoch 283\n",
      "2024-12-23 08:37:22.208705: Current learning rate: 0.00741\n",
      "2024-12-23 08:39:13.486188: train_loss -0.7836\n",
      "2024-12-23 08:39:13.486338: val_loss -0.6544\n",
      "2024-12-23 08:39:13.486374: Pseudo dice [np.float32(0.7024)]\n",
      "2024-12-23 08:39:13.486412: Epoch time: 111.28 s\n",
      "2024-12-23 08:39:14.064373: \n",
      "2024-12-23 08:39:14.064562: Epoch 284\n",
      "2024-12-23 08:39:14.064642: Current learning rate: 0.0074\n",
      "2024-12-23 08:41:05.314432: train_loss -0.7395\n",
      "2024-12-23 08:41:05.314586: val_loss -0.7426\n",
      "2024-12-23 08:41:05.314633: Pseudo dice [np.float32(0.8176)]\n",
      "2024-12-23 08:41:05.314678: Epoch time: 111.25 s\n",
      "2024-12-23 08:41:05.884480: \n",
      "2024-12-23 08:41:05.884598: Epoch 285\n",
      "2024-12-23 08:41:05.884676: Current learning rate: 0.00739\n",
      "2024-12-23 08:42:57.120062: train_loss -0.7521\n",
      "2024-12-23 08:42:57.120224: val_loss -0.6327\n",
      "2024-12-23 08:42:57.120374: Pseudo dice [np.float32(0.7212)]\n",
      "2024-12-23 08:42:57.120447: Epoch time: 111.24 s\n",
      "2024-12-23 08:42:57.700918: \n",
      "2024-12-23 08:42:57.701183: Epoch 286\n",
      "2024-12-23 08:42:57.701260: Current learning rate: 0.00738\n",
      "2024-12-23 08:44:48.973919: train_loss -0.7651\n",
      "2024-12-23 08:44:48.974115: val_loss -0.6923\n",
      "2024-12-23 08:44:48.974149: Pseudo dice [np.float32(0.7547)]\n",
      "2024-12-23 08:44:48.974186: Epoch time: 111.27 s\n",
      "2024-12-23 08:44:49.554069: \n",
      "2024-12-23 08:44:49.554424: Epoch 287\n",
      "2024-12-23 08:44:49.554585: Current learning rate: 0.00738\n",
      "2024-12-23 08:46:40.771056: train_loss -0.7684\n",
      "2024-12-23 08:46:40.771204: val_loss -0.7025\n",
      "2024-12-23 08:46:40.771240: Pseudo dice [np.float32(0.7905)]\n",
      "2024-12-23 08:46:40.771276: Epoch time: 111.22 s\n",
      "2024-12-23 08:46:41.353848: \n",
      "2024-12-23 08:46:41.354035: Epoch 288\n",
      "2024-12-23 08:46:41.354239: Current learning rate: 0.00737\n",
      "2024-12-23 08:48:32.956188: train_loss -0.7537\n",
      "2024-12-23 08:48:32.956340: val_loss -0.7304\n",
      "2024-12-23 08:48:32.956379: Pseudo dice [np.float32(0.7903)]\n",
      "2024-12-23 08:48:32.956415: Epoch time: 111.6 s\n",
      "2024-12-23 08:48:33.527337: \n",
      "2024-12-23 08:48:33.527593: Epoch 289\n",
      "2024-12-23 08:48:33.527670: Current learning rate: 0.00736\n",
      "2024-12-23 08:50:24.788299: train_loss -0.741\n",
      "2024-12-23 08:50:24.788655: val_loss -0.7008\n",
      "2024-12-23 08:50:24.788739: Pseudo dice [np.float32(0.7923)]\n",
      "2024-12-23 08:50:24.788787: Epoch time: 111.26 s\n",
      "2024-12-23 08:50:25.377929: \n",
      "2024-12-23 08:50:25.378082: Epoch 290\n",
      "2024-12-23 08:50:25.378169: Current learning rate: 0.00735\n",
      "2024-12-23 08:52:16.653590: train_loss -0.744\n",
      "2024-12-23 08:52:16.653724: val_loss -0.7444\n",
      "2024-12-23 08:52:16.653760: Pseudo dice [np.float32(0.7849)]\n",
      "2024-12-23 08:52:16.653796: Epoch time: 111.28 s\n",
      "2024-12-23 08:52:17.240900: \n",
      "2024-12-23 08:52:17.241040: Epoch 291\n",
      "2024-12-23 08:52:17.241114: Current learning rate: 0.00734\n",
      "2024-12-23 08:54:08.507678: train_loss -0.7458\n",
      "2024-12-23 08:54:08.508150: val_loss -0.5694\n",
      "2024-12-23 08:54:08.508208: Pseudo dice [np.float32(0.5878)]\n",
      "2024-12-23 08:54:08.508253: Epoch time: 111.27 s\n",
      "2024-12-23 08:54:09.095442: \n",
      "2024-12-23 08:54:09.095593: Epoch 292\n",
      "2024-12-23 08:54:09.095666: Current learning rate: 0.00733\n",
      "2024-12-23 08:56:00.359160: train_loss -0.6929\n",
      "2024-12-23 08:56:00.359306: val_loss -0.7091\n",
      "2024-12-23 08:56:00.359340: Pseudo dice [np.float32(0.7876)]\n",
      "2024-12-23 08:56:00.359376: Epoch time: 111.26 s\n",
      "2024-12-23 08:56:00.943933: \n",
      "2024-12-23 08:56:00.944293: Epoch 293\n",
      "2024-12-23 08:56:00.944426: Current learning rate: 0.00732\n",
      "2024-12-23 08:57:52.265176: train_loss -0.7221\n",
      "2024-12-23 08:57:52.265396: val_loss -0.7626\n",
      "2024-12-23 08:57:52.265434: Pseudo dice [np.float32(0.8283)]\n",
      "2024-12-23 08:57:52.265472: Epoch time: 111.32 s\n",
      "2024-12-23 08:57:52.851609: \n",
      "2024-12-23 08:57:52.852062: Epoch 294\n",
      "2024-12-23 08:57:52.852240: Current learning rate: 0.00731\n",
      "2024-12-23 08:59:44.100658: train_loss -0.7214\n",
      "2024-12-23 08:59:44.100815: val_loss -0.6534\n",
      "2024-12-23 08:59:44.103437: Pseudo dice [np.float32(0.7371)]\n",
      "2024-12-23 08:59:44.103537: Epoch time: 111.25 s\n",
      "2024-12-23 08:59:44.701695: \n",
      "2024-12-23 08:59:44.702202: Epoch 295\n",
      "2024-12-23 08:59:44.702316: Current learning rate: 0.0073\n",
      "2024-12-23 09:01:35.945514: train_loss -0.7216\n",
      "2024-12-23 09:01:35.945673: val_loss -0.6936\n",
      "2024-12-23 09:01:35.945709: Pseudo dice [np.float32(0.795)]\n",
      "2024-12-23 09:01:35.945746: Epoch time: 111.24 s\n",
      "2024-12-23 09:01:36.540151: \n",
      "2024-12-23 09:01:36.540383: Epoch 296\n",
      "2024-12-23 09:01:36.540734: Current learning rate: 0.00729\n",
      "2024-12-23 09:03:27.812970: train_loss -0.7426\n",
      "2024-12-23 09:03:27.813120: val_loss -0.7234\n",
      "2024-12-23 09:03:27.813152: Pseudo dice [np.float32(0.7894)]\n",
      "2024-12-23 09:03:27.813362: Epoch time: 111.27 s\n",
      "2024-12-23 09:03:28.404522: \n",
      "2024-12-23 09:03:28.404914: Epoch 297\n",
      "2024-12-23 09:03:28.405197: Current learning rate: 0.00728\n",
      "2024-12-23 09:05:19.640632: train_loss -0.7103\n",
      "2024-12-23 09:05:19.640928: val_loss -0.6566\n",
      "2024-12-23 09:05:19.640978: Pseudo dice [np.float32(0.6958)]\n",
      "2024-12-23 09:05:19.641016: Epoch time: 111.24 s\n",
      "2024-12-23 09:05:20.224165: \n",
      "2024-12-23 09:05:20.224288: Epoch 298\n",
      "2024-12-23 09:05:20.224361: Current learning rate: 0.00727\n",
      "2024-12-23 09:07:11.441514: train_loss -0.6887\n",
      "2024-12-23 09:07:11.441762: val_loss -0.735\n",
      "2024-12-23 09:07:11.441800: Pseudo dice [np.float32(0.7809)]\n",
      "2024-12-23 09:07:11.441838: Epoch time: 111.22 s\n",
      "2024-12-23 09:07:12.035820: \n",
      "2024-12-23 09:07:12.036010: Epoch 299\n",
      "2024-12-23 09:07:12.036096: Current learning rate: 0.00726\n",
      "2024-12-23 09:09:03.260473: train_loss -0.672\n",
      "2024-12-23 09:09:03.260624: val_loss -0.6514\n",
      "2024-12-23 09:09:03.260658: Pseudo dice [np.float32(0.7179)]\n",
      "2024-12-23 09:09:03.260695: Epoch time: 111.23 s\n",
      "2024-12-23 09:09:04.057138: \n",
      "2024-12-23 09:09:04.057350: Epoch 300\n",
      "2024-12-23 09:09:04.057439: Current learning rate: 0.00725\n",
      "2024-12-23 09:10:55.288078: train_loss -0.6602\n",
      "2024-12-23 09:10:55.288221: val_loss -0.7244\n",
      "2024-12-23 09:10:55.288255: Pseudo dice [np.float32(0.7591)]\n",
      "2024-12-23 09:10:55.288292: Epoch time: 111.23 s\n",
      "2024-12-23 09:10:55.872898: \n",
      "2024-12-23 09:10:55.873003: Epoch 301\n",
      "2024-12-23 09:10:55.873077: Current learning rate: 0.00724\n",
      "2024-12-23 09:12:47.069673: train_loss -0.6671\n",
      "2024-12-23 09:12:47.069829: val_loss -0.6804\n",
      "2024-12-23 09:12:47.069865: Pseudo dice [np.float32(0.7704)]\n",
      "2024-12-23 09:12:47.069981: Epoch time: 111.2 s\n",
      "2024-12-23 09:12:47.660954: \n",
      "2024-12-23 09:12:47.661063: Epoch 302\n",
      "2024-12-23 09:12:47.661139: Current learning rate: 0.00724\n",
      "2024-12-23 09:14:38.855105: train_loss -0.71\n",
      "2024-12-23 09:14:38.855255: val_loss -0.6754\n",
      "2024-12-23 09:14:38.855290: Pseudo dice [np.float32(0.768)]\n",
      "2024-12-23 09:14:38.855327: Epoch time: 111.19 s\n",
      "2024-12-23 09:14:39.432511: \n",
      "2024-12-23 09:14:39.432714: Epoch 303\n",
      "2024-12-23 09:14:39.432793: Current learning rate: 0.00723\n",
      "2024-12-23 09:16:30.665189: train_loss -0.6885\n",
      "2024-12-23 09:16:30.665351: val_loss -0.7436\n",
      "2024-12-23 09:16:30.665384: Pseudo dice [np.float32(0.8125)]\n",
      "2024-12-23 09:16:30.665426: Epoch time: 111.23 s\n",
      "2024-12-23 09:16:31.249105: \n",
      "2024-12-23 09:16:31.249201: Epoch 304\n",
      "2024-12-23 09:16:31.249280: Current learning rate: 0.00722\n",
      "2024-12-23 09:18:22.475723: train_loss -0.7102\n",
      "2024-12-23 09:18:22.475871: val_loss -0.6588\n",
      "2024-12-23 09:18:22.475907: Pseudo dice [np.float32(0.717)]\n",
      "2024-12-23 09:18:22.475942: Epoch time: 111.23 s\n",
      "2024-12-23 09:18:23.064481: \n",
      "2024-12-23 09:18:23.064592: Epoch 305\n",
      "2024-12-23 09:18:23.064671: Current learning rate: 0.00721\n",
      "2024-12-23 09:20:14.290199: train_loss -0.6865\n",
      "2024-12-23 09:20:14.290359: val_loss -0.7052\n",
      "2024-12-23 09:20:14.290397: Pseudo dice [np.float32(0.7708)]\n",
      "2024-12-23 09:20:14.290436: Epoch time: 111.23 s\n",
      "2024-12-23 09:20:15.309556: \n",
      "2024-12-23 09:20:15.309785: Epoch 306\n",
      "2024-12-23 09:20:15.309870: Current learning rate: 0.0072\n",
      "2024-12-23 09:22:06.523162: train_loss -0.7304\n",
      "2024-12-23 09:22:06.523511: val_loss -0.5552\n",
      "2024-12-23 09:22:06.523712: Pseudo dice [np.float32(0.7085)]\n",
      "2024-12-23 09:22:06.523810: Epoch time: 111.21 s\n",
      "2024-12-23 09:22:07.120201: \n",
      "2024-12-23 09:22:07.120488: Epoch 307\n",
      "2024-12-23 09:22:07.120584: Current learning rate: 0.00719\n",
      "2024-12-23 09:23:58.382794: train_loss -0.7102\n",
      "2024-12-23 09:23:58.382982: val_loss -0.7253\n",
      "2024-12-23 09:23:58.383051: Pseudo dice [np.float32(0.782)]\n",
      "2024-12-23 09:23:58.383160: Epoch time: 111.26 s\n",
      "2024-12-23 09:23:58.970062: \n",
      "2024-12-23 09:23:58.970219: Epoch 308\n",
      "2024-12-23 09:23:58.970293: Current learning rate: 0.00718\n",
      "2024-12-23 09:25:50.253911: train_loss -0.6908\n",
      "2024-12-23 09:25:50.254039: val_loss -0.6547\n",
      "2024-12-23 09:25:50.254073: Pseudo dice [np.float32(0.7214)]\n",
      "2024-12-23 09:25:50.254109: Epoch time: 111.28 s\n",
      "2024-12-23 09:25:50.848915: \n",
      "2024-12-23 09:25:50.849055: Epoch 309\n",
      "2024-12-23 09:25:50.849128: Current learning rate: 0.00717\n",
      "2024-12-23 09:27:42.089874: train_loss -0.6826\n",
      "2024-12-23 09:27:42.090380: val_loss -0.669\n",
      "2024-12-23 09:27:42.090425: Pseudo dice [np.float32(0.7521)]\n",
      "2024-12-23 09:27:42.090463: Epoch time: 111.24 s\n",
      "2024-12-23 09:27:42.678066: \n",
      "2024-12-23 09:27:42.678291: Epoch 310\n",
      "2024-12-23 09:27:42.678377: Current learning rate: 0.00716\n",
      "2024-12-23 09:29:34.021210: train_loss -0.7199\n",
      "2024-12-23 09:29:34.021410: val_loss -0.607\n",
      "2024-12-23 09:29:34.021459: Pseudo dice [np.float32(0.6793)]\n",
      "2024-12-23 09:29:34.021509: Epoch time: 111.34 s\n",
      "2024-12-23 09:29:34.615043: \n",
      "2024-12-23 09:29:34.615173: Epoch 311\n",
      "2024-12-23 09:29:34.615243: Current learning rate: 0.00715\n",
      "2024-12-23 09:31:25.866246: train_loss -0.6848\n",
      "2024-12-23 09:31:25.866483: val_loss -0.7353\n",
      "2024-12-23 09:31:25.866530: Pseudo dice [np.float32(0.7336)]\n",
      "2024-12-23 09:31:25.866569: Epoch time: 111.25 s\n",
      "2024-12-23 09:31:26.457623: \n",
      "2024-12-23 09:31:26.457823: Epoch 312\n",
      "2024-12-23 09:31:26.457949: Current learning rate: 0.00714\n",
      "2024-12-23 09:33:17.766876: train_loss -0.6612\n",
      "2024-12-23 09:33:17.767034: val_loss -0.6986\n",
      "2024-12-23 09:33:17.767069: Pseudo dice [np.float32(0.7787)]\n",
      "2024-12-23 09:33:17.767106: Epoch time: 111.31 s\n",
      "2024-12-23 09:33:18.360300: \n",
      "2024-12-23 09:33:18.360532: Epoch 313\n",
      "2024-12-23 09:33:18.360610: Current learning rate: 0.00713\n",
      "2024-12-23 09:35:09.610255: train_loss -0.7225\n",
      "2024-12-23 09:35:09.610544: val_loss -0.6909\n",
      "2024-12-23 09:35:09.610636: Pseudo dice [np.float32(0.7528)]\n",
      "2024-12-23 09:35:09.610746: Epoch time: 111.25 s\n",
      "2024-12-23 09:35:10.196900: \n",
      "2024-12-23 09:35:10.197348: Epoch 314\n",
      "2024-12-23 09:35:10.197522: Current learning rate: 0.00712\n",
      "2024-12-23 09:37:01.455455: train_loss -0.7111\n",
      "2024-12-23 09:37:01.455601: val_loss -0.6685\n",
      "2024-12-23 09:37:01.455637: Pseudo dice [np.float32(0.7527)]\n",
      "2024-12-23 09:37:01.455674: Epoch time: 111.26 s\n",
      "2024-12-23 09:37:02.047102: \n",
      "2024-12-23 09:37:02.047376: Epoch 315\n",
      "2024-12-23 09:37:02.047542: Current learning rate: 0.00711\n",
      "2024-12-23 09:38:53.333750: train_loss -0.7089\n",
      "2024-12-23 09:38:53.333907: val_loss -0.6955\n",
      "2024-12-23 09:38:53.333942: Pseudo dice [np.float32(0.7706)]\n",
      "2024-12-23 09:38:53.333980: Epoch time: 111.29 s\n",
      "2024-12-23 09:38:53.936551: \n",
      "2024-12-23 09:38:53.936953: Epoch 316\n",
      "2024-12-23 09:38:53.937075: Current learning rate: 0.0071\n",
      "2024-12-23 09:40:45.205341: train_loss -0.7496\n",
      "2024-12-23 09:40:45.205493: val_loss -0.6615\n",
      "2024-12-23 09:40:45.205649: Pseudo dice [np.float32(0.713)]\n",
      "2024-12-23 09:40:45.205772: Epoch time: 111.27 s\n",
      "2024-12-23 09:40:45.802352: \n",
      "2024-12-23 09:40:45.802642: Epoch 317\n",
      "2024-12-23 09:40:45.802724: Current learning rate: 0.0071\n",
      "2024-12-23 09:42:37.105591: train_loss -0.7809\n",
      "2024-12-23 09:42:37.105727: val_loss -0.7216\n",
      "2024-12-23 09:42:37.105764: Pseudo dice [np.float32(0.7967)]\n",
      "2024-12-23 09:42:37.105802: Epoch time: 111.3 s\n",
      "2024-12-23 09:42:37.694273: \n",
      "2024-12-23 09:42:37.694654: Epoch 318\n",
      "2024-12-23 09:42:37.694797: Current learning rate: 0.00709\n",
      "2024-12-23 09:44:28.979634: train_loss -0.7478\n",
      "2024-12-23 09:44:28.979802: val_loss -0.6818\n",
      "2024-12-23 09:44:28.979843: Pseudo dice [np.float32(0.7493)]\n",
      "2024-12-23 09:44:28.979883: Epoch time: 111.29 s\n",
      "2024-12-23 09:44:29.565319: \n",
      "2024-12-23 09:44:29.565622: Epoch 319\n",
      "2024-12-23 09:44:29.565828: Current learning rate: 0.00708\n",
      "2024-12-23 09:46:20.809831: train_loss -0.7633\n",
      "2024-12-23 09:46:20.809971: val_loss -0.7306\n",
      "2024-12-23 09:46:20.810004: Pseudo dice [np.float32(0.8065)]\n",
      "2024-12-23 09:46:20.810041: Epoch time: 111.25 s\n",
      "2024-12-23 09:46:21.396880: \n",
      "2024-12-23 09:46:21.396978: Epoch 320\n",
      "2024-12-23 09:46:21.397051: Current learning rate: 0.00707\n",
      "2024-12-23 09:48:12.651565: train_loss -0.7828\n",
      "2024-12-23 09:48:12.651723: val_loss -0.619\n",
      "2024-12-23 09:48:12.651760: Pseudo dice [np.float32(0.6597)]\n",
      "2024-12-23 09:48:12.651796: Epoch time: 111.26 s\n",
      "2024-12-23 09:48:13.241325: \n",
      "2024-12-23 09:48:13.241424: Epoch 321\n",
      "2024-12-23 09:48:13.241495: Current learning rate: 0.00706\n",
      "2024-12-23 09:50:04.532045: train_loss -0.7357\n",
      "2024-12-23 09:50:04.532326: val_loss -0.7109\n",
      "2024-12-23 09:50:04.532366: Pseudo dice [np.float32(0.7506)]\n",
      "2024-12-23 09:50:04.532402: Epoch time: 111.29 s\n",
      "2024-12-23 09:50:05.122378: \n",
      "2024-12-23 09:50:05.122702: Epoch 322\n",
      "2024-12-23 09:50:05.122942: Current learning rate: 0.00705\n",
      "2024-12-23 09:51:56.347704: train_loss -0.7653\n",
      "2024-12-23 09:51:56.347850: val_loss -0.744\n",
      "2024-12-23 09:51:56.347886: Pseudo dice [np.float32(0.794)]\n",
      "2024-12-23 09:51:56.347924: Epoch time: 111.23 s\n",
      "2024-12-23 09:51:57.382542: \n",
      "2024-12-23 09:51:57.382859: Epoch 323\n",
      "2024-12-23 09:51:57.382957: Current learning rate: 0.00704\n",
      "2024-12-23 09:53:48.603310: train_loss -0.7685\n",
      "2024-12-23 09:53:48.603514: val_loss -0.6449\n",
      "2024-12-23 09:53:48.603694: Pseudo dice [np.float32(0.7313)]\n",
      "2024-12-23 09:53:48.603783: Epoch time: 111.22 s\n",
      "2024-12-23 09:53:49.189547: \n",
      "2024-12-23 09:53:49.189836: Epoch 324\n",
      "2024-12-23 09:53:49.190027: Current learning rate: 0.00703\n",
      "2024-12-23 09:55:40.489176: train_loss -0.7102\n",
      "2024-12-23 09:55:40.489418: val_loss -0.6781\n",
      "2024-12-23 09:55:40.489465: Pseudo dice [np.float32(0.723)]\n",
      "2024-12-23 09:55:40.489503: Epoch time: 111.3 s\n",
      "2024-12-23 09:55:41.077942: \n",
      "2024-12-23 09:55:41.078164: Epoch 325\n",
      "2024-12-23 09:55:41.078266: Current learning rate: 0.00702\n",
      "2024-12-23 09:57:32.345945: train_loss -0.7655\n",
      "2024-12-23 09:57:32.346116: val_loss -0.6708\n",
      "2024-12-23 09:57:32.346219: Pseudo dice [np.float32(0.752)]\n",
      "2024-12-23 09:57:32.346264: Epoch time: 111.27 s\n",
      "2024-12-23 09:57:32.931010: \n",
      "2024-12-23 09:57:32.931169: Epoch 326\n",
      "2024-12-23 09:57:32.931245: Current learning rate: 0.00701\n",
      "2024-12-23 09:59:24.196598: train_loss -0.6543\n",
      "2024-12-23 09:59:24.196735: val_loss -0.6477\n",
      "2024-12-23 09:59:24.196766: Pseudo dice [np.float32(0.6854)]\n",
      "2024-12-23 09:59:24.196811: Epoch time: 111.27 s\n",
      "2024-12-23 09:59:24.790105: \n",
      "2024-12-23 09:59:24.790248: Epoch 327\n",
      "2024-12-23 09:59:24.790323: Current learning rate: 0.007\n",
      "2024-12-23 10:01:16.051464: train_loss -0.6694\n",
      "2024-12-23 10:01:16.051611: val_loss -0.7048\n",
      "2024-12-23 10:01:16.051646: Pseudo dice [np.float32(0.7742)]\n",
      "2024-12-23 10:01:16.051682: Epoch time: 111.26 s\n",
      "2024-12-23 10:01:16.653124: \n",
      "2024-12-23 10:01:16.653541: Epoch 328\n",
      "2024-12-23 10:01:16.653678: Current learning rate: 0.00699\n",
      "2024-12-23 10:03:07.932366: train_loss -0.7151\n",
      "2024-12-23 10:03:07.932705: val_loss -0.6345\n",
      "2024-12-23 10:03:07.932744: Pseudo dice [np.float32(0.6618)]\n",
      "2024-12-23 10:03:07.932784: Epoch time: 111.28 s\n",
      "2024-12-23 10:03:08.517632: \n",
      "2024-12-23 10:03:08.517764: Epoch 329\n",
      "2024-12-23 10:03:08.518189: Current learning rate: 0.00698\n",
      "2024-12-23 10:04:59.770800: train_loss -0.7079\n",
      "2024-12-23 10:04:59.770947: val_loss -0.7516\n",
      "2024-12-23 10:04:59.770982: Pseudo dice [np.float32(0.8333)]\n",
      "2024-12-23 10:04:59.771119: Epoch time: 111.25 s\n",
      "2024-12-23 10:05:00.369331: \n",
      "2024-12-23 10:05:00.369693: Epoch 330\n",
      "2024-12-23 10:05:00.369786: Current learning rate: 0.00697\n",
      "2024-12-23 10:06:51.595063: train_loss -0.7349\n",
      "2024-12-23 10:06:51.595206: val_loss -0.7666\n",
      "2024-12-23 10:06:51.595242: Pseudo dice [np.float32(0.8309)]\n",
      "2024-12-23 10:06:51.595278: Epoch time: 111.23 s\n",
      "2024-12-23 10:06:52.179018: \n",
      "2024-12-23 10:06:52.179133: Epoch 331\n",
      "2024-12-23 10:06:52.179219: Current learning rate: 0.00696\n",
      "2024-12-23 10:08:43.409047: train_loss -0.763\n",
      "2024-12-23 10:08:43.409307: val_loss -0.724\n",
      "2024-12-23 10:08:43.409511: Pseudo dice [np.float32(0.7828)]\n",
      "2024-12-23 10:08:43.409566: Epoch time: 111.23 s\n",
      "2024-12-23 10:08:44.002455: \n",
      "2024-12-23 10:08:44.002749: Epoch 332\n",
      "2024-12-23 10:08:44.002908: Current learning rate: 0.00696\n",
      "2024-12-23 10:10:35.221819: train_loss -0.7709\n",
      "2024-12-23 10:10:35.221982: val_loss -0.6882\n",
      "2024-12-23 10:10:35.222025: Pseudo dice [np.float32(0.7951)]\n",
      "2024-12-23 10:10:35.222142: Epoch time: 111.22 s\n",
      "2024-12-23 10:10:35.821736: \n",
      "2024-12-23 10:10:35.821857: Epoch 333\n",
      "2024-12-23 10:10:35.821932: Current learning rate: 0.00695\n",
      "2024-12-23 10:12:27.091685: train_loss -0.7605\n",
      "2024-12-23 10:12:27.091852: val_loss -0.7281\n",
      "2024-12-23 10:12:27.091888: Pseudo dice [np.float32(0.8165)]\n",
      "2024-12-23 10:12:27.091926: Epoch time: 111.27 s\n",
      "2024-12-23 10:12:27.676583: \n",
      "2024-12-23 10:12:27.676806: Epoch 334\n",
      "2024-12-23 10:12:27.676884: Current learning rate: 0.00694\n",
      "2024-12-23 10:14:18.943569: train_loss -0.7465\n",
      "2024-12-23 10:14:18.943830: val_loss -0.7403\n",
      "2024-12-23 10:14:18.943880: Pseudo dice [np.float32(0.8243)]\n",
      "2024-12-23 10:14:18.943919: Epoch time: 111.27 s\n",
      "2024-12-23 10:14:18.943942: Yayy! New best EMA pseudo Dice: 0.772599995136261\n",
      "2024-12-23 10:14:19.768191: \n",
      "2024-12-23 10:14:19.768304: Epoch 335\n",
      "2024-12-23 10:14:19.768375: Current learning rate: 0.00693\n",
      "2024-12-23 10:16:11.064780: train_loss -0.7838\n",
      "2024-12-23 10:16:11.064933: val_loss -0.7342\n",
      "2024-12-23 10:16:11.064970: Pseudo dice [np.float32(0.8029)]\n",
      "2024-12-23 10:16:11.065009: Epoch time: 111.3 s\n",
      "2024-12-23 10:16:11.065031: Yayy! New best EMA pseudo Dice: 0.775600016117096\n",
      "2024-12-23 10:16:11.897710: \n",
      "2024-12-23 10:16:11.898086: Epoch 336\n",
      "2024-12-23 10:16:11.898244: Current learning rate: 0.00692\n",
      "2024-12-23 10:18:03.158699: train_loss -0.7962\n",
      "2024-12-23 10:18:03.159024: val_loss -0.699\n",
      "2024-12-23 10:18:03.159106: Pseudo dice [np.float32(0.7992)]\n",
      "2024-12-23 10:18:03.159152: Epoch time: 111.26 s\n",
      "2024-12-23 10:18:03.159175: Yayy! New best EMA pseudo Dice: 0.777999997138977\n",
      "2024-12-23 10:18:03.991372: \n",
      "2024-12-23 10:18:03.991540: Epoch 337\n",
      "2024-12-23 10:18:03.991613: Current learning rate: 0.00691\n",
      "2024-12-23 10:19:55.205570: train_loss -0.7703\n",
      "2024-12-23 10:19:55.206018: val_loss -0.7254\n",
      "2024-12-23 10:19:55.206061: Pseudo dice [np.float32(0.8053)]\n",
      "2024-12-23 10:19:55.206100: Epoch time: 111.21 s\n",
      "2024-12-23 10:19:55.206140: Yayy! New best EMA pseudo Dice: 0.7807000279426575\n",
      "2024-12-23 10:19:56.032393: \n",
      "2024-12-23 10:19:56.032779: Epoch 338\n",
      "2024-12-23 10:19:56.032893: Current learning rate: 0.0069\n",
      "2024-12-23 10:21:47.298054: train_loss -0.7893\n",
      "2024-12-23 10:21:47.298188: val_loss -0.6869\n",
      "2024-12-23 10:21:47.298220: Pseudo dice [np.float32(0.7042)]\n",
      "2024-12-23 10:21:47.298255: Epoch time: 111.27 s\n",
      "2024-12-23 10:21:47.894914: \n",
      "2024-12-23 10:21:47.895019: Epoch 339\n",
      "2024-12-23 10:21:47.895092: Current learning rate: 0.00689\n",
      "2024-12-23 10:23:39.549725: train_loss -0.7812\n",
      "2024-12-23 10:23:39.549866: val_loss -0.6701\n",
      "2024-12-23 10:23:39.549906: Pseudo dice [np.float32(0.716)]\n",
      "2024-12-23 10:23:39.549942: Epoch time: 111.66 s\n",
      "2024-12-23 10:23:40.162338: \n",
      "2024-12-23 10:23:40.162804: Epoch 340\n",
      "2024-12-23 10:23:40.162897: Current learning rate: 0.00688\n",
      "2024-12-23 10:25:31.423160: train_loss -0.7659\n",
      "2024-12-23 10:25:31.423306: val_loss -0.7606\n",
      "2024-12-23 10:25:31.423341: Pseudo dice [np.float32(0.8028)]\n",
      "2024-12-23 10:25:31.423378: Epoch time: 111.26 s\n",
      "2024-12-23 10:25:32.018754: \n",
      "2024-12-23 10:25:32.018887: Epoch 341\n",
      "2024-12-23 10:25:32.018958: Current learning rate: 0.00687\n",
      "2024-12-23 10:27:23.259894: train_loss -0.7926\n",
      "2024-12-23 10:27:23.260035: val_loss -0.7073\n",
      "2024-12-23 10:27:23.260071: Pseudo dice [np.float32(0.7788)]\n",
      "2024-12-23 10:27:23.260115: Epoch time: 111.24 s\n",
      "2024-12-23 10:27:23.857993: \n",
      "2024-12-23 10:27:23.858288: Epoch 342\n",
      "2024-12-23 10:27:23.858446: Current learning rate: 0.00686\n",
      "2024-12-23 10:29:15.206404: train_loss -0.7919\n",
      "2024-12-23 10:29:15.206743: val_loss -0.6862\n",
      "2024-12-23 10:29:15.206951: Pseudo dice [np.float32(0.7215)]\n",
      "2024-12-23 10:29:15.207036: Epoch time: 111.35 s\n",
      "2024-12-23 10:29:15.795171: \n",
      "2024-12-23 10:29:15.795591: Epoch 343\n",
      "2024-12-23 10:29:15.795673: Current learning rate: 0.00685\n",
      "2024-12-23 10:31:07.091105: train_loss -0.7766\n",
      "2024-12-23 10:31:07.091263: val_loss -0.7085\n",
      "2024-12-23 10:31:07.091299: Pseudo dice [np.float32(0.7834)]\n",
      "2024-12-23 10:31:07.091337: Epoch time: 111.3 s\n",
      "2024-12-23 10:31:07.699310: \n",
      "2024-12-23 10:31:07.699668: Epoch 344\n",
      "2024-12-23 10:31:07.699910: Current learning rate: 0.00684\n",
      "2024-12-23 10:32:59.023818: train_loss -0.8014\n",
      "2024-12-23 10:32:59.023959: val_loss -0.7158\n",
      "2024-12-23 10:32:59.023993: Pseudo dice [np.float32(0.7902)]\n",
      "2024-12-23 10:32:59.024031: Epoch time: 111.33 s\n",
      "2024-12-23 10:32:59.621813: \n",
      "2024-12-23 10:32:59.622209: Epoch 345\n",
      "2024-12-23 10:32:59.622293: Current learning rate: 0.00683\n",
      "2024-12-23 10:34:50.917753: train_loss -0.7658\n",
      "2024-12-23 10:34:50.917896: val_loss -0.7737\n",
      "2024-12-23 10:34:50.917933: Pseudo dice [np.float32(0.82)]\n",
      "2024-12-23 10:34:50.917970: Epoch time: 111.3 s\n",
      "2024-12-23 10:34:51.523112: \n",
      "2024-12-23 10:34:51.523566: Epoch 346\n",
      "2024-12-23 10:34:51.523673: Current learning rate: 0.00682\n",
      "2024-12-23 10:36:42.779217: train_loss -0.7742\n",
      "2024-12-23 10:36:42.779358: val_loss -0.6913\n",
      "2024-12-23 10:36:42.779391: Pseudo dice [np.float32(0.775)]\n",
      "2024-12-23 10:36:42.779426: Epoch time: 111.26 s\n",
      "2024-12-23 10:36:43.376481: \n",
      "2024-12-23 10:36:43.376608: Epoch 347\n",
      "2024-12-23 10:36:43.376679: Current learning rate: 0.00681\n",
      "2024-12-23 10:38:34.628442: train_loss -0.7775\n",
      "2024-12-23 10:38:34.628662: val_loss -0.5732\n",
      "2024-12-23 10:38:34.628697: Pseudo dice [np.float32(0.6893)]\n",
      "2024-12-23 10:38:34.628734: Epoch time: 111.25 s\n",
      "2024-12-23 10:38:35.225431: \n",
      "2024-12-23 10:38:35.225991: Epoch 348\n",
      "2024-12-23 10:38:35.226161: Current learning rate: 0.0068\n",
      "2024-12-23 10:40:26.481812: train_loss -0.7332\n",
      "2024-12-23 10:40:26.482036: val_loss -0.6521\n",
      "2024-12-23 10:40:26.482073: Pseudo dice [np.float32(0.69)]\n",
      "2024-12-23 10:40:26.482111: Epoch time: 111.26 s\n",
      "2024-12-23 10:40:27.076698: \n",
      "2024-12-23 10:40:27.076818: Epoch 349\n",
      "2024-12-23 10:40:27.076893: Current learning rate: 0.0068\n",
      "2024-12-23 10:42:18.316300: train_loss -0.7755\n",
      "2024-12-23 10:42:18.316505: val_loss -0.6816\n",
      "2024-12-23 10:42:18.316590: Pseudo dice [np.float32(0.7408)]\n",
      "2024-12-23 10:42:18.316639: Epoch time: 111.24 s\n",
      "2024-12-23 10:42:19.139999: \n",
      "2024-12-23 10:42:19.140110: Epoch 350\n",
      "2024-12-23 10:42:19.140185: Current learning rate: 0.00679\n",
      "2024-12-23 10:44:10.396603: train_loss -0.7738\n",
      "2024-12-23 10:44:10.396740: val_loss -0.7225\n",
      "2024-12-23 10:44:10.396775: Pseudo dice [np.float32(0.7892)]\n",
      "2024-12-23 10:44:10.396812: Epoch time: 111.26 s\n",
      "2024-12-23 10:44:10.989122: \n",
      "2024-12-23 10:44:10.989418: Epoch 351\n",
      "2024-12-23 10:44:10.989528: Current learning rate: 0.00678\n",
      "2024-12-23 10:46:07.156117: train_loss -0.7733\n",
      "2024-12-23 10:46:07.156287: val_loss -0.6768\n",
      "2024-12-23 10:46:07.156324: Pseudo dice [np.float32(0.7207)]\n",
      "2024-12-23 10:46:07.156369: Epoch time: 116.17 s\n",
      "2024-12-23 10:46:07.800676: \n",
      "2024-12-23 10:46:07.801070: Epoch 352\n",
      "2024-12-23 10:46:07.801196: Current learning rate: 0.00677\n",
      "2024-12-23 10:48:02.123418: train_loss -0.7255\n",
      "2024-12-23 10:48:02.123549: val_loss -0.7324\n",
      "2024-12-23 10:48:02.123583: Pseudo dice [np.float32(0.7862)]\n",
      "2024-12-23 10:48:02.123619: Epoch time: 114.32 s\n",
      "2024-12-23 10:48:02.721685: \n",
      "2024-12-23 10:48:02.721866: Epoch 353\n",
      "2024-12-23 10:48:02.721941: Current learning rate: 0.00676\n",
      "2024-12-23 10:49:54.305527: train_loss -0.7775\n",
      "2024-12-23 10:49:54.305679: val_loss -0.7085\n",
      "2024-12-23 10:49:54.305713: Pseudo dice [np.float32(0.7962)]\n",
      "2024-12-23 10:49:54.305750: Epoch time: 111.58 s\n",
      "2024-12-23 10:49:54.905041: \n",
      "2024-12-23 10:49:54.905138: Epoch 354\n",
      "2024-12-23 10:49:54.905213: Current learning rate: 0.00675\n",
      "2024-12-23 10:51:47.331759: train_loss -0.745\n",
      "2024-12-23 10:51:47.331921: val_loss -0.7542\n",
      "2024-12-23 10:51:47.331958: Pseudo dice [np.float32(0.8202)]\n",
      "2024-12-23 10:51:47.332012: Epoch time: 112.43 s\n",
      "2024-12-23 10:51:47.930569: \n",
      "2024-12-23 10:51:47.930952: Epoch 355\n",
      "2024-12-23 10:51:47.931060: Current learning rate: 0.00674\n",
      "2024-12-23 10:53:40.324133: train_loss -0.7633\n",
      "2024-12-23 10:53:40.324373: val_loss -0.6434\n",
      "2024-12-23 10:53:40.324481: Pseudo dice [np.float32(0.7072)]\n",
      "2024-12-23 10:53:40.324637: Epoch time: 112.39 s\n",
      "2024-12-23 10:53:41.381571: \n",
      "2024-12-23 10:53:41.381691: Epoch 356\n",
      "2024-12-23 10:53:41.381847: Current learning rate: 0.00673\n",
      "2024-12-23 10:55:32.993552: train_loss -0.7479\n",
      "2024-12-23 10:55:32.993847: val_loss -0.7391\n",
      "2024-12-23 10:55:32.993897: Pseudo dice [np.float32(0.7964)]\n",
      "2024-12-23 10:55:32.994084: Epoch time: 111.61 s\n",
      "2024-12-23 10:55:33.595128: \n",
      "2024-12-23 10:55:33.595418: Epoch 357\n",
      "2024-12-23 10:55:33.595512: Current learning rate: 0.00672\n",
      "2024-12-23 10:57:25.317427: train_loss -0.7398\n",
      "2024-12-23 10:57:25.317584: val_loss -0.6747\n",
      "2024-12-23 10:57:25.317619: Pseudo dice [np.float32(0.7377)]\n",
      "2024-12-23 10:57:25.317657: Epoch time: 111.72 s\n",
      "2024-12-23 10:57:25.928886: \n",
      "2024-12-23 10:57:25.929332: Epoch 358\n",
      "2024-12-23 10:57:25.929598: Current learning rate: 0.00671\n",
      "2024-12-23 10:59:17.564096: train_loss -0.7395\n",
      "2024-12-23 10:59:17.564550: val_loss -0.6725\n",
      "2024-12-23 10:59:17.564662: Pseudo dice [np.float32(0.7549)]\n",
      "2024-12-23 10:59:17.564708: Epoch time: 111.64 s\n",
      "2024-12-23 10:59:18.169013: \n",
      "2024-12-23 10:59:18.169167: Epoch 359\n",
      "2024-12-23 10:59:18.169238: Current learning rate: 0.0067\n",
      "2024-12-23 11:01:09.783279: train_loss -0.7256\n",
      "2024-12-23 11:01:09.783669: val_loss -0.7183\n",
      "2024-12-23 11:01:09.783711: Pseudo dice [np.float32(0.7739)]\n",
      "2024-12-23 11:01:09.783751: Epoch time: 111.61 s\n",
      "2024-12-23 11:01:10.383227: \n",
      "2024-12-23 11:01:10.383358: Epoch 360\n",
      "2024-12-23 11:01:10.383432: Current learning rate: 0.00669\n",
      "2024-12-23 11:03:02.014278: train_loss -0.7706\n",
      "2024-12-23 11:03:02.014566: val_loss -0.5743\n",
      "2024-12-23 11:03:02.014629: Pseudo dice [np.float32(0.5668)]\n",
      "2024-12-23 11:03:02.014670: Epoch time: 111.63 s\n",
      "2024-12-23 11:03:02.618927: \n",
      "2024-12-23 11:03:02.619076: Epoch 361\n",
      "2024-12-23 11:03:02.619148: Current learning rate: 0.00668\n",
      "2024-12-23 11:04:54.211302: train_loss -0.7726\n",
      "2024-12-23 11:04:54.211534: val_loss -0.7195\n",
      "2024-12-23 11:04:54.211572: Pseudo dice [np.float32(0.7933)]\n",
      "2024-12-23 11:04:54.211612: Epoch time: 111.59 s\n",
      "2024-12-23 11:04:54.810438: \n",
      "2024-12-23 11:04:54.810839: Epoch 362\n",
      "2024-12-23 11:04:54.810916: Current learning rate: 0.00667\n",
      "2024-12-23 11:06:46.425319: train_loss -0.766\n",
      "2024-12-23 11:06:46.425477: val_loss -0.6399\n",
      "2024-12-23 11:06:46.425514: Pseudo dice [np.float32(0.7417)]\n",
      "2024-12-23 11:06:46.425551: Epoch time: 111.62 s\n",
      "2024-12-23 11:06:47.027386: \n",
      "2024-12-23 11:06:47.027783: Epoch 363\n",
      "2024-12-23 11:06:47.027923: Current learning rate: 0.00666\n",
      "2024-12-23 11:08:38.619151: train_loss -0.7941\n",
      "2024-12-23 11:08:38.619603: val_loss -0.6826\n",
      "2024-12-23 11:08:38.619743: Pseudo dice [np.float32(0.7151)]\n",
      "2024-12-23 11:08:38.619828: Epoch time: 111.59 s\n",
      "2024-12-23 11:08:39.232929: \n",
      "2024-12-23 11:08:39.233056: Epoch 364\n",
      "2024-12-23 11:08:39.233161: Current learning rate: 0.00665\n",
      "2024-12-23 11:10:30.816917: train_loss -0.7324\n",
      "2024-12-23 11:10:30.817180: val_loss -0.6368\n",
      "2024-12-23 11:10:30.817225: Pseudo dice [np.float32(0.6593)]\n",
      "2024-12-23 11:10:30.817262: Epoch time: 111.58 s\n",
      "2024-12-23 11:10:31.424711: \n",
      "2024-12-23 11:10:31.425074: Epoch 365\n",
      "2024-12-23 11:10:31.425295: Current learning rate: 0.00665\n",
      "2024-12-23 11:12:23.268478: train_loss -0.7801\n",
      "2024-12-23 11:12:23.268631: val_loss -0.7645\n",
      "2024-12-23 11:12:23.268695: Pseudo dice [np.float32(0.8172)]\n",
      "2024-12-23 11:12:23.269018: Epoch time: 111.84 s\n",
      "2024-12-23 11:12:23.865072: \n",
      "2024-12-23 11:12:23.865185: Epoch 366\n",
      "2024-12-23 11:12:23.865258: Current learning rate: 0.00664\n",
      "2024-12-23 11:14:15.615486: train_loss -0.7732\n",
      "2024-12-23 11:14:15.615646: val_loss -0.7651\n",
      "2024-12-23 11:14:15.615682: Pseudo dice [np.float32(0.8346)]\n",
      "2024-12-23 11:14:15.615717: Epoch time: 111.75 s\n",
      "2024-12-23 11:14:16.222436: \n",
      "2024-12-23 11:14:16.222853: Epoch 367\n",
      "2024-12-23 11:14:16.223042: Current learning rate: 0.00663\n",
      "2024-12-23 11:16:07.809027: train_loss -0.7966\n",
      "2024-12-23 11:16:07.809187: val_loss -0.7061\n",
      "2024-12-23 11:16:07.809223: Pseudo dice [np.float32(0.744)]\n",
      "2024-12-23 11:16:07.809261: Epoch time: 111.59 s\n",
      "2024-12-23 11:16:08.406630: \n",
      "2024-12-23 11:16:08.406746: Epoch 368\n",
      "2024-12-23 11:16:08.406820: Current learning rate: 0.00662\n",
      "2024-12-23 11:18:00.018282: train_loss -0.7474\n",
      "2024-12-23 11:18:00.018487: val_loss -0.7088\n",
      "2024-12-23 11:18:00.018523: Pseudo dice [np.float32(0.7545)]\n",
      "2024-12-23 11:18:00.018560: Epoch time: 111.61 s\n",
      "2024-12-23 11:18:00.616742: \n",
      "2024-12-23 11:18:00.616847: Epoch 369\n",
      "2024-12-23 11:18:00.616920: Current learning rate: 0.00661\n",
      "2024-12-23 11:19:52.156933: train_loss -0.7591\n",
      "2024-12-23 11:19:52.157085: val_loss -0.7123\n",
      "2024-12-23 11:19:52.157311: Pseudo dice [np.float32(0.7488)]\n",
      "2024-12-23 11:19:52.157627: Epoch time: 111.54 s\n",
      "2024-12-23 11:19:52.770195: \n",
      "2024-12-23 11:19:52.770378: Epoch 370\n",
      "2024-12-23 11:19:52.770468: Current learning rate: 0.0066\n",
      "2024-12-23 11:21:44.409762: train_loss -0.7468\n",
      "2024-12-23 11:21:44.409911: val_loss -0.7044\n",
      "2024-12-23 11:21:44.409945: Pseudo dice [np.float32(0.7555)]\n",
      "2024-12-23 11:21:44.409982: Epoch time: 111.64 s\n",
      "2024-12-23 11:21:45.013787: \n",
      "2024-12-23 11:21:45.013886: Epoch 371\n",
      "2024-12-23 11:21:45.013957: Current learning rate: 0.00659\n",
      "2024-12-23 11:23:36.553082: train_loss -0.7901\n",
      "2024-12-23 11:23:36.553231: val_loss -0.6694\n",
      "2024-12-23 11:23:36.553266: Pseudo dice [np.float32(0.7476)]\n",
      "2024-12-23 11:23:36.553303: Epoch time: 111.54 s\n",
      "2024-12-23 11:23:37.146340: \n",
      "2024-12-23 11:23:37.146595: Epoch 372\n",
      "2024-12-23 11:23:37.146770: Current learning rate: 0.00658\n",
      "2024-12-23 11:25:28.681588: train_loss -0.815\n",
      "2024-12-23 11:25:28.681730: val_loss -0.7316\n",
      "2024-12-23 11:25:28.681765: Pseudo dice [np.float32(0.795)]\n",
      "2024-12-23 11:25:28.681803: Epoch time: 111.54 s\n",
      "2024-12-23 11:25:29.744262: \n",
      "2024-12-23 11:25:29.744712: Epoch 373\n",
      "2024-12-23 11:25:29.744956: Current learning rate: 0.00657\n",
      "2024-12-23 11:27:21.563347: train_loss -0.7811\n",
      "2024-12-23 11:27:21.563499: val_loss -0.7504\n",
      "2024-12-23 11:27:21.563529: Pseudo dice [np.float32(0.7731)]\n",
      "2024-12-23 11:27:21.563621: Epoch time: 111.82 s\n",
      "2024-12-23 11:27:22.158467: \n",
      "2024-12-23 11:27:22.158926: Epoch 374\n",
      "2024-12-23 11:27:22.159025: Current learning rate: 0.00656\n",
      "2024-12-23 11:29:13.891731: train_loss -0.7886\n",
      "2024-12-23 11:29:13.891918: val_loss -0.7056\n",
      "2024-12-23 11:29:13.891951: Pseudo dice [np.float32(0.7699)]\n",
      "2024-12-23 11:29:13.891988: Epoch time: 111.73 s\n",
      "2024-12-23 11:29:14.484334: \n",
      "2024-12-23 11:29:14.484475: Epoch 375\n",
      "2024-12-23 11:29:14.484550: Current learning rate: 0.00655\n",
      "2024-12-23 11:31:06.030421: train_loss -0.7778\n",
      "2024-12-23 11:31:06.030577: val_loss -0.6623\n",
      "2024-12-23 11:31:06.030617: Pseudo dice [np.float32(0.7177)]\n",
      "2024-12-23 11:31:06.030658: Epoch time: 111.55 s\n",
      "2024-12-23 11:31:06.615505: \n",
      "2024-12-23 11:31:06.615705: Epoch 376\n",
      "2024-12-23 11:31:06.615780: Current learning rate: 0.00654\n",
      "2024-12-23 11:32:58.175950: train_loss -0.7907\n",
      "2024-12-23 11:32:58.176146: val_loss -0.7747\n",
      "2024-12-23 11:32:58.176193: Pseudo dice [np.float32(0.8398)]\n",
      "2024-12-23 11:32:58.176234: Epoch time: 111.56 s\n",
      "2024-12-23 11:32:58.767672: \n",
      "2024-12-23 11:32:58.767801: Epoch 377\n",
      "2024-12-23 11:32:58.767877: Current learning rate: 0.00653\n",
      "2024-12-23 11:34:50.336091: train_loss -0.7594\n",
      "2024-12-23 11:34:50.336447: val_loss -0.6577\n",
      "2024-12-23 11:34:50.336485: Pseudo dice [np.float32(0.7139)]\n",
      "2024-12-23 11:34:50.336524: Epoch time: 111.57 s\n",
      "2024-12-23 11:34:50.930939: \n",
      "2024-12-23 11:34:50.931059: Epoch 378\n",
      "2024-12-23 11:34:50.931283: Current learning rate: 0.00652\n",
      "2024-12-23 11:36:42.543712: train_loss -0.7495\n",
      "2024-12-23 11:36:42.543871: val_loss -0.7596\n",
      "2024-12-23 11:36:42.543907: Pseudo dice [np.float32(0.8313)]\n",
      "2024-12-23 11:36:42.543946: Epoch time: 111.61 s\n",
      "2024-12-23 11:36:43.137992: \n",
      "2024-12-23 11:36:43.138580: Epoch 379\n",
      "2024-12-23 11:36:43.138699: Current learning rate: 0.00651\n",
      "2024-12-23 11:38:34.806119: train_loss -0.76\n",
      "2024-12-23 11:38:34.806261: val_loss -0.7728\n",
      "2024-12-23 11:38:34.806294: Pseudo dice [np.float32(0.8274)]\n",
      "2024-12-23 11:38:34.806330: Epoch time: 111.67 s\n",
      "2024-12-23 11:38:35.393430: \n",
      "2024-12-23 11:38:35.393737: Epoch 380\n",
      "2024-12-23 11:38:35.393824: Current learning rate: 0.0065\n",
      "2024-12-23 11:40:26.611105: train_loss -0.7343\n",
      "2024-12-23 11:40:26.611290: val_loss -0.7481\n",
      "2024-12-23 11:40:26.611324: Pseudo dice [np.float32(0.8115)]\n",
      "2024-12-23 11:40:26.611360: Epoch time: 111.22 s\n",
      "2024-12-23 11:40:27.217604: \n",
      "2024-12-23 11:40:27.217707: Epoch 381\n",
      "2024-12-23 11:40:27.217778: Current learning rate: 0.00649\n",
      "2024-12-23 11:42:18.461629: train_loss -0.773\n",
      "2024-12-23 11:42:18.461860: val_loss -0.6929\n",
      "2024-12-23 11:42:18.461896: Pseudo dice [np.float32(0.7349)]\n",
      "2024-12-23 11:42:18.462057: Epoch time: 111.24 s\n",
      "2024-12-23 11:42:19.066144: \n",
      "2024-12-23 11:42:19.066332: Epoch 382\n",
      "2024-12-23 11:42:19.066416: Current learning rate: 0.00648\n",
      "2024-12-23 11:44:10.283369: train_loss -0.7468\n",
      "2024-12-23 11:44:10.283507: val_loss -0.7007\n",
      "2024-12-23 11:44:10.283541: Pseudo dice [np.float32(0.7568)]\n",
      "2024-12-23 11:44:10.283577: Epoch time: 111.22 s\n",
      "2024-12-23 11:44:10.882395: \n",
      "2024-12-23 11:44:10.882578: Epoch 383\n",
      "2024-12-23 11:44:10.882661: Current learning rate: 0.00648\n",
      "2024-12-23 11:46:02.064220: train_loss -0.7334\n",
      "2024-12-23 11:46:02.064362: val_loss -0.6594\n",
      "2024-12-23 11:46:02.064555: Pseudo dice [np.float32(0.7412)]\n",
      "2024-12-23 11:46:02.064615: Epoch time: 111.18 s\n",
      "2024-12-23 11:46:02.671160: \n",
      "2024-12-23 11:46:02.671481: Epoch 384\n",
      "2024-12-23 11:46:02.671566: Current learning rate: 0.00647\n",
      "2024-12-23 11:47:53.878963: train_loss -0.7815\n",
      "2024-12-23 11:47:53.879103: val_loss -0.6725\n",
      "2024-12-23 11:47:53.879138: Pseudo dice [np.float32(0.7696)]\n",
      "2024-12-23 11:47:53.879177: Epoch time: 111.21 s\n",
      "2024-12-23 11:47:54.480273: \n",
      "2024-12-23 11:47:54.480459: Epoch 385\n",
      "2024-12-23 11:47:54.480541: Current learning rate: 0.00646\n",
      "2024-12-23 11:49:45.722244: train_loss -0.7614\n",
      "2024-12-23 11:49:45.722392: val_loss -0.601\n",
      "2024-12-23 11:49:45.722428: Pseudo dice [np.float32(0.6116)]\n",
      "2024-12-23 11:49:45.722468: Epoch time: 111.24 s\n",
      "2024-12-23 11:49:46.320489: \n",
      "2024-12-23 11:49:46.320816: Epoch 386\n",
      "2024-12-23 11:49:46.320953: Current learning rate: 0.00645\n",
      "2024-12-23 11:51:37.541320: train_loss -0.7093\n",
      "2024-12-23 11:51:37.541463: val_loss -0.6554\n",
      "2024-12-23 11:51:37.541497: Pseudo dice [np.float32(0.6895)]\n",
      "2024-12-23 11:51:37.541535: Epoch time: 111.22 s\n",
      "2024-12-23 11:51:38.143732: \n",
      "2024-12-23 11:51:38.143900: Epoch 387\n",
      "2024-12-23 11:51:38.144034: Current learning rate: 0.00644\n",
      "2024-12-23 11:53:29.444488: train_loss -0.7389\n",
      "2024-12-23 11:53:29.444910: val_loss -0.6874\n",
      "2024-12-23 11:53:29.444988: Pseudo dice [np.float32(0.7761)]\n",
      "2024-12-23 11:53:29.445034: Epoch time: 111.3 s\n",
      "2024-12-23 11:53:30.044156: \n",
      "2024-12-23 11:53:30.044644: Epoch 388\n",
      "2024-12-23 11:53:30.044797: Current learning rate: 0.00643\n",
      "2024-12-23 11:55:21.280159: train_loss -0.7385\n",
      "2024-12-23 11:55:21.280746: val_loss -0.756\n",
      "2024-12-23 11:55:21.280832: Pseudo dice [np.float32(0.8236)]\n",
      "2024-12-23 11:55:21.280876: Epoch time: 111.24 s\n",
      "2024-12-23 11:55:21.866479: \n",
      "2024-12-23 11:55:21.866908: Epoch 389\n",
      "2024-12-23 11:55:21.866989: Current learning rate: 0.00642\n",
      "2024-12-23 11:57:13.129726: train_loss -0.7278\n",
      "2024-12-23 11:57:13.129907: val_loss -0.7581\n",
      "2024-12-23 11:57:13.129966: Pseudo dice [np.float32(0.8101)]\n",
      "2024-12-23 11:57:13.130009: Epoch time: 111.26 s\n",
      "2024-12-23 11:57:14.190863: \n",
      "2024-12-23 11:57:14.191051: Epoch 390\n",
      "2024-12-23 11:57:14.191150: Current learning rate: 0.00641\n",
      "2024-12-23 11:59:05.478062: train_loss -0.7772\n",
      "2024-12-23 11:59:05.478204: val_loss -0.7449\n",
      "2024-12-23 11:59:05.478238: Pseudo dice [np.float32(0.7879)]\n",
      "2024-12-23 11:59:05.478302: Epoch time: 111.29 s\n",
      "2024-12-23 11:59:06.092461: \n",
      "2024-12-23 11:59:06.092594: Epoch 391\n",
      "2024-12-23 11:59:06.092670: Current learning rate: 0.0064\n",
      "2024-12-23 12:00:57.402942: train_loss -0.7732\n",
      "2024-12-23 12:00:57.403085: val_loss -0.6289\n",
      "2024-12-23 12:00:57.403119: Pseudo dice [np.float32(0.7108)]\n",
      "2024-12-23 12:00:57.403155: Epoch time: 111.31 s\n",
      "2024-12-23 12:00:58.000018: \n",
      "2024-12-23 12:00:58.000203: Epoch 392\n",
      "2024-12-23 12:00:58.000282: Current learning rate: 0.00639\n",
      "2024-12-23 12:02:49.257652: train_loss -0.77\n",
      "2024-12-23 12:02:49.257787: val_loss -0.642\n",
      "2024-12-23 12:02:49.257820: Pseudo dice [np.float32(0.7287)]\n",
      "2024-12-23 12:02:49.257858: Epoch time: 111.26 s\n",
      "2024-12-23 12:02:49.863593: \n",
      "2024-12-23 12:02:49.863802: Epoch 393\n",
      "2024-12-23 12:02:49.863882: Current learning rate: 0.00638\n",
      "2024-12-23 12:04:41.152731: train_loss -0.7874\n",
      "2024-12-23 12:04:41.152881: val_loss -0.6798\n",
      "2024-12-23 12:04:41.152919: Pseudo dice [np.float32(0.7494)]\n",
      "2024-12-23 12:04:41.152957: Epoch time: 111.29 s\n",
      "2024-12-23 12:04:41.757481: \n",
      "2024-12-23 12:04:41.757879: Epoch 394\n",
      "2024-12-23 12:04:41.757975: Current learning rate: 0.00637\n",
      "2024-12-23 12:06:32.992001: train_loss -0.7964\n",
      "2024-12-23 12:06:32.992463: val_loss -0.6906\n",
      "2024-12-23 12:06:32.992544: Pseudo dice [np.float32(0.7643)]\n",
      "2024-12-23 12:06:32.992591: Epoch time: 111.24 s\n",
      "2024-12-23 12:06:33.591937: \n",
      "2024-12-23 12:06:33.592070: Epoch 395\n",
      "2024-12-23 12:06:33.592142: Current learning rate: 0.00636\n",
      "2024-12-23 12:08:24.854519: train_loss -0.805\n",
      "2024-12-23 12:08:24.854687: val_loss -0.7713\n",
      "2024-12-23 12:08:24.854721: Pseudo dice [np.float32(0.8079)]\n",
      "2024-12-23 12:08:24.854757: Epoch time: 111.26 s\n",
      "2024-12-23 12:08:25.475350: \n",
      "2024-12-23 12:08:25.475749: Epoch 396\n",
      "2024-12-23 12:08:25.475837: Current learning rate: 0.00635\n",
      "2024-12-23 12:10:16.686513: train_loss -0.7642\n",
      "2024-12-23 12:10:16.686674: val_loss -0.7486\n",
      "2024-12-23 12:10:16.686708: Pseudo dice [np.float32(0.8068)]\n",
      "2024-12-23 12:10:16.686746: Epoch time: 111.21 s\n",
      "2024-12-23 12:10:17.284029: \n",
      "2024-12-23 12:10:17.284144: Epoch 397\n",
      "2024-12-23 12:10:17.284215: Current learning rate: 0.00634\n",
      "2024-12-23 12:12:08.574209: train_loss -0.7121\n",
      "2024-12-23 12:12:08.574561: val_loss -0.6408\n",
      "2024-12-23 12:12:08.574732: Pseudo dice [np.float32(0.7088)]\n",
      "2024-12-23 12:12:08.574790: Epoch time: 111.29 s\n",
      "2024-12-23 12:12:09.182286: \n",
      "2024-12-23 12:12:09.182708: Epoch 398\n",
      "2024-12-23 12:12:09.182852: Current learning rate: 0.00633\n",
      "2024-12-23 12:14:00.444464: train_loss -0.714\n",
      "2024-12-23 12:14:00.444612: val_loss -0.7216\n",
      "2024-12-23 12:14:00.444646: Pseudo dice [np.float32(0.7925)]\n",
      "2024-12-23 12:14:00.444685: Epoch time: 111.26 s\n",
      "2024-12-23 12:14:01.049255: \n",
      "2024-12-23 12:14:01.049480: Epoch 399\n",
      "2024-12-23 12:14:01.049592: Current learning rate: 0.00632\n",
      "2024-12-23 12:15:52.338512: train_loss -0.7409\n",
      "2024-12-23 12:15:52.338729: val_loss -0.6914\n",
      "2024-12-23 12:15:52.338764: Pseudo dice [np.float32(0.7593)]\n",
      "2024-12-23 12:15:52.338800: Epoch time: 111.29 s\n",
      "2024-12-23 12:15:53.191677: \n",
      "2024-12-23 12:15:53.192009: Epoch 400\n",
      "2024-12-23 12:15:53.192190: Current learning rate: 0.00631\n",
      "2024-12-23 12:17:44.420552: train_loss -0.7701\n",
      "2024-12-23 12:17:44.420693: val_loss -0.6765\n",
      "2024-12-23 12:17:44.420727: Pseudo dice [np.float32(0.7521)]\n",
      "2024-12-23 12:17:44.420772: Epoch time: 111.23 s\n",
      "2024-12-23 12:17:45.018960: \n",
      "2024-12-23 12:17:45.019135: Epoch 401\n",
      "2024-12-23 12:17:45.019309: Current learning rate: 0.0063\n",
      "2024-12-23 12:19:36.278650: train_loss -0.7974\n",
      "2024-12-23 12:19:36.279016: val_loss -0.6983\n",
      "2024-12-23 12:19:36.279222: Pseudo dice [np.float32(0.7679)]\n",
      "2024-12-23 12:19:36.279279: Epoch time: 111.26 s\n",
      "2024-12-23 12:19:36.885860: \n",
      "2024-12-23 12:19:36.886126: Epoch 402\n",
      "2024-12-23 12:19:36.886258: Current learning rate: 0.0063\n",
      "2024-12-23 12:21:28.150179: train_loss -0.7805\n",
      "2024-12-23 12:21:28.150318: val_loss -0.7445\n",
      "2024-12-23 12:21:28.150351: Pseudo dice [np.float32(0.8095)]\n",
      "2024-12-23 12:21:28.150388: Epoch time: 111.26 s\n",
      "2024-12-23 12:21:28.746808: \n",
      "2024-12-23 12:21:28.746898: Epoch 403\n",
      "2024-12-23 12:21:28.746966: Current learning rate: 0.00629\n",
      "2024-12-23 12:23:19.935307: train_loss -0.7553\n",
      "2024-12-23 12:23:19.935531: val_loss -0.7282\n",
      "2024-12-23 12:23:19.935592: Pseudo dice [np.float32(0.8088)]\n",
      "2024-12-23 12:23:19.935637: Epoch time: 111.19 s\n",
      "2024-12-23 12:23:20.536058: \n",
      "2024-12-23 12:23:20.536213: Epoch 404\n",
      "2024-12-23 12:23:20.536785: Current learning rate: 0.00628\n",
      "2024-12-23 12:25:11.746762: train_loss -0.7841\n",
      "2024-12-23 12:25:11.746961: val_loss -0.7282\n",
      "2024-12-23 12:25:11.746993: Pseudo dice [np.float32(0.7896)]\n",
      "2024-12-23 12:25:11.747029: Epoch time: 111.21 s\n",
      "2024-12-23 12:25:12.344774: \n",
      "2024-12-23 12:25:12.345056: Epoch 405\n",
      "2024-12-23 12:25:12.345185: Current learning rate: 0.00627\n",
      "2024-12-23 12:27:03.650478: train_loss -0.7434\n",
      "2024-12-23 12:27:03.650624: val_loss -0.7126\n",
      "2024-12-23 12:27:03.650659: Pseudo dice [np.float32(0.7745)]\n",
      "2024-12-23 12:27:03.650694: Epoch time: 111.31 s\n",
      "2024-12-23 12:27:04.240568: \n",
      "2024-12-23 12:27:04.240676: Epoch 406\n",
      "2024-12-23 12:27:04.240750: Current learning rate: 0.00626\n",
      "2024-12-23 12:28:55.426639: train_loss -0.7239\n",
      "2024-12-23 12:28:55.426776: val_loss -0.7336\n",
      "2024-12-23 12:28:55.426810: Pseudo dice [np.float32(0.7939)]\n",
      "2024-12-23 12:28:55.426920: Epoch time: 111.19 s\n",
      "2024-12-23 12:28:56.463925: \n",
      "2024-12-23 12:28:56.464047: Epoch 407\n",
      "2024-12-23 12:28:56.464132: Current learning rate: 0.00625\n",
      "2024-12-23 12:30:47.632607: train_loss -0.7189\n",
      "2024-12-23 12:30:47.632867: val_loss -0.7116\n",
      "2024-12-23 12:30:47.633041: Pseudo dice [np.float32(0.7386)]\n",
      "2024-12-23 12:30:47.633119: Epoch time: 111.17 s\n",
      "2024-12-23 12:30:48.238621: \n",
      "2024-12-23 12:30:48.238755: Epoch 408\n",
      "2024-12-23 12:30:48.238833: Current learning rate: 0.00624\n",
      "2024-12-23 12:32:39.456517: train_loss -0.7764\n",
      "2024-12-23 12:32:39.456663: val_loss -0.6638\n",
      "2024-12-23 12:32:39.456698: Pseudo dice [np.float32(0.7322)]\n",
      "2024-12-23 12:32:39.456734: Epoch time: 111.22 s\n",
      "2024-12-23 12:32:40.063644: \n",
      "2024-12-23 12:32:40.064035: Epoch 409\n",
      "2024-12-23 12:32:40.064170: Current learning rate: 0.00623\n",
      "2024-12-23 12:34:31.267073: train_loss -0.7783\n",
      "2024-12-23 12:34:31.267206: val_loss -0.6448\n",
      "2024-12-23 12:34:31.267237: Pseudo dice [np.float32(0.7133)]\n",
      "2024-12-23 12:34:31.267272: Epoch time: 111.2 s\n",
      "2024-12-23 12:34:31.868237: \n",
      "2024-12-23 12:34:31.868472: Epoch 410\n",
      "2024-12-23 12:34:31.868664: Current learning rate: 0.00622\n",
      "2024-12-23 12:36:23.120030: train_loss -0.7585\n",
      "2024-12-23 12:36:23.120402: val_loss -0.6423\n",
      "2024-12-23 12:36:23.120467: Pseudo dice [np.float32(0.7468)]\n",
      "2024-12-23 12:36:23.120511: Epoch time: 111.25 s\n",
      "2024-12-23 12:36:23.687499: \n",
      "2024-12-23 12:36:23.687634: Epoch 411\n",
      "2024-12-23 12:36:23.687710: Current learning rate: 0.00621\n",
      "2024-12-23 12:38:14.886598: train_loss -0.7736\n",
      "2024-12-23 12:38:14.886924: val_loss -0.6848\n",
      "2024-12-23 12:38:14.886969: Pseudo dice [np.float32(0.7112)]\n",
      "2024-12-23 12:38:14.887010: Epoch time: 111.2 s\n",
      "2024-12-23 12:38:15.461749: \n",
      "2024-12-23 12:38:15.461881: Epoch 412\n",
      "2024-12-23 12:38:15.461951: Current learning rate: 0.0062\n",
      "2024-12-23 12:40:06.673567: train_loss -0.7999\n",
      "2024-12-23 12:40:06.673779: val_loss -0.7041\n",
      "2024-12-23 12:40:06.673814: Pseudo dice [np.float32(0.8088)]\n",
      "2024-12-23 12:40:06.673851: Epoch time: 111.21 s\n",
      "2024-12-23 12:40:07.246193: \n",
      "2024-12-23 12:40:07.246326: Epoch 413\n",
      "2024-12-23 12:40:07.246405: Current learning rate: 0.00619\n",
      "2024-12-23 12:41:58.523300: train_loss -0.7219\n",
      "2024-12-23 12:41:58.523461: val_loss -0.6186\n",
      "2024-12-23 12:41:58.523501: Pseudo dice [np.float32(0.7359)]\n",
      "2024-12-23 12:41:58.523539: Epoch time: 111.28 s\n",
      "2024-12-23 12:41:59.093757: \n",
      "2024-12-23 12:41:59.093875: Epoch 414\n",
      "2024-12-23 12:41:59.093946: Current learning rate: 0.00618\n",
      "2024-12-23 12:43:50.338176: train_loss -0.7667\n",
      "2024-12-23 12:43:50.338316: val_loss -0.6689\n",
      "2024-12-23 12:43:50.338351: Pseudo dice [np.float32(0.718)]\n",
      "2024-12-23 12:43:50.338388: Epoch time: 111.24 s\n",
      "2024-12-23 12:43:50.910622: \n",
      "2024-12-23 12:43:50.910805: Epoch 415\n",
      "2024-12-23 12:43:50.910881: Current learning rate: 0.00617\n",
      "2024-12-23 12:45:42.123222: train_loss -0.7653\n",
      "2024-12-23 12:45:42.123613: val_loss -0.6924\n",
      "2024-12-23 12:45:42.123654: Pseudo dice [np.float32(0.7541)]\n",
      "2024-12-23 12:45:42.123691: Epoch time: 111.21 s\n",
      "2024-12-23 12:45:42.701491: \n",
      "2024-12-23 12:45:42.701744: Epoch 416\n",
      "2024-12-23 12:45:42.702150: Current learning rate: 0.00616\n",
      "2024-12-23 12:47:33.913564: train_loss -0.7653\n",
      "2024-12-23 12:47:33.913718: val_loss -0.6227\n",
      "2024-12-23 12:47:33.913752: Pseudo dice [np.float32(0.7009)]\n",
      "2024-12-23 12:47:33.913789: Epoch time: 111.21 s\n",
      "2024-12-23 12:47:34.486716: \n",
      "2024-12-23 12:47:34.486833: Epoch 417\n",
      "2024-12-23 12:47:34.486905: Current learning rate: 0.00615\n",
      "2024-12-23 12:49:25.709215: train_loss -0.7962\n",
      "2024-12-23 12:49:25.709368: val_loss -0.698\n",
      "2024-12-23 12:49:25.709403: Pseudo dice [np.float32(0.745)]\n",
      "2024-12-23 12:49:25.709441: Epoch time: 111.22 s\n",
      "2024-12-23 12:49:26.290541: \n",
      "2024-12-23 12:49:26.290717: Epoch 418\n",
      "2024-12-23 12:49:26.290795: Current learning rate: 0.00614\n",
      "2024-12-23 12:51:17.461139: train_loss -0.7868\n",
      "2024-12-23 12:51:17.461296: val_loss -0.7532\n",
      "2024-12-23 12:51:17.461335: Pseudo dice [np.float32(0.7904)]\n",
      "2024-12-23 12:51:17.461372: Epoch time: 111.17 s\n",
      "2024-12-23 12:51:18.044404: \n",
      "2024-12-23 12:51:18.044819: Epoch 419\n",
      "2024-12-23 12:51:18.044991: Current learning rate: 0.00613\n",
      "2024-12-23 12:53:09.288626: train_loss -0.727\n",
      "2024-12-23 12:53:09.288764: val_loss -0.6996\n",
      "2024-12-23 12:53:09.288797: Pseudo dice [np.float32(0.7402)]\n",
      "2024-12-23 12:53:09.288835: Epoch time: 111.24 s\n",
      "2024-12-23 12:53:09.866817: \n",
      "2024-12-23 12:53:09.867048: Epoch 420\n",
      "2024-12-23 12:53:09.867128: Current learning rate: 0.00612\n",
      "2024-12-23 12:55:01.097677: train_loss -0.7967\n",
      "2024-12-23 12:55:01.097814: val_loss -0.7511\n",
      "2024-12-23 12:55:01.097849: Pseudo dice [np.float32(0.794)]\n",
      "2024-12-23 12:55:01.097940: Epoch time: 111.23 s\n",
      "2024-12-23 12:55:01.678333: \n",
      "2024-12-23 12:55:01.678669: Epoch 421\n",
      "2024-12-23 12:55:01.678756: Current learning rate: 0.00612\n",
      "2024-12-23 12:56:52.930261: train_loss -0.7911\n",
      "2024-12-23 12:56:52.930406: val_loss -0.6553\n",
      "2024-12-23 12:56:52.930440: Pseudo dice [np.float32(0.744)]\n",
      "2024-12-23 12:56:52.930478: Epoch time: 111.25 s\n",
      "2024-12-23 12:56:53.503489: \n",
      "2024-12-23 12:56:53.503886: Epoch 422\n",
      "2024-12-23 12:56:53.504000: Current learning rate: 0.00611\n",
      "2024-12-23 12:58:44.718380: train_loss -0.7619\n",
      "2024-12-23 12:58:44.718534: val_loss -0.7186\n",
      "2024-12-23 12:58:44.718575: Pseudo dice [np.float32(0.7823)]\n",
      "2024-12-23 12:58:44.718611: Epoch time: 111.22 s\n",
      "2024-12-23 12:58:45.297430: \n",
      "2024-12-23 12:58:45.297527: Epoch 423\n",
      "2024-12-23 12:58:45.297600: Current learning rate: 0.0061\n",
      "2024-12-23 13:00:36.527578: train_loss -0.7662\n",
      "2024-12-23 13:00:36.527787: val_loss -0.7231\n",
      "2024-12-23 13:00:36.527821: Pseudo dice [np.float32(0.786)]\n",
      "2024-12-23 13:00:36.527856: Epoch time: 111.23 s\n",
      "2024-12-23 13:00:37.101552: \n",
      "2024-12-23 13:00:37.101852: Epoch 424\n",
      "2024-12-23 13:00:37.101992: Current learning rate: 0.00609\n",
      "2024-12-23 13:02:28.390899: train_loss -0.7535\n",
      "2024-12-23 13:02:28.391047: val_loss -0.7\n",
      "2024-12-23 13:02:28.391084: Pseudo dice [np.float32(0.7623)]\n",
      "2024-12-23 13:02:28.391122: Epoch time: 111.29 s\n",
      "2024-12-23 13:02:29.422522: \n",
      "2024-12-23 13:02:29.422725: Epoch 425\n",
      "2024-12-23 13:02:29.422823: Current learning rate: 0.00608\n",
      "2024-12-23 13:04:20.691653: train_loss -0.7724\n",
      "2024-12-23 13:04:20.691813: val_loss -0.7197\n",
      "2024-12-23 13:04:20.691848: Pseudo dice [np.float32(0.7933)]\n",
      "2024-12-23 13:04:20.691885: Epoch time: 111.27 s\n",
      "2024-12-23 13:04:21.266768: \n",
      "2024-12-23 13:04:21.266900: Epoch 426\n",
      "2024-12-23 13:04:21.266974: Current learning rate: 0.00607\n",
      "2024-12-23 13:06:12.526376: train_loss -0.7754\n",
      "2024-12-23 13:06:12.526755: val_loss -0.7038\n",
      "2024-12-23 13:06:12.526987: Pseudo dice [np.float32(0.7641)]\n",
      "2024-12-23 13:06:12.527082: Epoch time: 111.26 s\n",
      "2024-12-23 13:06:13.104693: \n",
      "2024-12-23 13:06:13.105137: Epoch 427\n",
      "2024-12-23 13:06:13.105239: Current learning rate: 0.00606\n",
      "2024-12-23 13:08:04.383250: train_loss -0.8056\n",
      "2024-12-23 13:08:04.383402: val_loss -0.688\n",
      "2024-12-23 13:08:04.383437: Pseudo dice [np.float32(0.7479)]\n",
      "2024-12-23 13:08:04.383474: Epoch time: 111.28 s\n",
      "2024-12-23 13:08:04.957573: \n",
      "2024-12-23 13:08:04.957688: Epoch 428\n",
      "2024-12-23 13:08:04.957758: Current learning rate: 0.00605\n",
      "2024-12-23 13:09:56.210539: train_loss -0.8038\n",
      "2024-12-23 13:09:56.210948: val_loss -0.7357\n",
      "2024-12-23 13:09:56.210989: Pseudo dice [np.float32(0.7897)]\n",
      "2024-12-23 13:09:56.211027: Epoch time: 111.25 s\n",
      "2024-12-23 13:09:56.801863: \n",
      "2024-12-23 13:09:56.801997: Epoch 429\n",
      "2024-12-23 13:09:56.802071: Current learning rate: 0.00604\n",
      "2024-12-23 13:11:48.053114: train_loss -0.779\n",
      "2024-12-23 13:11:48.053267: val_loss -0.6742\n",
      "2024-12-23 13:11:48.053301: Pseudo dice [np.float32(0.7573)]\n",
      "2024-12-23 13:11:48.053336: Epoch time: 111.25 s\n",
      "2024-12-23 13:11:48.637083: \n",
      "2024-12-23 13:11:48.637206: Epoch 430\n",
      "2024-12-23 13:11:48.637281: Current learning rate: 0.00603\n",
      "2024-12-23 13:13:39.905685: train_loss -0.707\n",
      "2024-12-23 13:13:39.905830: val_loss -0.7561\n",
      "2024-12-23 13:13:39.905866: Pseudo dice [np.float32(0.7832)]\n",
      "2024-12-23 13:13:39.905902: Epoch time: 111.27 s\n",
      "2024-12-23 13:13:40.485283: \n",
      "2024-12-23 13:13:40.485579: Epoch 431\n",
      "2024-12-23 13:13:40.485662: Current learning rate: 0.00602\n",
      "2024-12-23 13:15:31.751341: train_loss -0.7663\n",
      "2024-12-23 13:15:31.751489: val_loss -0.7238\n",
      "2024-12-23 13:15:31.751523: Pseudo dice [np.float32(0.8039)]\n",
      "2024-12-23 13:15:31.751631: Epoch time: 111.27 s\n",
      "2024-12-23 13:15:32.336047: \n",
      "2024-12-23 13:15:32.336293: Epoch 432\n",
      "2024-12-23 13:15:32.336377: Current learning rate: 0.00601\n",
      "2024-12-23 13:17:23.580705: train_loss -0.7783\n",
      "2024-12-23 13:17:23.580863: val_loss -0.7088\n",
      "2024-12-23 13:17:23.580896: Pseudo dice [np.float32(0.7588)]\n",
      "2024-12-23 13:17:23.580933: Epoch time: 111.25 s\n",
      "2024-12-23 13:17:24.157224: \n",
      "2024-12-23 13:17:24.157328: Epoch 433\n",
      "2024-12-23 13:17:24.157398: Current learning rate: 0.006\n",
      "2024-12-23 13:19:15.391193: train_loss -0.7563\n",
      "2024-12-23 13:19:15.391391: val_loss -0.7634\n",
      "2024-12-23 13:19:15.391424: Pseudo dice [np.float32(0.8171)]\n",
      "2024-12-23 13:19:15.391459: Epoch time: 111.23 s\n",
      "2024-12-23 13:19:15.972111: \n",
      "2024-12-23 13:19:15.972215: Epoch 434\n",
      "2024-12-23 13:19:15.972370: Current learning rate: 0.00599\n",
      "2024-12-23 13:21:07.238349: train_loss -0.7709\n",
      "2024-12-23 13:21:07.238517: val_loss -0.7687\n",
      "2024-12-23 13:21:07.238552: Pseudo dice [np.float32(0.8334)]\n",
      "2024-12-23 13:21:07.238588: Epoch time: 111.27 s\n",
      "2024-12-23 13:21:07.812646: \n",
      "2024-12-23 13:21:07.813013: Epoch 435\n",
      "2024-12-23 13:21:07.813100: Current learning rate: 0.00598\n",
      "2024-12-23 13:22:59.041717: train_loss -0.7658\n",
      "2024-12-23 13:22:59.041885: val_loss -0.7365\n",
      "2024-12-23 13:22:59.041920: Pseudo dice [np.float32(0.776)]\n",
      "2024-12-23 13:22:59.041958: Epoch time: 111.23 s\n",
      "2024-12-23 13:22:59.617745: \n",
      "2024-12-23 13:22:59.617855: Epoch 436\n",
      "2024-12-23 13:22:59.617928: Current learning rate: 0.00597\n",
      "2024-12-23 13:24:50.874241: train_loss -0.7697\n",
      "2024-12-23 13:24:50.874381: val_loss -0.7361\n",
      "2024-12-23 13:24:50.874415: Pseudo dice [np.float32(0.7846)]\n",
      "2024-12-23 13:24:50.874451: Epoch time: 111.26 s\n",
      "2024-12-23 13:24:51.446645: \n",
      "2024-12-23 13:24:51.447026: Epoch 437\n",
      "2024-12-23 13:24:51.447237: Current learning rate: 0.00596\n",
      "2024-12-23 13:26:42.714394: train_loss -0.7929\n",
      "2024-12-23 13:26:42.714733: val_loss -0.7177\n",
      "2024-12-23 13:26:42.714800: Pseudo dice [np.float32(0.7906)]\n",
      "2024-12-23 13:26:42.714845: Epoch time: 111.27 s\n",
      "2024-12-23 13:26:42.714874: Yayy! New best EMA pseudo Dice: 0.7807999849319458\n",
      "2024-12-23 13:26:43.550661: \n",
      "2024-12-23 13:26:43.551008: Epoch 438\n",
      "2024-12-23 13:26:43.551133: Current learning rate: 0.00595\n",
      "2024-12-23 13:28:34.871021: train_loss -0.782\n",
      "2024-12-23 13:28:34.871314: val_loss -0.7019\n",
      "2024-12-23 13:28:34.871360: Pseudo dice [np.float32(0.7725)]\n",
      "2024-12-23 13:28:34.871398: Epoch time: 111.32 s\n",
      "2024-12-23 13:28:35.441308: \n",
      "2024-12-23 13:28:35.441575: Epoch 439\n",
      "2024-12-23 13:28:35.441700: Current learning rate: 0.00594\n",
      "2024-12-23 13:30:26.686309: train_loss -0.7832\n",
      "2024-12-23 13:30:26.686518: val_loss -0.6578\n",
      "2024-12-23 13:30:26.686555: Pseudo dice [np.float32(0.7224)]\n",
      "2024-12-23 13:30:26.686594: Epoch time: 111.25 s\n",
      "2024-12-23 13:30:27.261926: \n",
      "2024-12-23 13:30:27.262035: Epoch 440\n",
      "2024-12-23 13:30:27.262111: Current learning rate: 0.00593\n",
      "2024-12-23 13:32:18.495822: train_loss -0.8048\n",
      "2024-12-23 13:32:18.495962: val_loss -0.7759\n",
      "2024-12-23 13:32:18.496026: Pseudo dice [np.float32(0.7773)]\n",
      "2024-12-23 13:32:18.496065: Epoch time: 111.23 s\n",
      "2024-12-23 13:32:19.075093: \n",
      "2024-12-23 13:32:19.075198: Epoch 441\n",
      "2024-12-23 13:32:19.075273: Current learning rate: 0.00592\n",
      "2024-12-23 13:34:10.296115: train_loss -0.7975\n",
      "2024-12-23 13:34:10.296261: val_loss -0.7171\n",
      "2024-12-23 13:34:10.296297: Pseudo dice [np.float32(0.7805)]\n",
      "2024-12-23 13:34:10.296333: Epoch time: 111.22 s\n",
      "2024-12-23 13:34:10.874455: \n",
      "2024-12-23 13:34:10.874619: Epoch 442\n",
      "2024-12-23 13:34:10.874694: Current learning rate: 0.00592\n",
      "2024-12-23 13:36:02.128443: train_loss -0.7796\n",
      "2024-12-23 13:36:02.128598: val_loss -0.7131\n",
      "2024-12-23 13:36:02.128632: Pseudo dice [np.float32(0.77)]\n",
      "2024-12-23 13:36:02.128669: Epoch time: 111.25 s\n",
      "2024-12-23 13:36:02.724753: \n",
      "2024-12-23 13:36:02.724854: Epoch 443\n",
      "2024-12-23 13:36:02.724925: Current learning rate: 0.00591\n",
      "2024-12-23 13:37:53.950622: train_loss -0.7908\n",
      "2024-12-23 13:37:53.950769: val_loss -0.6885\n",
      "2024-12-23 13:37:53.950831: Pseudo dice [np.float32(0.7719)]\n",
      "2024-12-23 13:37:53.950978: Epoch time: 111.23 s\n",
      "2024-12-23 13:37:54.953044: \n",
      "2024-12-23 13:37:54.953305: Epoch 444\n",
      "2024-12-23 13:37:54.953578: Current learning rate: 0.0059\n",
      "2024-12-23 13:39:46.173985: train_loss -0.738\n",
      "2024-12-23 13:39:46.174126: val_loss -0.7082\n",
      "2024-12-23 13:39:46.174228: Pseudo dice [np.float32(0.7658)]\n",
      "2024-12-23 13:39:46.174353: Epoch time: 111.22 s\n",
      "2024-12-23 13:39:46.742238: \n",
      "2024-12-23 13:39:46.742610: Epoch 445\n",
      "2024-12-23 13:39:46.742821: Current learning rate: 0.00589\n",
      "2024-12-23 13:41:37.993978: train_loss -0.7304\n",
      "2024-12-23 13:41:37.994444: val_loss -0.6071\n",
      "2024-12-23 13:41:37.994527: Pseudo dice [np.float32(0.708)]\n",
      "2024-12-23 13:41:37.994574: Epoch time: 111.25 s\n",
      "2024-12-23 13:41:38.641026: \n",
      "2024-12-23 13:41:38.641476: Epoch 446\n",
      "2024-12-23 13:41:38.641640: Current learning rate: 0.00588\n",
      "2024-12-23 13:43:29.916206: train_loss -0.7706\n",
      "2024-12-23 13:43:29.916391: val_loss -0.682\n",
      "2024-12-23 13:43:29.916424: Pseudo dice [np.float32(0.7516)]\n",
      "2024-12-23 13:43:29.916460: Epoch time: 111.28 s\n",
      "2024-12-23 13:43:30.491590: \n",
      "2024-12-23 13:43:30.491717: Epoch 447\n",
      "2024-12-23 13:43:30.491791: Current learning rate: 0.00587\n",
      "2024-12-23 13:45:21.750221: train_loss -0.7475\n",
      "2024-12-23 13:45:21.750407: val_loss -0.7244\n",
      "2024-12-23 13:45:21.750437: Pseudo dice [np.float32(0.782)]\n",
      "2024-12-23 13:45:21.750472: Epoch time: 111.26 s\n",
      "2024-12-23 13:45:22.315126: \n",
      "2024-12-23 13:45:22.315518: Epoch 448\n",
      "2024-12-23 13:45:22.315600: Current learning rate: 0.00586\n",
      "2024-12-23 13:47:13.595094: train_loss -0.7311\n",
      "2024-12-23 13:47:13.595308: val_loss -0.7255\n",
      "2024-12-23 13:47:13.595351: Pseudo dice [np.float32(0.785)]\n",
      "2024-12-23 13:47:13.595415: Epoch time: 111.28 s\n",
      "2024-12-23 13:47:14.163793: \n",
      "2024-12-23 13:47:14.164162: Epoch 449\n",
      "2024-12-23 13:47:14.164297: Current learning rate: 0.00585\n",
      "2024-12-23 13:49:05.421303: train_loss -0.7899\n",
      "2024-12-23 13:49:05.421473: val_loss -0.6647\n",
      "2024-12-23 13:49:05.421511: Pseudo dice [np.float32(0.7665)]\n",
      "2024-12-23 13:49:05.421546: Epoch time: 111.26 s\n",
      "2024-12-23 13:49:06.253336: \n",
      "2024-12-23 13:49:06.253450: Epoch 450\n",
      "2024-12-23 13:49:06.253520: Current learning rate: 0.00584\n",
      "2024-12-23 13:50:57.550947: train_loss -0.7735\n",
      "2024-12-23 13:50:57.551095: val_loss -0.7117\n",
      "2024-12-23 13:50:57.551129: Pseudo dice [np.float32(0.7586)]\n",
      "2024-12-23 13:50:57.551168: Epoch time: 111.3 s\n",
      "2024-12-23 13:50:58.130030: \n",
      "2024-12-23 13:50:58.130397: Epoch 451\n",
      "2024-12-23 13:50:58.130482: Current learning rate: 0.00583\n",
      "2024-12-23 13:52:49.378802: train_loss -0.7438\n",
      "2024-12-23 13:52:49.379173: val_loss -0.7217\n",
      "2024-12-23 13:52:49.379237: Pseudo dice [np.float32(0.7762)]\n",
      "2024-12-23 13:52:49.379283: Epoch time: 111.25 s\n",
      "2024-12-23 13:52:49.951092: \n",
      "2024-12-23 13:52:49.951425: Epoch 452\n",
      "2024-12-23 13:52:49.951510: Current learning rate: 0.00582\n",
      "2024-12-23 13:54:41.200712: train_loss -0.7589\n",
      "2024-12-23 13:54:41.200847: val_loss -0.7158\n",
      "2024-12-23 13:54:41.200882: Pseudo dice [np.float32(0.7775)]\n",
      "2024-12-23 13:54:41.200919: Epoch time: 111.25 s\n",
      "2024-12-23 13:54:41.767848: \n",
      "2024-12-23 13:54:41.768145: Epoch 453\n",
      "2024-12-23 13:54:41.768232: Current learning rate: 0.00581\n",
      "2024-12-23 13:56:32.993155: train_loss -0.7567\n",
      "2024-12-23 13:56:32.993291: val_loss -0.7223\n",
      "2024-12-23 13:56:32.993324: Pseudo dice [np.float32(0.7739)]\n",
      "2024-12-23 13:56:32.993360: Epoch time: 111.23 s\n",
      "2024-12-23 13:56:33.564101: \n",
      "2024-12-23 13:56:33.564481: Epoch 454\n",
      "2024-12-23 13:56:33.564590: Current learning rate: 0.0058\n",
      "2024-12-23 13:58:24.834906: train_loss -0.7734\n",
      "2024-12-23 13:58:24.835042: val_loss -0.6348\n",
      "2024-12-23 13:58:24.835075: Pseudo dice [np.float32(0.6674)]\n",
      "2024-12-23 13:58:24.835112: Epoch time: 111.27 s\n",
      "2024-12-23 13:58:25.406624: \n",
      "2024-12-23 13:58:25.406732: Epoch 455\n",
      "2024-12-23 13:58:25.406816: Current learning rate: 0.00579\n",
      "2024-12-23 14:00:16.635672: train_loss -0.7843\n",
      "2024-12-23 14:00:16.635828: val_loss -0.7356\n",
      "2024-12-23 14:00:16.635865: Pseudo dice [np.float32(0.7791)]\n",
      "2024-12-23 14:00:16.635900: Epoch time: 111.23 s\n",
      "2024-12-23 14:00:17.210244: \n",
      "2024-12-23 14:00:17.210617: Epoch 456\n",
      "2024-12-23 14:00:17.210706: Current learning rate: 0.00578\n",
      "2024-12-23 14:02:08.433379: train_loss -0.7731\n",
      "2024-12-23 14:02:08.433638: val_loss -0.7707\n",
      "2024-12-23 14:02:08.433744: Pseudo dice [np.float32(0.8313)]\n",
      "2024-12-23 14:02:08.433817: Epoch time: 111.22 s\n",
      "2024-12-23 14:02:09.003547: \n",
      "2024-12-23 14:02:09.003736: Epoch 457\n",
      "2024-12-23 14:02:09.003821: Current learning rate: 0.00577\n",
      "2024-12-23 14:04:00.298913: train_loss -0.7732\n",
      "2024-12-23 14:04:00.299065: val_loss -0.6534\n",
      "2024-12-23 14:04:00.299103: Pseudo dice [np.float32(0.6723)]\n",
      "2024-12-23 14:04:00.299142: Epoch time: 111.3 s\n",
      "2024-12-23 14:04:00.877187: \n",
      "2024-12-23 14:04:00.877424: Epoch 458\n",
      "2024-12-23 14:04:00.877505: Current learning rate: 0.00576\n",
      "2024-12-23 14:05:52.099899: train_loss -0.7453\n",
      "2024-12-23 14:05:52.100130: val_loss -0.6823\n",
      "2024-12-23 14:05:52.100179: Pseudo dice [np.float32(0.7497)]\n",
      "2024-12-23 14:05:52.100219: Epoch time: 111.22 s\n",
      "2024-12-23 14:05:52.673847: \n",
      "2024-12-23 14:05:52.674176: Epoch 459\n",
      "2024-12-23 14:05:52.674255: Current learning rate: 0.00575\n",
      "2024-12-23 14:07:43.899035: train_loss -0.7313\n",
      "2024-12-23 14:07:43.899177: val_loss -0.7347\n",
      "2024-12-23 14:07:43.899210: Pseudo dice [np.float32(0.7984)]\n",
      "2024-12-23 14:07:43.899246: Epoch time: 111.23 s\n",
      "2024-12-23 14:07:44.469053: \n",
      "2024-12-23 14:07:44.469162: Epoch 460\n",
      "2024-12-23 14:07:44.469238: Current learning rate: 0.00574\n",
      "2024-12-23 14:09:35.722016: train_loss -0.7708\n",
      "2024-12-23 14:09:35.722157: val_loss -0.6918\n",
      "2024-12-23 14:09:35.722191: Pseudo dice [np.float32(0.7203)]\n",
      "2024-12-23 14:09:35.722228: Epoch time: 111.25 s\n",
      "2024-12-23 14:09:36.288376: \n",
      "2024-12-23 14:09:36.288536: Epoch 461\n",
      "2024-12-23 14:09:36.288615: Current learning rate: 0.00573\n",
      "2024-12-23 14:11:27.522906: train_loss -0.7795\n",
      "2024-12-23 14:11:27.523366: val_loss -0.7301\n",
      "2024-12-23 14:11:27.523428: Pseudo dice [np.float32(0.7922)]\n",
      "2024-12-23 14:11:27.523479: Epoch time: 111.24 s\n",
      "2024-12-23 14:11:28.095436: \n",
      "2024-12-23 14:11:28.095537: Epoch 462\n",
      "2024-12-23 14:11:28.095610: Current learning rate: 0.00572\n",
      "2024-12-23 14:13:19.363460: train_loss -0.7795\n",
      "2024-12-23 14:13:19.363604: val_loss -0.7606\n",
      "2024-12-23 14:13:19.363639: Pseudo dice [np.float32(0.8071)]\n",
      "2024-12-23 14:13:19.363678: Epoch time: 111.27 s\n",
      "2024-12-23 14:13:20.378564: \n",
      "2024-12-23 14:13:20.378773: Epoch 463\n",
      "2024-12-23 14:13:20.378869: Current learning rate: 0.00571\n",
      "2024-12-23 14:15:11.638781: train_loss -0.784\n",
      "2024-12-23 14:15:11.639123: val_loss -0.76\n",
      "2024-12-23 14:15:11.639253: Pseudo dice [np.float32(0.8272)]\n",
      "2024-12-23 14:15:11.639349: Epoch time: 111.26 s\n",
      "2024-12-23 14:15:12.209069: \n",
      "2024-12-23 14:15:12.209260: Epoch 464\n",
      "2024-12-23 14:15:12.209336: Current learning rate: 0.0057\n",
      "2024-12-23 14:17:03.457912: train_loss -0.7726\n",
      "2024-12-23 14:17:03.458169: val_loss -0.7457\n",
      "2024-12-23 14:17:03.458343: Pseudo dice [np.float32(0.8353)]\n",
      "2024-12-23 14:17:03.458387: Epoch time: 111.25 s\n",
      "2024-12-23 14:17:04.030467: \n",
      "2024-12-23 14:17:04.030757: Epoch 465\n",
      "2024-12-23 14:17:04.030842: Current learning rate: 0.0057\n",
      "2024-12-23 14:18:55.301841: train_loss -0.7615\n",
      "2024-12-23 14:18:55.301987: val_loss -0.693\n",
      "2024-12-23 14:18:55.302023: Pseudo dice [np.float32(0.7598)]\n",
      "2024-12-23 14:18:55.302073: Epoch time: 111.27 s\n",
      "2024-12-23 14:18:55.873539: \n",
      "2024-12-23 14:18:55.873911: Epoch 466\n",
      "2024-12-23 14:18:55.873995: Current learning rate: 0.00569\n",
      "2024-12-23 14:20:47.136924: train_loss -0.7477\n",
      "2024-12-23 14:20:47.137070: val_loss -0.728\n",
      "2024-12-23 14:20:47.137103: Pseudo dice [np.float32(0.8034)]\n",
      "2024-12-23 14:20:47.137141: Epoch time: 111.26 s\n",
      "2024-12-23 14:20:47.710704: \n",
      "2024-12-23 14:20:47.710940: Epoch 467\n",
      "2024-12-23 14:20:47.711071: Current learning rate: 0.00568\n",
      "2024-12-23 14:22:38.946754: train_loss -0.7769\n",
      "2024-12-23 14:22:38.946902: val_loss -0.768\n",
      "2024-12-23 14:22:38.946938: Pseudo dice [np.float32(0.8163)]\n",
      "2024-12-23 14:22:38.946976: Epoch time: 111.24 s\n",
      "2024-12-23 14:22:38.946998: Yayy! New best EMA pseudo Dice: 0.7828999757766724\n",
      "2024-12-23 14:22:39.779003: \n",
      "2024-12-23 14:22:39.779130: Epoch 468\n",
      "2024-12-23 14:22:39.779202: Current learning rate: 0.00567\n",
      "2024-12-23 14:24:31.024420: train_loss -0.7867\n",
      "2024-12-23 14:24:31.024607: val_loss -0.6525\n",
      "2024-12-23 14:24:31.024642: Pseudo dice [np.float32(0.7322)]\n",
      "2024-12-23 14:24:31.024682: Epoch time: 111.25 s\n",
      "2024-12-23 14:24:31.594731: \n",
      "2024-12-23 14:24:31.594922: Epoch 469\n",
      "2024-12-23 14:24:31.595008: Current learning rate: 0.00566\n",
      "2024-12-23 14:26:22.925814: train_loss -0.8063\n",
      "2024-12-23 14:26:22.926135: val_loss -0.706\n",
      "2024-12-23 14:26:22.926175: Pseudo dice [np.float32(0.7633)]\n",
      "2024-12-23 14:26:22.926212: Epoch time: 111.33 s\n",
      "2024-12-23 14:26:23.495303: \n",
      "2024-12-23 14:26:23.495483: Epoch 470\n",
      "2024-12-23 14:26:23.495587: Current learning rate: 0.00565\n",
      "2024-12-23 14:28:14.919345: train_loss -0.7981\n",
      "2024-12-23 14:28:14.919484: val_loss -0.7877\n",
      "2024-12-23 14:28:14.919519: Pseudo dice [np.float32(0.8458)]\n",
      "2024-12-23 14:28:14.919554: Epoch time: 111.42 s\n",
      "2024-12-23 14:28:14.919576: Yayy! New best EMA pseudo Dice: 0.78329998254776\n",
      "2024-12-23 14:28:15.767626: \n",
      "2024-12-23 14:28:15.767737: Epoch 471\n",
      "2024-12-23 14:28:15.767888: Current learning rate: 0.00564\n",
      "2024-12-23 14:30:06.959559: train_loss -0.7977\n",
      "2024-12-23 14:30:06.959719: val_loss -0.7564\n",
      "2024-12-23 14:30:06.959751: Pseudo dice [np.float32(0.8174)]\n",
      "2024-12-23 14:30:06.959790: Epoch time: 111.19 s\n",
      "2024-12-23 14:30:06.959810: Yayy! New best EMA pseudo Dice: 0.7867000102996826\n",
      "2024-12-23 14:30:07.774367: \n",
      "2024-12-23 14:30:07.774578: Epoch 472\n",
      "2024-12-23 14:30:07.774657: Current learning rate: 0.00563\n",
      "2024-12-23 14:31:59.042132: train_loss -0.7913\n",
      "2024-12-23 14:31:59.042312: val_loss -0.7255\n",
      "2024-12-23 14:31:59.042424: Pseudo dice [np.float32(0.8185)]\n",
      "2024-12-23 14:31:59.042471: Epoch time: 111.27 s\n",
      "2024-12-23 14:31:59.042500: Yayy! New best EMA pseudo Dice: 0.789900004863739\n",
      "2024-12-23 14:31:59.849282: \n",
      "2024-12-23 14:31:59.849664: Epoch 473\n",
      "2024-12-23 14:31:59.849756: Current learning rate: 0.00562\n",
      "2024-12-23 14:33:51.114351: train_loss -0.7794\n",
      "2024-12-23 14:33:51.114497: val_loss -0.7945\n",
      "2024-12-23 14:33:51.114533: Pseudo dice [np.float32(0.8197)]\n",
      "2024-12-23 14:33:51.114570: Epoch time: 111.27 s\n",
      "2024-12-23 14:33:51.114654: Yayy! New best EMA pseudo Dice: 0.792900025844574\n",
      "2024-12-23 14:33:51.920434: \n",
      "2024-12-23 14:33:51.921081: Epoch 474\n",
      "2024-12-23 14:33:51.921163: Current learning rate: 0.00561\n",
      "2024-12-23 14:35:43.123423: train_loss -0.7813\n",
      "2024-12-23 14:35:43.123883: val_loss -0.6914\n",
      "2024-12-23 14:35:43.124025: Pseudo dice [np.float32(0.7431)]\n",
      "2024-12-23 14:35:43.124074: Epoch time: 111.2 s\n",
      "2024-12-23 14:35:43.692630: \n",
      "2024-12-23 14:35:43.692737: Epoch 475\n",
      "2024-12-23 14:35:43.692809: Current learning rate: 0.0056\n",
      "2024-12-23 14:37:34.900461: train_loss -0.8194\n",
      "2024-12-23 14:37:34.900607: val_loss -0.7136\n",
      "2024-12-23 14:37:34.900642: Pseudo dice [np.float32(0.7817)]\n",
      "2024-12-23 14:37:34.900679: Epoch time: 111.21 s\n",
      "2024-12-23 14:37:35.465794: \n",
      "2024-12-23 14:37:35.465900: Epoch 476\n",
      "2024-12-23 14:37:35.465972: Current learning rate: 0.00559\n",
      "2024-12-23 14:39:26.693441: train_loss -0.8014\n",
      "2024-12-23 14:39:26.693621: val_loss -0.748\n",
      "2024-12-23 14:39:26.693662: Pseudo dice [np.float32(0.7936)]\n",
      "2024-12-23 14:39:26.693700: Epoch time: 111.23 s\n",
      "2024-12-23 14:39:27.265799: \n",
      "2024-12-23 14:39:27.265925: Epoch 477\n",
      "2024-12-23 14:39:27.266002: Current learning rate: 0.00558\n",
      "2024-12-23 14:41:18.509245: train_loss -0.8112\n",
      "2024-12-23 14:41:18.509387: val_loss -0.6858\n",
      "2024-12-23 14:41:18.509419: Pseudo dice [np.float32(0.7547)]\n",
      "2024-12-23 14:41:18.509455: Epoch time: 111.24 s\n",
      "2024-12-23 14:41:19.081805: \n",
      "2024-12-23 14:41:19.081909: Epoch 478\n",
      "2024-12-23 14:41:19.081981: Current learning rate: 0.00557\n",
      "2024-12-23 14:43:10.399578: train_loss -0.797\n",
      "2024-12-23 14:43:10.399945: val_loss -0.7484\n",
      "2024-12-23 14:43:10.400006: Pseudo dice [np.float32(0.8025)]\n",
      "2024-12-23 14:43:10.400050: Epoch time: 111.32 s\n",
      "2024-12-23 14:43:10.974871: \n",
      "2024-12-23 14:43:10.975270: Epoch 479\n",
      "2024-12-23 14:43:10.975357: Current learning rate: 0.00556\n",
      "2024-12-23 14:45:02.223778: train_loss -0.804\n",
      "2024-12-23 14:45:02.223913: val_loss -0.6928\n",
      "2024-12-23 14:45:02.223946: Pseudo dice [np.float32(0.7387)]\n",
      "2024-12-23 14:45:02.224108: Epoch time: 111.25 s\n",
      "2024-12-23 14:45:02.795934: \n",
      "2024-12-23 14:45:02.796026: Epoch 480\n",
      "2024-12-23 14:45:02.796098: Current learning rate: 0.00555\n",
      "2024-12-23 14:46:54.060750: train_loss -0.7892\n",
      "2024-12-23 14:46:54.060895: val_loss -0.7737\n",
      "2024-12-23 14:46:54.060928: Pseudo dice [np.float32(0.8139)]\n",
      "2024-12-23 14:46:54.060965: Epoch time: 111.27 s\n",
      "2024-12-23 14:46:55.077435: \n",
      "2024-12-23 14:46:55.077617: Epoch 481\n",
      "2024-12-23 14:46:55.077724: Current learning rate: 0.00554\n",
      "2024-12-23 14:48:46.308130: train_loss -0.7749\n",
      "2024-12-23 14:48:46.308274: val_loss -0.7132\n",
      "2024-12-23 14:48:46.308308: Pseudo dice [np.float32(0.7586)]\n",
      "2024-12-23 14:48:46.308365: Epoch time: 111.23 s\n",
      "2024-12-23 14:48:46.872924: \n",
      "2024-12-23 14:48:46.873423: Epoch 482\n",
      "2024-12-23 14:48:46.873625: Current learning rate: 0.00553\n",
      "2024-12-23 14:50:38.075300: train_loss -0.8113\n",
      "2024-12-23 14:50:38.075513: val_loss -0.7162\n",
      "2024-12-23 14:50:38.075547: Pseudo dice [np.float32(0.7871)]\n",
      "2024-12-23 14:50:38.075582: Epoch time: 111.2 s\n",
      "2024-12-23 14:50:38.643092: \n",
      "2024-12-23 14:50:38.643437: Epoch 483\n",
      "2024-12-23 14:50:38.643535: Current learning rate: 0.00552\n",
      "2024-12-23 14:52:29.902366: train_loss -0.7741\n",
      "2024-12-23 14:52:29.902544: val_loss -0.6723\n",
      "2024-12-23 14:52:29.902615: Pseudo dice [np.float32(0.6288)]\n",
      "2024-12-23 14:52:29.902659: Epoch time: 111.26 s\n",
      "2024-12-23 14:52:30.489957: \n",
      "2024-12-23 14:52:30.490166: Epoch 484\n",
      "2024-12-23 14:52:30.490245: Current learning rate: 0.00551\n",
      "2024-12-23 14:54:21.757724: train_loss -0.7598\n",
      "2024-12-23 14:54:21.757886: val_loss -0.7616\n",
      "2024-12-23 14:54:21.757921: Pseudo dice [np.float32(0.8314)]\n",
      "2024-12-23 14:54:21.757961: Epoch time: 111.27 s\n",
      "2024-12-23 14:54:22.333437: \n",
      "2024-12-23 14:54:22.333784: Epoch 485\n",
      "2024-12-23 14:54:22.334021: Current learning rate: 0.0055\n",
      "2024-12-23 14:56:13.534812: train_loss -0.7841\n",
      "2024-12-23 14:56:13.534958: val_loss -0.6453\n",
      "2024-12-23 14:56:13.534991: Pseudo dice [np.float32(0.6259)]\n",
      "2024-12-23 14:56:13.535027: Epoch time: 111.2 s\n",
      "2024-12-23 14:56:14.121021: \n",
      "2024-12-23 14:56:14.121134: Epoch 486\n",
      "2024-12-23 14:56:14.121212: Current learning rate: 0.00549\n",
      "2024-12-23 14:58:05.367985: train_loss -0.7648\n",
      "2024-12-23 14:58:05.368135: val_loss -0.7188\n",
      "2024-12-23 14:58:05.368170: Pseudo dice [np.float32(0.7655)]\n",
      "2024-12-23 14:58:05.368205: Epoch time: 111.25 s\n",
      "2024-12-23 14:58:05.946824: \n",
      "2024-12-23 14:58:05.947202: Epoch 487\n",
      "2024-12-23 14:58:05.947291: Current learning rate: 0.00548\n",
      "2024-12-23 14:59:57.237761: train_loss -0.7579\n",
      "2024-12-23 14:59:57.238124: val_loss -0.7356\n",
      "2024-12-23 14:59:57.238198: Pseudo dice [np.float32(0.8031)]\n",
      "2024-12-23 14:59:57.238240: Epoch time: 111.29 s\n",
      "2024-12-23 14:59:57.822273: \n",
      "2024-12-23 14:59:57.822685: Epoch 488\n",
      "2024-12-23 14:59:57.822798: Current learning rate: 0.00547\n",
      "2024-12-23 15:01:51.979596: train_loss -0.799\n",
      "2024-12-23 15:01:51.979813: val_loss -0.7745\n",
      "2024-12-23 15:01:51.979851: Pseudo dice [np.float32(0.8183)]\n",
      "2024-12-23 15:01:51.979889: Epoch time: 114.16 s\n",
      "2024-12-23 15:01:52.561028: \n",
      "2024-12-23 15:01:52.561202: Epoch 489\n",
      "2024-12-23 15:01:52.561287: Current learning rate: 0.00546\n",
      "2024-12-23 15:03:44.444928: train_loss -0.7902\n",
      "2024-12-23 15:03:44.445066: val_loss -0.6874\n",
      "2024-12-23 15:03:44.445099: Pseudo dice [np.float32(0.7796)]\n",
      "2024-12-23 15:03:44.445134: Epoch time: 111.88 s\n",
      "2024-12-23 15:03:45.026549: \n",
      "2024-12-23 15:03:45.026886: Epoch 490\n",
      "2024-12-23 15:03:45.026968: Current learning rate: 0.00546\n",
      "2024-12-23 15:05:36.681800: train_loss -0.8037\n",
      "2024-12-23 15:05:36.681987: val_loss -0.733\n",
      "2024-12-23 15:05:36.682059: Pseudo dice [np.float32(0.8196)]\n",
      "2024-12-23 15:05:36.682182: Epoch time: 111.66 s\n",
      "2024-12-23 15:05:37.267220: \n",
      "2024-12-23 15:05:37.267746: Epoch 491\n",
      "2024-12-23 15:05:37.267841: Current learning rate: 0.00545\n",
      "2024-12-23 15:07:29.406268: train_loss -0.8208\n",
      "2024-12-23 15:07:29.406444: val_loss -0.7735\n",
      "2024-12-23 15:07:29.406479: Pseudo dice [np.float32(0.8126)]\n",
      "2024-12-23 15:07:29.406517: Epoch time: 112.14 s\n",
      "2024-12-23 15:07:29.987435: \n",
      "2024-12-23 15:07:29.987543: Epoch 492\n",
      "2024-12-23 15:07:29.987614: Current learning rate: 0.00544\n",
      "2024-12-23 15:09:22.562704: train_loss -0.8169\n",
      "2024-12-23 15:09:22.562852: val_loss -0.7189\n",
      "2024-12-23 15:09:22.562885: Pseudo dice [np.float32(0.7405)]\n",
      "2024-12-23 15:09:22.562924: Epoch time: 112.58 s\n",
      "2024-12-23 15:09:23.149532: \n",
      "2024-12-23 15:09:23.149638: Epoch 493\n",
      "2024-12-23 15:09:23.149712: Current learning rate: 0.00543\n",
      "2024-12-23 15:11:14.716840: train_loss -0.7674\n",
      "2024-12-23 15:11:14.717004: val_loss -0.7278\n",
      "2024-12-23 15:11:14.717047: Pseudo dice [np.float32(0.7984)]\n",
      "2024-12-23 15:11:14.717090: Epoch time: 111.57 s\n",
      "2024-12-23 15:11:15.295301: \n",
      "2024-12-23 15:11:15.295473: Epoch 494\n",
      "2024-12-23 15:11:15.295549: Current learning rate: 0.00542\n",
      "2024-12-23 15:13:06.829365: train_loss -0.7794\n",
      "2024-12-23 15:13:06.829505: val_loss -0.7039\n",
      "2024-12-23 15:13:06.829539: Pseudo dice [np.float32(0.7956)]\n",
      "2024-12-23 15:13:06.829577: Epoch time: 111.53 s\n",
      "2024-12-23 15:13:07.410338: \n",
      "2024-12-23 15:13:07.410943: Epoch 495\n",
      "2024-12-23 15:13:07.411110: Current learning rate: 0.00541\n",
      "2024-12-23 15:14:58.971191: train_loss -0.7967\n",
      "2024-12-23 15:14:58.971619: val_loss -0.7207\n",
      "2024-12-23 15:14:58.971692: Pseudo dice [np.float32(0.7801)]\n",
      "2024-12-23 15:14:58.971739: Epoch time: 111.56 s\n",
      "2024-12-23 15:14:59.553661: \n",
      "2024-12-23 15:14:59.554018: Epoch 496\n",
      "2024-12-23 15:14:59.554105: Current learning rate: 0.0054\n",
      "2024-12-23 15:16:51.079329: train_loss -0.7832\n",
      "2024-12-23 15:16:51.079542: val_loss -0.7027\n",
      "2024-12-23 15:16:51.079580: Pseudo dice [np.float32(0.7912)]\n",
      "2024-12-23 15:16:51.079617: Epoch time: 111.53 s\n",
      "2024-12-23 15:16:51.657089: \n",
      "2024-12-23 15:16:51.657417: Epoch 497\n",
      "2024-12-23 15:16:51.657506: Current learning rate: 0.00539\n",
      "2024-12-23 15:18:43.227063: train_loss -0.7878\n",
      "2024-12-23 15:18:43.227193: val_loss -0.7377\n",
      "2024-12-23 15:18:43.227227: Pseudo dice [np.float32(0.79)]\n",
      "2024-12-23 15:18:43.227262: Epoch time: 111.57 s\n",
      "2024-12-23 15:18:43.809867: \n",
      "2024-12-23 15:18:43.809968: Epoch 498\n",
      "2024-12-23 15:18:43.810042: Current learning rate: 0.00538\n",
      "2024-12-23 15:20:35.049542: train_loss -0.8022\n",
      "2024-12-23 15:20:35.049697: val_loss -0.7686\n",
      "2024-12-23 15:20:35.050027: Pseudo dice [np.float32(0.8225)]\n",
      "2024-12-23 15:20:35.050140: Epoch time: 111.24 s\n",
      "2024-12-23 15:20:35.628462: \n",
      "2024-12-23 15:20:35.628808: Epoch 499\n",
      "2024-12-23 15:20:35.628907: Current learning rate: 0.00537\n",
      "2024-12-23 15:22:26.852254: train_loss -0.7866\n",
      "2024-12-23 15:22:26.852395: val_loss -0.7062\n",
      "2024-12-23 15:22:26.852430: Pseudo dice [np.float32(0.7607)]\n",
      "2024-12-23 15:22:26.852465: Epoch time: 111.22 s\n",
      "2024-12-23 15:22:28.130839: \n",
      "2024-12-23 15:22:28.131416: Epoch 500\n",
      "2024-12-23 15:22:28.131514: Current learning rate: 0.00536\n",
      "2024-12-23 15:24:19.379659: train_loss -0.7568\n",
      "2024-12-23 15:24:19.379827: val_loss -0.7346\n",
      "2024-12-23 15:24:19.379869: Pseudo dice [np.float32(0.8105)]\n",
      "2024-12-23 15:24:19.379906: Epoch time: 111.25 s\n",
      "2024-12-23 15:24:19.955554: \n",
      "2024-12-23 15:24:19.955895: Epoch 501\n",
      "2024-12-23 15:24:19.955980: Current learning rate: 0.00535\n",
      "2024-12-23 15:26:11.231523: train_loss -0.7908\n",
      "2024-12-23 15:26:11.231763: val_loss -0.7606\n",
      "2024-12-23 15:26:11.231810: Pseudo dice [np.float32(0.8192)]\n",
      "2024-12-23 15:26:11.231848: Epoch time: 111.28 s\n",
      "2024-12-23 15:26:11.812391: \n",
      "2024-12-23 15:26:11.812515: Epoch 502\n",
      "2024-12-23 15:26:11.812588: Current learning rate: 0.00534\n",
      "2024-12-23 15:28:03.045398: train_loss -0.8032\n",
      "2024-12-23 15:28:03.045559: val_loss -0.7056\n",
      "2024-12-23 15:28:03.045598: Pseudo dice [np.float32(0.7898)]\n",
      "2024-12-23 15:28:03.045635: Epoch time: 111.23 s\n",
      "2024-12-23 15:28:03.630745: \n",
      "2024-12-23 15:28:03.631235: Epoch 503\n",
      "2024-12-23 15:28:03.631371: Current learning rate: 0.00533\n",
      "2024-12-23 15:29:54.879500: train_loss -0.8173\n",
      "2024-12-23 15:29:54.879642: val_loss -0.7437\n",
      "2024-12-23 15:29:54.879676: Pseudo dice [np.float32(0.8112)]\n",
      "2024-12-23 15:29:54.879712: Epoch time: 111.25 s\n",
      "2024-12-23 15:29:55.460688: \n",
      "2024-12-23 15:29:55.461113: Epoch 504\n",
      "2024-12-23 15:29:55.461383: Current learning rate: 0.00532\n",
      "2024-12-23 15:31:46.733589: train_loss -0.7818\n",
      "2024-12-23 15:31:46.733748: val_loss -0.7274\n",
      "2024-12-23 15:31:46.733782: Pseudo dice [np.float32(0.7679)]\n",
      "2024-12-23 15:31:46.733820: Epoch time: 111.27 s\n",
      "2024-12-23 15:31:47.309483: \n",
      "2024-12-23 15:31:47.309761: Epoch 505\n",
      "2024-12-23 15:31:47.310004: Current learning rate: 0.00531\n",
      "2024-12-23 15:33:38.576886: train_loss -0.7823\n",
      "2024-12-23 15:33:38.577024: val_loss -0.7043\n",
      "2024-12-23 15:33:38.577058: Pseudo dice [np.float32(0.7876)]\n",
      "2024-12-23 15:33:38.577111: Epoch time: 111.27 s\n",
      "2024-12-23 15:33:39.163831: \n",
      "2024-12-23 15:33:39.164110: Epoch 506\n",
      "2024-12-23 15:33:39.164308: Current learning rate: 0.0053\n",
      "2024-12-23 15:35:30.420497: train_loss -0.8015\n",
      "2024-12-23 15:35:30.420636: val_loss -0.7181\n",
      "2024-12-23 15:35:30.420670: Pseudo dice [np.float32(0.7974)]\n",
      "2024-12-23 15:35:30.420706: Epoch time: 111.26 s\n",
      "2024-12-23 15:35:30.996983: \n",
      "2024-12-23 15:35:30.997248: Epoch 507\n",
      "2024-12-23 15:35:30.997395: Current learning rate: 0.00529\n",
      "2024-12-23 15:37:22.264382: train_loss -0.796\n",
      "2024-12-23 15:37:22.264542: val_loss -0.6502\n",
      "2024-12-23 15:37:22.264577: Pseudo dice [np.float32(0.7449)]\n",
      "2024-12-23 15:37:22.264618: Epoch time: 111.27 s\n",
      "2024-12-23 15:37:22.870036: \n",
      "2024-12-23 15:37:22.870147: Epoch 508\n",
      "2024-12-23 15:37:22.870223: Current learning rate: 0.00528\n",
      "2024-12-23 15:39:14.177276: train_loss -0.8141\n",
      "2024-12-23 15:39:14.177519: val_loss -0.7564\n",
      "2024-12-23 15:39:14.177733: Pseudo dice [np.float32(0.8163)]\n",
      "2024-12-23 15:39:14.177819: Epoch time: 111.31 s\n",
      "2024-12-23 15:39:14.760100: \n",
      "2024-12-23 15:39:14.760216: Epoch 509\n",
      "2024-12-23 15:39:14.760290: Current learning rate: 0.00527\n",
      "2024-12-23 15:41:06.017602: train_loss -0.8194\n",
      "2024-12-23 15:41:06.017747: val_loss -0.7585\n",
      "2024-12-23 15:41:06.017788: Pseudo dice [np.float32(0.8297)]\n",
      "2024-12-23 15:41:06.017826: Epoch time: 111.26 s\n",
      "2024-12-23 15:41:06.596433: \n",
      "2024-12-23 15:41:06.596697: Epoch 510\n",
      "2024-12-23 15:41:06.596889: Current learning rate: 0.00526\n",
      "2024-12-23 15:42:57.878715: train_loss -0.7984\n",
      "2024-12-23 15:42:57.878850: val_loss -0.7266\n",
      "2024-12-23 15:42:57.878882: Pseudo dice [np.float32(0.7842)]\n",
      "2024-12-23 15:42:57.878917: Epoch time: 111.28 s\n",
      "2024-12-23 15:42:58.460796: \n",
      "2024-12-23 15:42:58.461185: Epoch 511\n",
      "2024-12-23 15:42:58.461473: Current learning rate: 0.00525\n",
      "2024-12-23 15:44:49.789105: train_loss -0.7824\n",
      "2024-12-23 15:44:49.789330: val_loss -0.7268\n",
      "2024-12-23 15:44:49.789372: Pseudo dice [np.float32(0.7894)]\n",
      "2024-12-23 15:44:49.789459: Epoch time: 111.33 s\n",
      "2024-12-23 15:44:50.363663: \n",
      "2024-12-23 15:44:50.364029: Epoch 512\n",
      "2024-12-23 15:44:50.364172: Current learning rate: 0.00524\n",
      "2024-12-23 15:46:41.648377: train_loss -0.7958\n",
      "2024-12-23 15:46:41.648516: val_loss -0.7028\n",
      "2024-12-23 15:46:41.648550: Pseudo dice [np.float32(0.7507)]\n",
      "2024-12-23 15:46:41.648585: Epoch time: 111.29 s\n",
      "2024-12-23 15:46:42.229291: \n",
      "2024-12-23 15:46:42.229470: Epoch 513\n",
      "2024-12-23 15:46:42.229745: Current learning rate: 0.00523\n",
      "2024-12-23 15:48:33.478793: train_loss -0.809\n",
      "2024-12-23 15:48:33.478930: val_loss -0.7101\n",
      "2024-12-23 15:48:33.478964: Pseudo dice [np.float32(0.7795)]\n",
      "2024-12-23 15:48:33.479000: Epoch time: 111.25 s\n",
      "2024-12-23 15:48:34.056985: \n",
      "2024-12-23 15:48:34.057280: Epoch 514\n",
      "2024-12-23 15:48:34.057418: Current learning rate: 0.00522\n",
      "2024-12-23 15:50:25.353013: train_loss -0.8143\n",
      "2024-12-23 15:50:25.353153: val_loss -0.7345\n",
      "2024-12-23 15:50:25.353186: Pseudo dice [np.float32(0.7952)]\n",
      "2024-12-23 15:50:25.353222: Epoch time: 111.3 s\n",
      "2024-12-23 15:50:25.946366: \n",
      "2024-12-23 15:50:25.946812: Epoch 515\n",
      "2024-12-23 15:50:25.946985: Current learning rate: 0.00521\n",
      "2024-12-23 15:52:17.210572: train_loss -0.8218\n",
      "2024-12-23 15:52:17.210804: val_loss -0.6431\n",
      "2024-12-23 15:52:17.210849: Pseudo dice [np.float32(0.7192)]\n",
      "2024-12-23 15:52:17.210977: Epoch time: 111.26 s\n",
      "2024-12-23 15:52:17.786489: \n",
      "2024-12-23 15:52:17.786590: Epoch 516\n",
      "2024-12-23 15:52:17.786664: Current learning rate: 0.0052\n",
      "2024-12-23 15:54:09.051143: train_loss -0.8198\n",
      "2024-12-23 15:54:09.051292: val_loss -0.7377\n",
      "2024-12-23 15:54:09.051326: Pseudo dice [np.float32(0.8074)]\n",
      "2024-12-23 15:54:09.051363: Epoch time: 111.27 s\n",
      "2024-12-23 15:54:09.636593: \n",
      "2024-12-23 15:54:09.636700: Epoch 517\n",
      "2024-12-23 15:54:09.636775: Current learning rate: 0.00519\n",
      "2024-12-23 15:56:00.866551: train_loss -0.8225\n",
      "2024-12-23 15:56:00.866717: val_loss -0.6951\n",
      "2024-12-23 15:56:00.866751: Pseudo dice [np.float32(0.7558)]\n",
      "2024-12-23 15:56:00.866788: Epoch time: 111.23 s\n",
      "2024-12-23 15:56:01.900844: \n",
      "2024-12-23 15:56:01.900975: Epoch 518\n",
      "2024-12-23 15:56:01.901064: Current learning rate: 0.00518\n",
      "2024-12-23 15:57:53.163964: train_loss -0.8004\n",
      "2024-12-23 15:57:53.164113: val_loss -0.6652\n",
      "2024-12-23 15:57:53.164153: Pseudo dice [np.float32(0.7196)]\n",
      "2024-12-23 15:57:53.164195: Epoch time: 111.26 s\n",
      "2024-12-23 15:57:53.742236: \n",
      "2024-12-23 15:57:53.742368: Epoch 519\n",
      "2024-12-23 15:57:53.742442: Current learning rate: 0.00518\n",
      "2024-12-23 15:59:45.014311: train_loss -0.804\n",
      "2024-12-23 15:59:45.014456: val_loss -0.7577\n",
      "2024-12-23 15:59:45.014490: Pseudo dice [np.float32(0.7906)]\n",
      "2024-12-23 15:59:45.014526: Epoch time: 111.27 s\n",
      "2024-12-23 15:59:45.598135: \n",
      "2024-12-23 15:59:45.598523: Epoch 520\n",
      "2024-12-23 15:59:45.598748: Current learning rate: 0.00517\n",
      "2024-12-23 16:01:36.939022: train_loss -0.7983\n",
      "2024-12-23 16:01:36.939169: val_loss -0.736\n",
      "2024-12-23 16:01:36.939363: Pseudo dice [np.float32(0.8049)]\n",
      "2024-12-23 16:01:36.939438: Epoch time: 111.34 s\n",
      "2024-12-23 16:01:37.520118: \n",
      "2024-12-23 16:01:37.520331: Epoch 521\n",
      "2024-12-23 16:01:37.520451: Current learning rate: 0.00516\n",
      "2024-12-23 16:03:28.830898: train_loss -0.7923\n",
      "2024-12-23 16:03:28.831301: val_loss -0.748\n",
      "2024-12-23 16:03:28.831341: Pseudo dice [np.float32(0.7991)]\n",
      "2024-12-23 16:03:28.831389: Epoch time: 111.31 s\n",
      "2024-12-23 16:03:29.402090: \n",
      "2024-12-23 16:03:29.402221: Epoch 522\n",
      "2024-12-23 16:03:29.402294: Current learning rate: 0.00515\n",
      "2024-12-23 16:05:20.668023: train_loss -0.81\n",
      "2024-12-23 16:05:20.668195: val_loss -0.7218\n",
      "2024-12-23 16:05:20.668324: Pseudo dice [np.float32(0.7769)]\n",
      "2024-12-23 16:05:20.668371: Epoch time: 111.27 s\n",
      "2024-12-23 16:05:21.243067: \n",
      "2024-12-23 16:05:21.243185: Epoch 523\n",
      "2024-12-23 16:05:21.243258: Current learning rate: 0.00514\n",
      "2024-12-23 16:07:12.545376: train_loss -0.8019\n",
      "2024-12-23 16:07:12.545519: val_loss -0.71\n",
      "2024-12-23 16:07:12.545553: Pseudo dice [np.float32(0.6947)]\n",
      "2024-12-23 16:07:12.545589: Epoch time: 111.3 s\n",
      "2024-12-23 16:07:13.120796: \n",
      "2024-12-23 16:07:13.120983: Epoch 524\n",
      "2024-12-23 16:07:13.121060: Current learning rate: 0.00513\n",
      "2024-12-23 16:09:04.357061: train_loss -0.7515\n",
      "2024-12-23 16:09:04.357296: val_loss -0.66\n",
      "2024-12-23 16:09:04.357330: Pseudo dice [np.float32(0.7086)]\n",
      "2024-12-23 16:09:04.357367: Epoch time: 111.24 s\n",
      "2024-12-23 16:09:04.947634: \n",
      "2024-12-23 16:09:04.947743: Epoch 525\n",
      "2024-12-23 16:09:04.947887: Current learning rate: 0.00512\n",
      "2024-12-23 16:10:56.154643: train_loss -0.719\n",
      "2024-12-23 16:10:56.154790: val_loss -0.6807\n",
      "2024-12-23 16:10:56.154822: Pseudo dice [np.float32(0.7465)]\n",
      "2024-12-23 16:10:56.154860: Epoch time: 111.21 s\n",
      "2024-12-23 16:10:56.740454: \n",
      "2024-12-23 16:10:56.740807: Epoch 526\n",
      "2024-12-23 16:10:56.740894: Current learning rate: 0.00511\n",
      "2024-12-23 16:12:47.988907: train_loss -0.7478\n",
      "2024-12-23 16:12:47.989116: val_loss -0.7625\n",
      "2024-12-23 16:12:47.989152: Pseudo dice [np.float32(0.7862)]\n",
      "2024-12-23 16:12:47.989189: Epoch time: 111.25 s\n",
      "2024-12-23 16:12:48.572806: \n",
      "2024-12-23 16:12:48.573005: Epoch 527\n",
      "2024-12-23 16:12:48.573081: Current learning rate: 0.0051\n",
      "2024-12-23 16:14:39.790375: train_loss -0.7854\n",
      "2024-12-23 16:14:39.790580: val_loss -0.7202\n",
      "2024-12-23 16:14:39.790627: Pseudo dice [np.float32(0.7558)]\n",
      "2024-12-23 16:14:39.790664: Epoch time: 111.22 s\n",
      "2024-12-23 16:14:40.364814: \n",
      "2024-12-23 16:14:40.365153: Epoch 528\n",
      "2024-12-23 16:14:40.365261: Current learning rate: 0.00509\n",
      "2024-12-23 16:16:31.569192: train_loss -0.7858\n",
      "2024-12-23 16:16:31.569392: val_loss -0.6955\n",
      "2024-12-23 16:16:31.569488: Pseudo dice [np.float32(0.773)]\n",
      "2024-12-23 16:16:31.569597: Epoch time: 111.2 s\n",
      "2024-12-23 16:16:32.142587: \n",
      "2024-12-23 16:16:32.142945: Epoch 529\n",
      "2024-12-23 16:16:32.143135: Current learning rate: 0.00508\n",
      "2024-12-23 16:18:23.363230: train_loss -0.7688\n",
      "2024-12-23 16:18:23.363411: val_loss -0.7677\n",
      "2024-12-23 16:18:23.363463: Pseudo dice [np.float32(0.8254)]\n",
      "2024-12-23 16:18:23.363514: Epoch time: 111.22 s\n",
      "2024-12-23 16:18:23.940409: \n",
      "2024-12-23 16:18:23.940810: Epoch 530\n",
      "2024-12-23 16:18:23.940897: Current learning rate: 0.00507\n",
      "2024-12-23 16:20:15.135765: train_loss -0.7872\n",
      "2024-12-23 16:20:15.135916: val_loss -0.7019\n",
      "2024-12-23 16:20:15.135950: Pseudo dice [np.float32(0.7736)]\n",
      "2024-12-23 16:20:15.135984: Epoch time: 111.2 s\n",
      "2024-12-23 16:20:15.711308: \n",
      "2024-12-23 16:20:15.711419: Epoch 531\n",
      "2024-12-23 16:20:15.711492: Current learning rate: 0.00506\n",
      "2024-12-23 16:22:06.912252: train_loss -0.7884\n",
      "2024-12-23 16:22:06.912400: val_loss -0.6995\n",
      "2024-12-23 16:22:06.912435: Pseudo dice [np.float32(0.7766)]\n",
      "2024-12-23 16:22:06.912472: Epoch time: 111.2 s\n",
      "2024-12-23 16:22:07.494566: \n",
      "2024-12-23 16:22:07.494821: Epoch 532\n",
      "2024-12-23 16:22:07.494905: Current learning rate: 0.00505\n",
      "2024-12-23 16:23:58.737980: train_loss -0.7877\n",
      "2024-12-23 16:23:58.738126: val_loss -0.7125\n",
      "2024-12-23 16:23:58.738160: Pseudo dice [np.float32(0.7706)]\n",
      "2024-12-23 16:23:58.738195: Epoch time: 111.24 s\n",
      "2024-12-23 16:23:59.313535: \n",
      "2024-12-23 16:23:59.313637: Epoch 533\n",
      "2024-12-23 16:23:59.313711: Current learning rate: 0.00504\n",
      "2024-12-23 16:25:50.544417: train_loss -0.778\n",
      "2024-12-23 16:25:50.544614: val_loss -0.7244\n",
      "2024-12-23 16:25:50.544646: Pseudo dice [np.float32(0.7876)]\n",
      "2024-12-23 16:25:50.544683: Epoch time: 111.23 s\n",
      "2024-12-23 16:25:51.125298: \n",
      "2024-12-23 16:25:51.125401: Epoch 534\n",
      "2024-12-23 16:25:51.125480: Current learning rate: 0.00503\n",
      "2024-12-23 16:27:42.304094: train_loss -0.8116\n",
      "2024-12-23 16:27:42.304306: val_loss -0.7179\n",
      "2024-12-23 16:27:42.304418: Pseudo dice [np.float32(0.7698)]\n",
      "2024-12-23 16:27:42.304495: Epoch time: 111.18 s\n",
      "2024-12-23 16:27:42.883822: \n",
      "2024-12-23 16:27:42.884139: Epoch 535\n",
      "2024-12-23 16:27:42.884254: Current learning rate: 0.00502\n",
      "2024-12-23 16:29:34.123191: train_loss -0.7877\n",
      "2024-12-23 16:29:34.123328: val_loss -0.752\n",
      "2024-12-23 16:29:34.123364: Pseudo dice [np.float32(0.8157)]\n",
      "2024-12-23 16:29:34.123403: Epoch time: 111.24 s\n",
      "2024-12-23 16:29:34.707474: \n",
      "2024-12-23 16:29:34.707590: Epoch 536\n",
      "2024-12-23 16:29:34.707720: Current learning rate: 0.00501\n",
      "2024-12-23 16:31:25.977268: train_loss -0.8218\n",
      "2024-12-23 16:31:25.977653: val_loss -0.6755\n",
      "2024-12-23 16:31:25.977715: Pseudo dice [np.float32(0.7733)]\n",
      "2024-12-23 16:31:25.977760: Epoch time: 111.27 s\n",
      "2024-12-23 16:31:27.032934: \n",
      "2024-12-23 16:31:27.033479: Epoch 537\n",
      "2024-12-23 16:31:27.033576: Current learning rate: 0.005\n",
      "2024-12-23 16:33:18.197336: train_loss -0.8172\n",
      "2024-12-23 16:33:18.197560: val_loss -0.5403\n",
      "2024-12-23 16:33:18.197732: Pseudo dice [np.float32(0.6729)]\n",
      "2024-12-23 16:33:18.197784: Epoch time: 111.16 s\n",
      "2024-12-23 16:33:18.779511: \n",
      "2024-12-23 16:33:18.779892: Epoch 538\n",
      "2024-12-23 16:33:18.780082: Current learning rate: 0.00499\n",
      "2024-12-23 16:35:09.839327: train_loss -0.8121\n",
      "2024-12-23 16:35:09.839472: val_loss -0.7151\n",
      "2024-12-23 16:35:09.839506: Pseudo dice [np.float32(0.7866)]\n",
      "2024-12-23 16:35:09.839542: Epoch time: 111.06 s\n",
      "2024-12-23 16:35:10.422585: \n",
      "2024-12-23 16:35:10.422986: Epoch 539\n",
      "2024-12-23 16:35:10.423280: Current learning rate: 0.00498\n",
      "2024-12-23 16:37:01.508531: train_loss -0.8024\n",
      "2024-12-23 16:37:01.508672: val_loss -0.6937\n",
      "2024-12-23 16:37:01.508705: Pseudo dice [np.float32(0.7572)]\n",
      "2024-12-23 16:37:01.508742: Epoch time: 111.09 s\n",
      "2024-12-23 16:37:02.090983: \n",
      "2024-12-23 16:37:02.091119: Epoch 540\n",
      "2024-12-23 16:37:02.091192: Current learning rate: 0.00497\n",
      "2024-12-23 16:38:53.376827: train_loss -0.8103\n",
      "2024-12-23 16:38:53.376975: val_loss -0.7704\n",
      "2024-12-23 16:38:53.377012: Pseudo dice [np.float32(0.8054)]\n",
      "2024-12-23 16:38:53.377048: Epoch time: 111.29 s\n",
      "2024-12-23 16:38:53.961123: \n",
      "2024-12-23 16:38:53.961245: Epoch 541\n",
      "2024-12-23 16:38:53.961318: Current learning rate: 0.00496\n",
      "2024-12-23 16:40:45.198440: train_loss -0.7867\n",
      "2024-12-23 16:40:45.198579: val_loss -0.7345\n",
      "2024-12-23 16:40:45.198612: Pseudo dice [np.float32(0.8108)]\n",
      "2024-12-23 16:40:45.198648: Epoch time: 111.24 s\n",
      "2024-12-23 16:40:45.788410: \n",
      "2024-12-23 16:40:45.788528: Epoch 542\n",
      "2024-12-23 16:40:45.788600: Current learning rate: 0.00495\n",
      "2024-12-23 16:42:36.997222: train_loss -0.7825\n",
      "2024-12-23 16:42:36.997425: val_loss -0.7693\n",
      "2024-12-23 16:42:36.997462: Pseudo dice [np.float32(0.8302)]\n",
      "2024-12-23 16:42:36.997499: Epoch time: 111.21 s\n",
      "2024-12-23 16:42:37.580895: \n",
      "2024-12-23 16:42:37.581005: Epoch 543\n",
      "2024-12-23 16:42:37.581076: Current learning rate: 0.00494\n",
      "2024-12-23 16:44:28.681392: train_loss -0.818\n",
      "2024-12-23 16:44:28.681548: val_loss -0.7135\n",
      "2024-12-23 16:44:28.681584: Pseudo dice [np.float32(0.7554)]\n",
      "2024-12-23 16:44:28.681621: Epoch time: 111.1 s\n",
      "2024-12-23 16:44:29.257785: \n",
      "2024-12-23 16:44:29.257983: Epoch 544\n",
      "2024-12-23 16:44:29.258126: Current learning rate: 0.00493\n",
      "2024-12-23 16:46:20.487093: train_loss -0.8088\n",
      "2024-12-23 16:46:20.487260: val_loss -0.6377\n",
      "2024-12-23 16:46:20.487296: Pseudo dice [np.float32(0.6887)]\n",
      "2024-12-23 16:46:20.487331: Epoch time: 111.23 s\n",
      "2024-12-23 16:46:21.070619: \n",
      "2024-12-23 16:46:21.070933: Epoch 545\n",
      "2024-12-23 16:46:21.071074: Current learning rate: 0.00492\n",
      "2024-12-23 16:48:12.285530: train_loss -0.8181\n",
      "2024-12-23 16:48:12.285679: val_loss -0.6848\n",
      "2024-12-23 16:48:12.285716: Pseudo dice [np.float32(0.7462)]\n",
      "2024-12-23 16:48:12.285770: Epoch time: 111.22 s\n",
      "2024-12-23 16:48:12.868711: \n",
      "2024-12-23 16:48:12.869066: Epoch 546\n",
      "2024-12-23 16:48:12.869276: Current learning rate: 0.00491\n",
      "2024-12-23 16:50:04.094274: train_loss -0.8155\n",
      "2024-12-23 16:50:04.094426: val_loss -0.7183\n",
      "2024-12-23 16:50:04.094463: Pseudo dice [np.float32(0.7516)]\n",
      "2024-12-23 16:50:04.094502: Epoch time: 111.23 s\n",
      "2024-12-23 16:50:04.676720: \n",
      "2024-12-23 16:50:04.676913: Epoch 547\n",
      "2024-12-23 16:50:04.676996: Current learning rate: 0.0049\n",
      "2024-12-23 16:51:55.729834: train_loss -0.8188\n",
      "2024-12-23 16:51:55.729986: val_loss -0.7176\n",
      "2024-12-23 16:51:55.730023: Pseudo dice [np.float32(0.8023)]\n",
      "2024-12-23 16:51:55.730059: Epoch time: 111.05 s\n",
      "2024-12-23 16:51:56.302248: \n",
      "2024-12-23 16:51:56.302684: Epoch 548\n",
      "2024-12-23 16:51:56.302805: Current learning rate: 0.00489\n",
      "2024-12-23 16:53:47.483063: train_loss -0.8305\n",
      "2024-12-23 16:53:47.483264: val_loss -0.6775\n",
      "2024-12-23 16:53:47.483363: Pseudo dice [np.float32(0.7465)]\n",
      "2024-12-23 16:53:47.483407: Epoch time: 111.18 s\n",
      "2024-12-23 16:53:48.060935: \n",
      "2024-12-23 16:53:48.061123: Epoch 549\n",
      "2024-12-23 16:53:48.061200: Current learning rate: 0.00488\n",
      "2024-12-23 16:55:39.083335: train_loss -0.8327\n",
      "2024-12-23 16:55:39.083469: val_loss -0.7264\n",
      "2024-12-23 16:55:39.083503: Pseudo dice [np.float32(0.7567)]\n",
      "2024-12-23 16:55:39.083539: Epoch time: 111.02 s\n",
      "2024-12-23 16:55:39.915025: \n",
      "2024-12-23 16:55:39.915227: Epoch 550\n",
      "2024-12-23 16:55:39.915305: Current learning rate: 0.00487\n",
      "2024-12-23 16:57:31.124544: train_loss -0.8253\n",
      "2024-12-23 16:57:31.124717: val_loss -0.7268\n",
      "2024-12-23 16:57:31.124754: Pseudo dice [np.float32(0.7827)]\n",
      "2024-12-23 16:57:31.124818: Epoch time: 111.21 s\n",
      "2024-12-23 16:57:31.697218: \n",
      "2024-12-23 16:57:31.697634: Epoch 551\n",
      "2024-12-23 16:57:31.697903: Current learning rate: 0.00486\n",
      "2024-12-23 16:59:22.913035: train_loss -0.811\n",
      "2024-12-23 16:59:22.913197: val_loss -0.6367\n",
      "2024-12-23 16:59:22.913234: Pseudo dice [np.float32(0.8212)]\n",
      "2024-12-23 16:59:22.913272: Epoch time: 111.22 s\n",
      "2024-12-23 16:59:23.489498: \n",
      "2024-12-23 16:59:23.489883: Epoch 552\n",
      "2024-12-23 16:59:23.490105: Current learning rate: 0.00485\n",
      "2024-12-23 17:01:14.617108: train_loss -0.8091\n",
      "2024-12-23 17:01:14.617262: val_loss -0.7314\n",
      "2024-12-23 17:01:14.617298: Pseudo dice [np.float32(0.7954)]\n",
      "2024-12-23 17:01:14.617334: Epoch time: 111.13 s\n",
      "2024-12-23 17:01:15.194981: \n",
      "2024-12-23 17:01:15.195077: Epoch 553\n",
      "2024-12-23 17:01:15.195147: Current learning rate: 0.00484\n",
      "2024-12-23 17:03:06.255830: train_loss -0.8059\n",
      "2024-12-23 17:03:06.256058: val_loss -0.7134\n",
      "2024-12-23 17:03:06.256112: Pseudo dice [np.float32(0.7712)]\n",
      "2024-12-23 17:03:06.256155: Epoch time: 111.06 s\n",
      "2024-12-23 17:03:06.831466: \n",
      "2024-12-23 17:03:06.831563: Epoch 554\n",
      "2024-12-23 17:03:06.831633: Current learning rate: 0.00484\n",
      "2024-12-23 17:04:57.904254: train_loss -0.7878\n",
      "2024-12-23 17:04:57.904389: val_loss -0.7808\n",
      "2024-12-23 17:04:57.904423: Pseudo dice [np.float32(0.8181)]\n",
      "2024-12-23 17:04:57.904460: Epoch time: 111.07 s\n",
      "2024-12-23 17:04:58.957804: \n",
      "2024-12-23 17:04:58.958032: Epoch 555\n",
      "2024-12-23 17:04:58.958183: Current learning rate: 0.00483\n",
      "2024-12-23 17:06:50.073881: train_loss -0.7861\n",
      "2024-12-23 17:06:50.074073: val_loss -0.6894\n",
      "2024-12-23 17:06:50.074234: Pseudo dice [np.float32(0.7697)]\n",
      "2024-12-23 17:06:50.074293: Epoch time: 111.12 s\n",
      "2024-12-23 17:06:50.660114: \n",
      "2024-12-23 17:06:50.660421: Epoch 556\n",
      "2024-12-23 17:06:50.660550: Current learning rate: 0.00482\n",
      "2024-12-23 17:08:41.852535: train_loss -0.7992\n",
      "2024-12-23 17:08:41.852684: val_loss -0.7629\n",
      "2024-12-23 17:08:41.852721: Pseudo dice [np.float32(0.8256)]\n",
      "2024-12-23 17:08:41.852756: Epoch time: 111.19 s\n",
      "2024-12-23 17:08:42.444027: \n",
      "2024-12-23 17:08:42.444374: Epoch 557\n",
      "2024-12-23 17:08:42.444623: Current learning rate: 0.00481\n",
      "2024-12-23 17:10:33.655211: train_loss -0.7917\n",
      "2024-12-23 17:10:33.655359: val_loss -0.6882\n",
      "2024-12-23 17:10:33.655395: Pseudo dice [np.float32(0.7188)]\n",
      "2024-12-23 17:10:33.655432: Epoch time: 111.21 s\n",
      "2024-12-23 17:10:34.231527: \n",
      "2024-12-23 17:10:34.231895: Epoch 558\n",
      "2024-12-23 17:10:34.231980: Current learning rate: 0.0048\n",
      "2024-12-23 17:12:25.268696: train_loss -0.8066\n",
      "2024-12-23 17:12:25.268947: val_loss -0.7092\n",
      "2024-12-23 17:12:25.269016: Pseudo dice [np.float32(0.7814)]\n",
      "2024-12-23 17:12:25.269062: Epoch time: 111.04 s\n",
      "2024-12-23 17:12:25.855776: \n",
      "2024-12-23 17:12:25.856210: Epoch 559\n",
      "2024-12-23 17:12:25.856386: Current learning rate: 0.00479\n",
      "2024-12-23 17:14:16.888704: train_loss -0.8336\n",
      "2024-12-23 17:14:16.888948: val_loss -0.7516\n",
      "2024-12-23 17:14:16.888997: Pseudo dice [np.float32(0.7973)]\n",
      "2024-12-23 17:14:16.889038: Epoch time: 111.03 s\n",
      "2024-12-23 17:14:17.473218: \n",
      "2024-12-23 17:14:17.473389: Epoch 560\n",
      "2024-12-23 17:14:17.473466: Current learning rate: 0.00478\n",
      "2024-12-23 17:16:08.644758: train_loss -0.8143\n",
      "2024-12-23 17:16:08.644896: val_loss -0.6218\n",
      "2024-12-23 17:16:08.644928: Pseudo dice [np.float32(0.7115)]\n",
      "2024-12-23 17:16:08.644962: Epoch time: 111.17 s\n",
      "2024-12-23 17:16:09.228932: \n",
      "2024-12-23 17:16:09.229308: Epoch 561\n",
      "2024-12-23 17:16:09.229391: Current learning rate: 0.00477\n",
      "2024-12-23 17:18:00.408209: train_loss -0.783\n",
      "2024-12-23 17:18:00.408376: val_loss -0.7084\n",
      "2024-12-23 17:18:00.408664: Pseudo dice [np.float32(0.7606)]\n",
      "2024-12-23 17:18:00.408779: Epoch time: 111.18 s\n",
      "2024-12-23 17:18:00.986025: \n",
      "2024-12-23 17:18:00.986226: Epoch 562\n",
      "2024-12-23 17:18:00.986321: Current learning rate: 0.00476\n",
      "2024-12-23 17:19:52.180558: train_loss -0.789\n",
      "2024-12-23 17:19:52.180715: val_loss -0.6948\n",
      "2024-12-23 17:19:52.180751: Pseudo dice [np.float32(0.7637)]\n",
      "2024-12-23 17:19:52.180787: Epoch time: 111.2 s\n",
      "2024-12-23 17:19:52.756635: \n",
      "2024-12-23 17:19:52.756825: Epoch 563\n",
      "2024-12-23 17:19:52.756907: Current learning rate: 0.00475\n",
      "2024-12-23 17:21:43.781829: train_loss -0.8115\n",
      "2024-12-23 17:21:43.781976: val_loss -0.7193\n",
      "2024-12-23 17:21:43.782014: Pseudo dice [np.float32(0.7828)]\n",
      "2024-12-23 17:21:43.782051: Epoch time: 111.03 s\n",
      "2024-12-23 17:21:44.369780: \n",
      "2024-12-23 17:21:44.370076: Epoch 564\n",
      "2024-12-23 17:21:44.370172: Current learning rate: 0.00474\n",
      "2024-12-23 17:23:35.383407: train_loss -0.7973\n",
      "2024-12-23 17:23:35.383599: val_loss -0.6918\n",
      "2024-12-23 17:23:35.383714: Pseudo dice [np.float32(0.735)]\n",
      "2024-12-23 17:23:35.383799: Epoch time: 111.01 s\n",
      "2024-12-23 17:23:35.968673: \n",
      "2024-12-23 17:23:35.968786: Epoch 565\n",
      "2024-12-23 17:23:35.968857: Current learning rate: 0.00473\n",
      "2024-12-23 17:25:26.981049: train_loss -0.7976\n",
      "2024-12-23 17:25:26.981199: val_loss -0.6857\n",
      "2024-12-23 17:25:26.981235: Pseudo dice [np.float32(0.7475)]\n",
      "2024-12-23 17:25:26.981272: Epoch time: 111.01 s\n",
      "2024-12-23 17:25:27.563501: \n",
      "2024-12-23 17:25:27.563608: Epoch 566\n",
      "2024-12-23 17:25:27.563681: Current learning rate: 0.00472\n",
      "2024-12-23 17:27:18.621150: train_loss -0.7914\n",
      "2024-12-23 17:27:18.621305: val_loss -0.73\n",
      "2024-12-23 17:27:18.621339: Pseudo dice [np.float32(0.8074)]\n",
      "2024-12-23 17:27:18.621376: Epoch time: 111.06 s\n",
      "2024-12-23 17:27:19.195856: \n",
      "2024-12-23 17:27:19.196169: Epoch 567\n",
      "2024-12-23 17:27:19.196316: Current learning rate: 0.00471\n",
      "2024-12-23 17:29:10.222916: train_loss -0.7766\n",
      "2024-12-23 17:29:10.223053: val_loss -0.7493\n",
      "2024-12-23 17:29:10.223085: Pseudo dice [np.float32(0.799)]\n",
      "2024-12-23 17:29:10.223119: Epoch time: 111.03 s\n",
      "2024-12-23 17:29:10.813351: \n",
      "2024-12-23 17:29:10.813526: Epoch 568\n",
      "2024-12-23 17:29:10.813600: Current learning rate: 0.0047\n",
      "2024-12-23 17:31:01.859333: train_loss -0.7913\n",
      "2024-12-23 17:31:01.859492: val_loss -0.7494\n",
      "2024-12-23 17:31:01.859527: Pseudo dice [np.float32(0.8046)]\n",
      "2024-12-23 17:31:01.859563: Epoch time: 111.05 s\n",
      "2024-12-23 17:31:02.436726: \n",
      "2024-12-23 17:31:02.436831: Epoch 569\n",
      "2024-12-23 17:31:02.436902: Current learning rate: 0.00469\n",
      "2024-12-23 17:32:53.463335: train_loss -0.7659\n",
      "2024-12-23 17:32:53.463494: val_loss -0.6672\n",
      "2024-12-23 17:32:53.463529: Pseudo dice [np.float32(0.7577)]\n",
      "2024-12-23 17:32:53.463567: Epoch time: 111.03 s\n",
      "2024-12-23 17:32:54.045415: \n",
      "2024-12-23 17:32:54.045509: Epoch 570\n",
      "2024-12-23 17:32:54.045585: Current learning rate: 0.00468\n",
      "2024-12-23 17:34:45.238266: train_loss -0.8013\n",
      "2024-12-23 17:34:45.238474: val_loss -0.713\n",
      "2024-12-23 17:34:45.238509: Pseudo dice [np.float32(0.784)]\n",
      "2024-12-23 17:34:45.238550: Epoch time: 111.19 s\n",
      "2024-12-23 17:34:45.812956: \n",
      "2024-12-23 17:34:45.813320: Epoch 571\n",
      "2024-12-23 17:34:45.813583: Current learning rate: 0.00467\n",
      "2024-12-23 17:36:36.868161: train_loss -0.72\n",
      "2024-12-23 17:36:36.868424: val_loss -0.6628\n",
      "2024-12-23 17:36:36.868467: Pseudo dice [np.float32(0.7383)]\n",
      "2024-12-23 17:36:36.868503: Epoch time: 111.06 s\n",
      "2024-12-23 17:36:37.445037: \n",
      "2024-12-23 17:36:37.445134: Epoch 572\n",
      "2024-12-23 17:36:37.445207: Current learning rate: 0.00466\n",
      "2024-12-23 17:38:28.499333: train_loss -0.7496\n",
      "2024-12-23 17:38:28.499475: val_loss -0.6901\n",
      "2024-12-23 17:38:28.499510: Pseudo dice [np.float32(0.7601)]\n",
      "2024-12-23 17:38:28.499547: Epoch time: 111.05 s\n",
      "2024-12-23 17:38:29.518259: \n",
      "2024-12-23 17:38:29.518452: Epoch 573\n",
      "2024-12-23 17:38:29.518541: Current learning rate: 0.00465\n",
      "2024-12-23 17:40:20.600511: train_loss -0.7906\n",
      "2024-12-23 17:40:20.600679: val_loss -0.7862\n",
      "2024-12-23 17:40:20.600712: Pseudo dice [np.float32(0.8376)]\n",
      "2024-12-23 17:40:20.600749: Epoch time: 111.08 s\n",
      "2024-12-23 17:40:21.181668: \n",
      "2024-12-23 17:40:21.182076: Epoch 574\n",
      "2024-12-23 17:40:21.182266: Current learning rate: 0.00464\n",
      "2024-12-23 17:42:12.201377: train_loss -0.793\n",
      "2024-12-23 17:42:12.201738: val_loss -0.6395\n",
      "2024-12-23 17:42:12.201894: Pseudo dice [np.float32(0.7003)]\n",
      "2024-12-23 17:42:12.201951: Epoch time: 111.02 s\n",
      "2024-12-23 17:42:12.784536: \n",
      "2024-12-23 17:42:12.784665: Epoch 575\n",
      "2024-12-23 17:42:12.784742: Current learning rate: 0.00463\n",
      "2024-12-23 17:44:03.798222: train_loss -0.7903\n",
      "2024-12-23 17:44:03.798365: val_loss -0.7213\n",
      "2024-12-23 17:44:03.798396: Pseudo dice [np.float32(0.7808)]\n",
      "2024-12-23 17:44:03.798433: Epoch time: 111.01 s\n",
      "2024-12-23 17:44:04.392209: \n",
      "2024-12-23 17:44:04.392332: Epoch 576\n",
      "2024-12-23 17:44:04.392405: Current learning rate: 0.00462\n",
      "2024-12-23 17:45:55.540057: train_loss -0.7811\n",
      "2024-12-23 17:45:55.540197: val_loss -0.6846\n",
      "2024-12-23 17:45:55.540230: Pseudo dice [np.float32(0.7399)]\n",
      "2024-12-23 17:45:55.540266: Epoch time: 111.15 s\n",
      "2024-12-23 17:45:56.121666: \n",
      "2024-12-23 17:45:56.121890: Epoch 577\n",
      "2024-12-23 17:45:56.121972: Current learning rate: 0.00461\n",
      "2024-12-23 17:47:47.164602: train_loss -0.7856\n",
      "2024-12-23 17:47:47.164743: val_loss -0.7255\n",
      "2024-12-23 17:47:47.164777: Pseudo dice [np.float32(0.7942)]\n",
      "2024-12-23 17:47:47.164812: Epoch time: 111.04 s\n",
      "2024-12-23 17:47:47.754003: \n",
      "2024-12-23 17:47:47.754119: Epoch 578\n",
      "2024-12-23 17:47:47.754188: Current learning rate: 0.0046\n",
      "2024-12-23 17:49:38.846886: train_loss -0.7823\n",
      "2024-12-23 17:49:38.847034: val_loss -0.7242\n",
      "2024-12-23 17:49:38.847070: Pseudo dice [np.float32(0.761)]\n",
      "2024-12-23 17:49:38.847106: Epoch time: 111.09 s\n",
      "2024-12-23 17:49:39.439053: \n",
      "2024-12-23 17:49:39.439458: Epoch 579\n",
      "2024-12-23 17:49:39.439542: Current learning rate: 0.00459\n",
      "2024-12-23 17:51:30.466480: train_loss -0.8013\n",
      "2024-12-23 17:51:30.466627: val_loss -0.7195\n",
      "2024-12-23 17:51:30.466660: Pseudo dice [np.float32(0.8204)]\n",
      "2024-12-23 17:51:30.466696: Epoch time: 111.03 s\n",
      "2024-12-23 17:51:31.063826: \n",
      "2024-12-23 17:51:31.063947: Epoch 580\n",
      "2024-12-23 17:51:31.064022: Current learning rate: 0.00458\n",
      "2024-12-23 17:53:22.282764: train_loss -0.7893\n",
      "2024-12-23 17:53:22.282914: val_loss -0.7243\n",
      "2024-12-23 17:53:22.282949: Pseudo dice [np.float32(0.7935)]\n",
      "2024-12-23 17:53:22.282986: Epoch time: 111.22 s\n",
      "2024-12-23 17:53:22.868919: \n",
      "2024-12-23 17:53:22.869039: Epoch 581\n",
      "2024-12-23 17:53:22.869117: Current learning rate: 0.00457\n",
      "2024-12-23 17:55:13.934246: train_loss -0.7495\n",
      "2024-12-23 17:55:13.934495: val_loss -0.6542\n",
      "2024-12-23 17:55:13.934532: Pseudo dice [np.float32(0.7393)]\n",
      "2024-12-23 17:55:13.934569: Epoch time: 111.07 s\n",
      "2024-12-23 17:55:14.532179: \n",
      "2024-12-23 17:55:14.532300: Epoch 582\n",
      "2024-12-23 17:55:14.532375: Current learning rate: 0.00456\n",
      "2024-12-23 17:57:05.533993: train_loss -0.8015\n",
      "2024-12-23 17:57:05.534150: val_loss -0.6939\n",
      "2024-12-23 17:57:05.534186: Pseudo dice [np.float32(0.8226)]\n",
      "2024-12-23 17:57:05.534222: Epoch time: 111.0 s\n",
      "2024-12-23 17:57:06.114226: \n",
      "2024-12-23 17:57:06.114590: Epoch 583\n",
      "2024-12-23 17:57:06.114698: Current learning rate: 0.00455\n",
      "2024-12-23 17:58:57.329019: train_loss -0.799\n",
      "2024-12-23 17:58:57.329301: val_loss -0.6229\n",
      "2024-12-23 17:58:57.329502: Pseudo dice [np.float32(0.7098)]\n",
      "2024-12-23 17:58:57.329559: Epoch time: 111.22 s\n",
      "2024-12-23 17:58:57.925564: \n",
      "2024-12-23 17:58:57.925768: Epoch 584\n",
      "2024-12-23 17:58:57.925927: Current learning rate: 0.00454\n",
      "2024-12-23 18:00:49.135188: train_loss -0.7415\n",
      "2024-12-23 18:00:49.135323: val_loss -0.6887\n",
      "2024-12-23 18:00:49.135358: Pseudo dice [np.float32(0.7391)]\n",
      "2024-12-23 18:00:49.135396: Epoch time: 111.21 s\n",
      "2024-12-23 18:00:49.726792: \n",
      "2024-12-23 18:00:49.727138: Epoch 585\n",
      "2024-12-23 18:00:49.727285: Current learning rate: 0.00453\n",
      "2024-12-23 18:02:40.833921: train_loss -0.7681\n",
      "2024-12-23 18:02:40.834135: val_loss -0.7026\n",
      "2024-12-23 18:02:40.834168: Pseudo dice [np.float32(0.7543)]\n",
      "2024-12-23 18:02:40.834204: Epoch time: 111.11 s\n",
      "2024-12-23 18:02:41.420429: \n",
      "2024-12-23 18:02:41.420617: Epoch 586\n",
      "2024-12-23 18:02:41.420696: Current learning rate: 0.00452\n",
      "2024-12-23 18:04:32.556839: train_loss -0.7586\n",
      "2024-12-23 18:04:32.557071: val_loss -0.7256\n",
      "2024-12-23 18:04:32.557108: Pseudo dice [np.float32(0.7632)]\n",
      "2024-12-23 18:04:32.557145: Epoch time: 111.14 s\n",
      "2024-12-23 18:04:33.144132: \n",
      "2024-12-23 18:04:33.144238: Epoch 587\n",
      "2024-12-23 18:04:33.144312: Current learning rate: 0.00451\n",
      "2024-12-23 18:06:24.189743: train_loss -0.7815\n",
      "2024-12-23 18:06:24.190154: val_loss -0.7466\n",
      "2024-12-23 18:06:24.190309: Pseudo dice [np.float32(0.7961)]\n",
      "2024-12-23 18:06:24.190361: Epoch time: 111.05 s\n",
      "2024-12-23 18:06:24.777550: \n",
      "2024-12-23 18:06:24.777743: Epoch 588\n",
      "2024-12-23 18:06:24.777822: Current learning rate: 0.0045\n",
      "2024-12-23 18:08:15.820838: train_loss -0.7724\n",
      "2024-12-23 18:08:15.820989: val_loss -0.7782\n",
      "2024-12-23 18:08:15.821023: Pseudo dice [np.float32(0.8215)]\n",
      "2024-12-23 18:08:15.821060: Epoch time: 111.04 s\n",
      "2024-12-23 18:08:16.407316: \n",
      "2024-12-23 18:08:16.407664: Epoch 589\n",
      "2024-12-23 18:08:16.407870: Current learning rate: 0.00449\n",
      "2024-12-23 18:10:07.528009: train_loss -0.8196\n",
      "2024-12-23 18:10:07.528445: val_loss -0.7427\n",
      "2024-12-23 18:10:07.528492: Pseudo dice [np.float32(0.7982)]\n",
      "2024-12-23 18:10:07.528530: Epoch time: 111.12 s\n",
      "2024-12-23 18:10:08.113966: \n",
      "2024-12-23 18:10:08.114147: Epoch 590\n",
      "2024-12-23 18:10:08.114228: Current learning rate: 0.00448\n",
      "2024-12-23 18:11:59.124154: train_loss -0.7727\n",
      "2024-12-23 18:11:59.124371: val_loss -0.7135\n",
      "2024-12-23 18:11:59.124407: Pseudo dice [np.float32(0.7423)]\n",
      "2024-12-23 18:11:59.124445: Epoch time: 111.01 s\n",
      "2024-12-23 18:11:59.711793: \n",
      "2024-12-23 18:11:59.711886: Epoch 591\n",
      "2024-12-23 18:11:59.711956: Current learning rate: 0.00447\n",
      "2024-12-23 18:13:50.756904: train_loss -0.7814\n",
      "2024-12-23 18:13:50.757044: val_loss -0.6803\n",
      "2024-12-23 18:13:50.757078: Pseudo dice [np.float32(0.7255)]\n",
      "2024-12-23 18:13:50.757116: Epoch time: 111.05 s\n",
      "2024-12-23 18:13:51.816273: \n",
      "2024-12-23 18:13:51.816463: Epoch 592\n",
      "2024-12-23 18:13:51.816539: Current learning rate: 0.00446\n",
      "2024-12-23 18:15:42.857777: train_loss -0.8026\n",
      "2024-12-23 18:15:42.857971: val_loss -0.6349\n",
      "2024-12-23 18:15:42.858094: Pseudo dice [np.float32(0.706)]\n",
      "2024-12-23 18:15:42.858139: Epoch time: 111.04 s\n",
      "2024-12-23 18:15:43.450322: \n",
      "2024-12-23 18:15:43.450683: Epoch 593\n",
      "2024-12-23 18:15:43.450836: Current learning rate: 0.00445\n",
      "2024-12-23 18:17:34.585335: train_loss -0.8069\n",
      "2024-12-23 18:17:34.585482: val_loss -0.7586\n",
      "2024-12-23 18:17:34.585516: Pseudo dice [np.float32(0.8001)]\n",
      "2024-12-23 18:17:34.585551: Epoch time: 111.14 s\n",
      "2024-12-23 18:17:35.167968: \n",
      "2024-12-23 18:17:35.168332: Epoch 594\n",
      "2024-12-23 18:17:35.168417: Current learning rate: 0.00444\n",
      "2024-12-23 18:19:26.174078: train_loss -0.8174\n",
      "2024-12-23 18:19:26.174225: val_loss -0.7112\n",
      "2024-12-23 18:19:26.174260: Pseudo dice [np.float32(0.7787)]\n",
      "2024-12-23 18:19:26.174305: Epoch time: 111.01 s\n",
      "2024-12-23 18:19:26.765265: \n",
      "2024-12-23 18:19:26.765702: Epoch 595\n",
      "2024-12-23 18:19:26.765802: Current learning rate: 0.00443\n",
      "2024-12-23 18:21:17.995835: train_loss -0.7944\n",
      "2024-12-23 18:21:17.996233: val_loss -0.6975\n",
      "2024-12-23 18:21:17.996365: Pseudo dice [np.float32(0.7331)]\n",
      "2024-12-23 18:21:17.996412: Epoch time: 111.23 s\n",
      "2024-12-23 18:21:18.583434: \n",
      "2024-12-23 18:21:18.583755: Epoch 596\n",
      "2024-12-23 18:21:18.583844: Current learning rate: 0.00442\n",
      "2024-12-23 18:23:09.765596: train_loss -0.7964\n",
      "2024-12-23 18:23:09.765743: val_loss -0.7653\n",
      "2024-12-23 18:23:09.765777: Pseudo dice [np.float32(0.8099)]\n",
      "2024-12-23 18:23:09.765814: Epoch time: 111.18 s\n",
      "2024-12-23 18:23:10.351431: \n",
      "2024-12-23 18:23:10.351871: Epoch 597\n",
      "2024-12-23 18:23:10.351979: Current learning rate: 0.00441\n",
      "2024-12-23 18:25:01.308482: train_loss -0.8055\n",
      "2024-12-23 18:25:01.308917: val_loss -0.7643\n",
      "2024-12-23 18:25:01.308992: Pseudo dice [np.float32(0.7975)]\n",
      "2024-12-23 18:25:01.309038: Epoch time: 110.96 s\n",
      "2024-12-23 18:25:01.896877: \n",
      "2024-12-23 18:25:01.897077: Epoch 598\n",
      "2024-12-23 18:25:01.897153: Current learning rate: 0.0044\n",
      "2024-12-23 18:26:52.866668: train_loss -0.7983\n",
      "2024-12-23 18:26:52.866810: val_loss -0.7071\n",
      "2024-12-23 18:26:52.866842: Pseudo dice [np.float32(0.7476)]\n",
      "2024-12-23 18:26:52.866877: Epoch time: 110.97 s\n",
      "2024-12-23 18:26:53.455746: \n",
      "2024-12-23 18:26:53.455859: Epoch 599\n",
      "2024-12-23 18:26:53.455938: Current learning rate: 0.00439\n",
      "2024-12-23 18:28:44.428710: train_loss -0.7929\n",
      "2024-12-23 18:28:44.428850: val_loss -0.7294\n",
      "2024-12-23 18:28:44.428883: Pseudo dice [np.float32(0.7869)]\n",
      "2024-12-23 18:28:44.428918: Epoch time: 110.97 s\n",
      "2024-12-23 18:28:45.283891: \n",
      "2024-12-23 18:28:45.284156: Epoch 600\n",
      "2024-12-23 18:28:45.284244: Current learning rate: 0.00438\n",
      "2024-12-23 18:30:36.416815: train_loss -0.7824\n",
      "2024-12-23 18:30:36.416967: val_loss -0.7015\n",
      "2024-12-23 18:30:36.417002: Pseudo dice [np.float32(0.7623)]\n",
      "2024-12-23 18:30:36.417039: Epoch time: 111.13 s\n",
      "2024-12-23 18:30:37.012831: \n",
      "2024-12-23 18:30:37.013285: Epoch 601\n",
      "2024-12-23 18:30:37.013471: Current learning rate: 0.00437\n",
      "2024-12-23 18:32:28.086320: train_loss -0.7861\n",
      "2024-12-23 18:32:28.086459: val_loss -0.7168\n",
      "2024-12-23 18:32:28.086492: Pseudo dice [np.float32(0.7935)]\n",
      "2024-12-23 18:32:28.086528: Epoch time: 111.07 s\n",
      "2024-12-23 18:32:28.674489: \n",
      "2024-12-23 18:32:28.674595: Epoch 602\n",
      "2024-12-23 18:32:28.674669: Current learning rate: 0.00436\n",
      "2024-12-23 18:34:19.670826: train_loss -0.7771\n",
      "2024-12-23 18:34:19.670985: val_loss -0.7436\n",
      "2024-12-23 18:34:19.671021: Pseudo dice [np.float32(0.8054)]\n",
      "2024-12-23 18:34:19.671055: Epoch time: 111.0 s\n",
      "2024-12-23 18:34:20.260561: \n",
      "2024-12-23 18:34:20.261045: Epoch 603\n",
      "2024-12-23 18:34:20.261233: Current learning rate: 0.00435\n",
      "2024-12-23 18:36:11.226912: train_loss -0.7939\n",
      "2024-12-23 18:36:11.227074: val_loss -0.7655\n",
      "2024-12-23 18:36:11.227111: Pseudo dice [np.float32(0.8124)]\n",
      "2024-12-23 18:36:11.227150: Epoch time: 110.97 s\n",
      "2024-12-23 18:36:11.810981: \n",
      "2024-12-23 18:36:11.811343: Epoch 604\n",
      "2024-12-23 18:36:11.811433: Current learning rate: 0.00434\n",
      "2024-12-23 18:38:02.850843: train_loss -0.8037\n",
      "2024-12-23 18:38:02.850987: val_loss -0.7905\n",
      "2024-12-23 18:38:02.851024: Pseudo dice [np.float32(0.8432)]\n",
      "2024-12-23 18:38:02.851061: Epoch time: 111.04 s\n",
      "2024-12-23 18:38:03.441185: \n",
      "Epoch 6053 18:38:03.441289: \n",
      "2024-12-23 18:38:03.441468: Current learning rate: 0.00433\n",
      "2024-12-23 18:39:54.439570: train_loss -0.7823\n",
      "2024-12-23 18:39:54.439816: val_loss -0.7565\n",
      "2024-12-23 18:39:54.440044: Pseudo dice [np.float32(0.7922)]\n",
      "2024-12-23 18:39:54.440128: Epoch time: 111.0 s\n",
      "2024-12-23 18:39:55.021907: \n",
      "2024-12-23 18:39:55.022202: Epoch 606\n",
      "2024-12-23 18:39:55.022304: Current learning rate: 0.00432\n",
      "2024-12-23 18:41:46.144373: train_loss -0.7907\n",
      "2024-12-23 18:41:46.144564: val_loss -0.745\n",
      "2024-12-23 18:41:46.144601: Pseudo dice [np.float32(0.7975)]\n",
      "2024-12-23 18:41:46.144637: Epoch time: 111.12 s\n",
      "2024-12-23 18:41:46.727852: \n",
      "2024-12-23 18:41:46.728174: Epoch 607\n",
      "2024-12-23 18:41:46.728306: Current learning rate: 0.00431\n",
      "2024-12-23 18:43:37.717093: train_loss -0.7992\n",
      "2024-12-23 18:43:37.717231: val_loss -0.6605\n",
      "2024-12-23 18:43:37.717272: Pseudo dice [np.float32(0.7751)]\n",
      "2024-12-23 18:43:37.717313: Epoch time: 110.99 s\n",
      "2024-12-23 18:43:38.303454: \n",
      "2024-12-23 18:43:38.303553: Epoch 608\n",
      "2024-12-23 18:43:38.303622: Current learning rate: 0.0043\n",
      "2024-12-23 18:45:29.265810: train_loss -0.8226\n",
      "2024-12-23 18:45:29.265953: val_loss -0.6499\n",
      "2024-12-23 18:45:29.265987: Pseudo dice [np.float32(0.7503)]\n",
      "2024-12-23 18:45:29.266025: Epoch time: 110.96 s\n",
      "2024-12-23 18:45:29.851189: \n",
      "2024-12-23 18:45:29.851555: Epoch 609\n",
      "2024-12-23 18:45:29.851818: Current learning rate: 0.00429\n",
      "2024-12-23 18:47:20.865750: train_loss -0.8187\n",
      "2024-12-23 18:47:20.865896: val_loss -0.7411\n",
      "2024-12-23 18:47:20.865932: Pseudo dice [np.float32(0.7727)]\n",
      "2024-12-23 18:47:20.866069: Epoch time: 111.02 s\n",
      "2024-12-23 18:47:21.919446: \n",
      "2024-12-23 18:47:21.919631: Epoch 610\n",
      "2024-12-23 18:47:21.919709: Current learning rate: 0.00429\n",
      "2024-12-23 18:49:12.972810: train_loss -0.8012\n",
      "2024-12-23 18:49:12.973108: val_loss -0.7421\n",
      "2024-12-23 18:49:12.973164: Pseudo dice [np.float32(0.7689)]\n",
      "2024-12-23 18:49:12.973204: Epoch time: 111.05 s\n",
      "2024-12-23 18:49:13.554852: \n",
      "2024-12-23 18:49:13.554983: Epoch 611\n",
      "2024-12-23 18:49:13.555056: Current learning rate: 0.00428\n",
      "2024-12-23 18:51:04.570610: train_loss -0.8022\n",
      "2024-12-23 18:51:04.570778: val_loss -0.6418\n",
      "2024-12-23 18:51:04.570942: Pseudo dice [np.float32(0.6818)]\n",
      "2024-12-23 18:51:04.571025: Epoch time: 111.02 s\n",
      "2024-12-23 18:51:05.165314: \n",
      "2024-12-23 18:51:05.165562: Epoch 612\n",
      "2024-12-23 18:51:05.165676: Current learning rate: 0.00427\n",
      "2024-12-23 18:52:56.370685: train_loss -0.7998\n",
      "2024-12-23 18:52:56.370832: val_loss -0.7193\n",
      "2024-12-23 18:52:56.370865: Pseudo dice [np.float32(0.7578)]\n",
      "2024-12-23 18:52:56.370902: Epoch time: 111.21 s\n",
      "2024-12-23 18:52:56.964772: \n",
      "2024-12-23 18:52:56.965066: Epoch 613\n",
      "2024-12-23 18:52:56.965271: Current learning rate: 0.00426\n",
      "2024-12-23 18:54:47.960440: train_loss -0.7963\n",
      "2024-12-23 18:54:47.960684: val_loss -0.7342\n",
      "2024-12-23 18:54:47.960722: Pseudo dice [np.float32(0.795)]\n",
      "2024-12-23 18:54:47.960759: Epoch time: 111.0 s\n",
      "2024-12-23 18:54:48.547633: \n",
      "2024-12-23 18:54:48.548102: Epoch 614\n",
      "2024-12-23 18:54:48.548200: Current learning rate: 0.00425\n",
      "2024-12-23 18:56:39.558414: train_loss -0.7963\n",
      "2024-12-23 18:56:39.558558: val_loss -0.6622\n",
      "2024-12-23 18:56:39.558594: Pseudo dice [np.float32(0.7344)]\n",
      "2024-12-23 18:56:39.558631: Epoch time: 111.01 s\n",
      "2024-12-23 18:56:40.151471: \n",
      "2024-12-23 18:56:40.151580: Epoch 615\n",
      "2024-12-23 18:56:40.151652: Current learning rate: 0.00424\n",
      "2024-12-23 18:58:31.346822: train_loss -0.7651\n",
      "2024-12-23 18:58:31.347087: val_loss -0.6664\n",
      "2024-12-23 18:58:31.347128: Pseudo dice [np.float32(0.742)]\n",
      "2024-12-23 18:58:31.347164: Epoch time: 111.2 s\n",
      "2024-12-23 18:58:31.944192: \n",
      "2024-12-23 18:58:31.944569: Epoch 616\n",
      "2024-12-23 18:58:31.944705: Current learning rate: 0.00423\n",
      "2024-12-23 19:00:22.953298: train_loss -0.7705\n",
      "2024-12-23 19:00:22.953443: val_loss -0.7637\n",
      "2024-12-23 19:00:22.953475: Pseudo dice [np.float32(0.8041)]\n",
      "2024-12-23 19:00:22.953513: Epoch time: 111.01 s\n",
      "2024-12-23 19:00:23.548177: \n",
      "2024-12-23 19:00:23.548279: Epoch 617\n",
      "2024-12-23 19:00:23.548350: Current learning rate: 0.00422\n",
      "2024-12-23 19:02:14.836298: train_loss -0.8208\n",
      "2024-12-23 19:02:14.836524: val_loss -0.6675\n",
      "2024-12-23 19:02:14.836573: Pseudo dice [np.float32(0.7005)]\n",
      "2024-12-23 19:02:14.836611: Epoch time: 111.29 s\n",
      "2024-12-23 19:02:15.425313: \n",
      "2024-12-23 19:02:15.425673: Epoch 618\n",
      "2024-12-23 19:02:15.425764: Current learning rate: 0.00421\n",
      "2024-12-23 19:04:06.606915: train_loss -0.831\n",
      "2024-12-23 19:04:06.607136: val_loss -0.6883\n",
      "2024-12-23 19:04:06.607186: Pseudo dice [np.float32(0.7422)]\n",
      "2024-12-23 19:04:06.607231: Epoch time: 111.18 s\n",
      "2024-12-23 19:04:07.204079: \n",
      "2024-12-23 19:04:07.204188: Epoch 619\n",
      "2024-12-23 19:04:07.204260: Current learning rate: 0.0042\n",
      "2024-12-23 19:05:58.206907: train_loss -0.7891\n",
      "2024-12-23 19:05:58.207118: val_loss -0.7098\n",
      "2024-12-23 19:05:58.207165: Pseudo dice [np.float32(0.7468)]\n",
      "2024-12-23 19:05:58.207207: Epoch time: 111.0 s\n",
      "2024-12-23 19:05:58.795280: \n",
      "2024-12-23 19:05:58.795386: Epoch 620\n",
      "2024-12-23 19:05:58.795457: Current learning rate: 0.00419\n",
      "2024-12-23 19:07:49.826947: train_loss -0.7788\n",
      "2024-12-23 19:07:49.827133: val_loss -0.6982\n",
      "2024-12-23 19:07:49.827257: Pseudo dice [np.float32(0.7484)]\n",
      "2024-12-23 19:07:49.827335: Epoch time: 111.03 s\n",
      "2024-12-23 19:07:50.425134: \n",
      "2024-12-23 19:07:50.425246: Epoch 621\n",
      "2024-12-23 19:07:50.425318: Current learning rate: 0.00418\n",
      "2024-12-23 19:09:41.436542: train_loss -0.8012\n",
      "2024-12-23 19:09:41.436926: val_loss -0.7265\n",
      "2024-12-23 19:09:41.437078: Pseudo dice [np.float32(0.7753)]\n",
      "2024-12-23 19:09:41.437151: Epoch time: 111.01 s\n",
      "2024-12-23 19:09:42.025713: \n",
      "2024-12-23 19:09:42.025818: Epoch 622\n",
      "2024-12-23 19:09:42.025888: Current learning rate: 0.00417\n",
      "2024-12-23 19:11:33.020439: train_loss -0.7958\n",
      "2024-12-23 19:11:33.020681: val_loss -0.7005\n",
      "2024-12-23 19:11:33.020724: Pseudo dice [np.float32(0.7419)]\n",
      "2024-12-23 19:11:33.020760: Epoch time: 111.0 s\n",
      "2024-12-23 19:11:33.617734: \n",
      "2024-12-23 19:11:33.617841: Epoch 623\n",
      "2024-12-23 19:11:33.617917: Current learning rate: 0.00416\n",
      "2024-12-23 19:13:24.774819: train_loss -0.8074\n",
      "2024-12-23 19:13:24.774982: val_loss -0.6661\n",
      "2024-12-23 19:13:24.775017: Pseudo dice [np.float32(0.6892)]\n",
      "2024-12-23 19:13:24.775056: Epoch time: 111.16 s\n",
      "2024-12-23 19:13:25.357265: \n",
      "2024-12-23 19:13:25.357493: Epoch 624\n",
      "2024-12-23 19:13:25.357646: Current learning rate: 0.00415\n",
      "2024-12-23 19:15:16.385920: train_loss -0.8065\n",
      "2024-12-23 19:15:16.386070: val_loss -0.742\n",
      "2024-12-23 19:15:16.386103: Pseudo dice [np.float32(0.769)]\n",
      "2024-12-23 19:15:16.386139: Epoch time: 111.03 s\n",
      "2024-12-23 19:15:16.955401: \n",
      "2024-12-23 19:15:16.955595: Epoch 625\n",
      "2024-12-23 19:15:16.955677: Current learning rate: 0.00414\n",
      "2024-12-23 19:17:07.968671: train_loss -0.7836\n",
      "2024-12-23 19:17:07.968807: val_loss -0.6738\n",
      "2024-12-23 19:17:07.968843: Pseudo dice [np.float32(0.7572)]\n",
      "2024-12-23 19:17:07.968878: Epoch time: 111.01 s\n",
      "2024-12-23 19:17:08.561382: \n",
      "2024-12-23 19:17:08.561488: Epoch 626\n",
      "2024-12-23 19:17:08.561564: Current learning rate: 0.00413\n",
      "2024-12-23 19:18:59.749151: train_loss -0.8204\n",
      "2024-12-23 19:18:59.749291: val_loss -0.7486\n",
      "2024-12-23 19:18:59.749326: Pseudo dice [np.float32(0.7991)]\n",
      "2024-12-23 19:18:59.749362: Epoch time: 111.19 s\n",
      "2024-12-23 19:19:00.756085: \n",
      "2024-12-23 19:19:00.756205: Epoch 627\n",
      "2024-12-23 19:19:00.756283: Current learning rate: 0.00412\n",
      "2024-12-23 19:20:51.899488: train_loss -0.7891\n",
      "2024-12-23 19:20:51.899793: val_loss -0.672\n",
      "2024-12-23 19:20:51.899931: Pseudo dice [np.float32(0.7965)]\n",
      "2024-12-23 19:20:51.899999: Epoch time: 111.14 s\n",
      "2024-12-23 19:20:52.493424: \n",
      "2024-12-23 19:20:52.493761: Epoch 628\n",
      "2024-12-23 19:20:52.493858: Current learning rate: 0.00411\n",
      "2024-12-23 19:22:43.524226: train_loss -0.7721\n",
      "2024-12-23 19:22:43.524403: val_loss -0.7185\n",
      "2024-12-23 19:22:43.524442: Pseudo dice [np.float32(0.7895)]\n",
      "2024-12-23 19:22:43.524479: Epoch time: 111.03 s\n",
      "2024-12-23 19:22:44.112827: \n",
      "2024-12-23 19:22:44.113207: Epoch 629\n",
      "2024-12-23 19:22:44.113384: Current learning rate: 0.0041\n",
      "2024-12-23 19:24:35.276368: train_loss -0.8164\n",
      "2024-12-23 19:24:35.276522: val_loss -0.6894\n",
      "2024-12-23 19:24:35.276558: Pseudo dice [np.float32(0.739)]\n",
      "2024-12-23 19:24:35.276596: Epoch time: 111.16 s\n",
      "2024-12-23 19:24:35.942842: \n",
      "Epoch 6303 19:24:35.943305: \n",
      "2024-12-23 19:24:35.943458: Current learning rate: 0.00409\n",
      "2024-12-23 19:26:27.023228: train_loss -0.7965\n",
      "2024-12-23 19:26:27.023391: val_loss -0.6562\n",
      "2024-12-23 19:26:27.023427: Pseudo dice [np.float32(0.7488)]\n",
      "2024-12-23 19:26:27.023463: Epoch time: 111.08 s\n",
      "2024-12-23 19:26:27.612239: \n",
      "2024-12-23 19:26:27.612458: Epoch 631\n",
      "2024-12-23 19:26:27.612534: Current learning rate: 0.00408\n",
      "2024-12-23 19:28:18.846209: train_loss -0.824\n",
      "2024-12-23 19:28:18.846355: val_loss -0.6519\n",
      "2024-12-23 19:28:18.846391: Pseudo dice [np.float32(0.7371)]\n",
      "2024-12-23 19:28:18.846439: Epoch time: 111.23 s\n",
      "2024-12-23 19:28:19.439522: \n",
      "2024-12-23 19:28:19.439646: Epoch 632\n",
      "2024-12-23 19:28:19.439721: Current learning rate: 0.00407\n",
      "2024-12-23 19:30:10.643808: train_loss -0.8228\n",
      "2024-12-23 19:30:10.643962: val_loss -0.7008\n",
      "2024-12-23 19:30:10.643998: Pseudo dice [np.float32(0.7591)]\n",
      "2024-12-23 19:30:10.644035: Epoch time: 111.2 s\n",
      "2024-12-23 19:30:11.234850: \n",
      "2024-12-23 19:30:11.234959: Epoch 633\n",
      "2024-12-23 19:30:11.235034: Current learning rate: 0.00406\n",
      "2024-12-23 19:32:02.305921: train_loss -0.7973\n",
      "2024-12-23 19:32:02.306122: val_loss -0.6922\n",
      "2024-12-23 19:32:02.306156: Pseudo dice [np.float32(0.7415)]\n",
      "2024-12-23 19:32:02.306195: Epoch time: 111.07 s\n",
      "2024-12-23 19:32:02.899928: \n",
      "2024-12-23 19:32:02.900040: Epoch 634\n",
      "2024-12-23 19:32:02.900110: Current learning rate: 0.00405\n",
      "2024-12-23 19:33:54.059592: train_loss -0.7926\n",
      "2024-12-23 19:33:54.059749: val_loss -0.7082\n",
      "2024-12-23 19:33:54.059790: Pseudo dice [np.float32(0.7513)]\n",
      "2024-12-23 19:33:54.059828: Epoch time: 111.16 s\n",
      "2024-12-23 19:33:54.657021: \n",
      "2024-12-23 19:33:54.657466: Epoch 635\n",
      "2024-12-23 19:33:54.657726: Current learning rate: 0.00404\n",
      "2024-12-23 19:35:45.706641: train_loss -0.8083\n",
      "2024-12-23 19:35:45.706780: val_loss -0.6434\n",
      "2024-12-23 19:35:45.706815: Pseudo dice [np.float32(0.6947)]\n",
      "2024-12-23 19:35:45.706853: Epoch time: 111.05 s\n",
      "2024-12-23 19:35:46.301063: \n",
      "2024-12-23 19:35:46.301346: Epoch 636\n",
      "2024-12-23 19:35:46.301493: Current learning rate: 0.00403\n",
      "2024-12-23 19:37:37.506453: train_loss -0.7913\n",
      "2024-12-23 19:37:37.506595: val_loss -0.7423\n",
      "2024-12-23 19:37:37.506628: Pseudo dice [np.float32(0.7784)]\n",
      "2024-12-23 19:37:37.506662: Epoch time: 111.21 s\n",
      "2024-12-23 19:37:38.099474: \n",
      "2024-12-23 19:37:38.099580: Epoch 637\n",
      "2024-12-23 19:37:38.099660: Current learning rate: 0.00402\n",
      "2024-12-23 19:39:29.246519: train_loss -0.827\n",
      "2024-12-23 19:39:29.246732: val_loss -0.7422\n",
      "2024-12-23 19:39:29.246768: Pseudo dice [np.float32(0.798)]\n",
      "2024-12-23 19:39:29.246807: Epoch time: 111.15 s\n",
      "2024-12-23 19:39:29.837264: \n",
      "2024-12-23 19:39:29.837710: Epoch 638\n",
      "2024-12-23 19:39:29.837790: Current learning rate: 0.00401\n",
      "2024-12-23 19:41:21.068800: train_loss -0.8069\n",
      "2024-12-23 19:41:21.068945: val_loss -0.6882\n",
      "2024-12-23 19:41:21.068980: Pseudo dice [np.float32(0.7745)]\n",
      "2024-12-23 19:41:21.069016: Epoch time: 111.23 s\n",
      "2024-12-23 19:41:21.656131: \n",
      "2024-12-23 19:41:21.656304: Epoch 639\n",
      "2024-12-23 19:41:21.656378: Current learning rate: 0.004\n",
      "2024-12-23 19:43:12.935673: train_loss -0.8171\n",
      "2024-12-23 19:43:12.935814: val_loss -0.6744\n",
      "2024-12-23 19:43:12.935849: Pseudo dice [np.float32(0.7385)]\n",
      "2024-12-23 19:43:12.935887: Epoch time: 111.28 s\n",
      "2024-12-23 19:43:13.554404: \n",
      "2024-12-23 19:43:13.554580: Epoch 640\n",
      "2024-12-23 19:43:13.554666: Current learning rate: 0.00399\n",
      "2024-12-23 19:45:04.881674: train_loss -0.8374\n",
      "2024-12-23 19:45:04.881822: val_loss -0.7191\n",
      "2024-12-23 19:45:04.881858: Pseudo dice [np.float32(0.7631)]\n",
      "2024-12-23 19:45:04.881896: Epoch time: 111.33 s\n",
      "2024-12-23 19:45:05.476226: \n",
      "2024-12-23 19:45:05.476334: Epoch 641\n",
      "2024-12-23 19:45:05.476407: Current learning rate: 0.00398\n",
      "2024-12-23 19:46:56.694088: train_loss -0.833\n",
      "2024-12-23 19:46:56.694249: val_loss -0.6838\n",
      "2024-12-23 19:46:56.694289: Pseudo dice [np.float32(0.774)]\n",
      "2024-12-23 19:46:56.694326: Epoch time: 111.22 s\n",
      "2024-12-23 19:46:57.282907: \n",
      "2024-12-23 19:46:57.283066: Epoch 642\n",
      "2024-12-23 19:46:57.283314: Current learning rate: 0.00397\n",
      "2024-12-23 19:48:48.516590: train_loss -0.8174\n",
      "2024-12-23 19:48:48.516750: val_loss -0.719\n",
      "2024-12-23 19:48:48.516852: Pseudo dice [np.float32(0.7433)]\n",
      "2024-12-23 19:48:48.516935: Epoch time: 111.23 s\n",
      "2024-12-23 19:48:49.111262: \n",
      "2024-12-23 19:48:49.111621: Epoch 643\n",
      "2024-12-23 19:48:49.111709: Current learning rate: 0.00396\n",
      "2024-12-23 19:50:40.290012: train_loss -0.8395\n",
      "2024-12-23 19:50:40.290150: val_loss -0.7253\n",
      "2024-12-23 19:50:40.290183: Pseudo dice [np.float32(0.7958)]\n",
      "2024-12-23 19:50:40.290219: Epoch time: 111.18 s\n",
      "2024-12-23 19:50:40.883756: \n",
      "2024-12-23 19:50:40.883857: Epoch 644\n",
      "2024-12-23 19:50:40.883930: Current learning rate: 0.00395\n",
      "2024-12-23 19:52:32.099526: train_loss -0.8323\n",
      "2024-12-23 19:52:32.099674: val_loss -0.715\n",
      "2024-12-23 19:52:32.099709: Pseudo dice [np.float32(0.7917)]\n",
      "2024-12-23 19:52:32.099745: Epoch time: 111.22 s\n",
      "2024-12-23 19:52:33.128868: \n",
      "2024-12-23 19:52:33.128983: Epoch 645\n",
      "2024-12-23 19:52:33.129082: Current learning rate: 0.00394\n",
      "2024-12-23 19:54:24.189403: train_loss -0.8296\n",
      "2024-12-23 19:54:24.189641: val_loss -0.7501\n",
      "2024-12-23 19:54:24.189676: Pseudo dice [np.float32(0.8034)]\n",
      "2024-12-23 19:54:24.189714: Epoch time: 111.06 s\n",
      "2024-12-23 19:54:24.778190: \n",
      "2024-12-23 19:54:24.778601: Epoch 646\n",
      "2024-12-23 19:54:24.778705: Current learning rate: 0.00393\n",
      "2024-12-23 19:56:15.992029: train_loss -0.8213\n",
      "2024-12-23 19:56:15.992169: val_loss -0.7306\n",
      "2024-12-23 19:56:15.992203: Pseudo dice [np.float32(0.7503)]\n",
      "2024-12-23 19:56:15.992240: Epoch time: 111.21 s\n",
      "2024-12-23 19:56:16.588463: \n",
      "2024-12-23 19:56:16.588820: Epoch 647\n",
      "2024-12-23 19:56:16.588990: Current learning rate: 0.00392\n",
      "2024-12-23 19:58:07.806853: train_loss -0.7877\n",
      "2024-12-23 19:58:07.807045: val_loss -0.7054\n",
      "2024-12-23 19:58:07.807080: Pseudo dice [np.float32(0.7561)]\n",
      "2024-12-23 19:58:07.807116: Epoch time: 111.22 s\n",
      "2024-12-23 19:58:08.399226: \n",
      "2024-12-23 19:58:08.399352: Epoch 648\n",
      "2024-12-23 19:58:08.399428: Current learning rate: 0.00391\n",
      "2024-12-23 19:59:59.645912: train_loss -0.8292\n",
      "2024-12-23 19:59:59.646065: val_loss -0.6804\n",
      "2024-12-23 19:59:59.646101: Pseudo dice [np.float32(0.7656)]\n",
      "2024-12-23 19:59:59.646138: Epoch time: 111.25 s\n",
      "2024-12-23 20:00:00.244509: \n",
      "2024-12-23 20:00:00.244961: Epoch 649\n",
      "2024-12-23 20:00:00.245148: Current learning rate: 0.0039\n",
      "2024-12-23 20:01:51.497405: train_loss -0.7997\n",
      "2024-12-23 20:01:51.497607: val_loss -0.6722\n",
      "2024-12-23 20:01:51.497637: Pseudo dice [np.float32(0.7518)]\n",
      "2024-12-23 20:01:51.497674: Epoch time: 111.25 s\n",
      "2024-12-23 20:01:52.339236: \n",
      "2024-12-23 20:01:52.339487: Epoch 650\n",
      "2024-12-23 20:01:52.339564: Current learning rate: 0.00389\n",
      "2024-12-23 20:03:47.032499: train_loss -0.8094\n",
      "2024-12-23 20:03:47.032672: val_loss -0.6882\n",
      "2024-12-23 20:03:47.032711: Pseudo dice [np.float32(0.7856)]\n",
      "2024-12-23 20:03:47.032748: Epoch time: 114.69 s\n",
      "2024-12-23 20:03:47.625690: \n",
      "2024-12-23 20:03:47.625896: Epoch 651\n",
      "2024-12-23 20:03:47.625971: Current learning rate: 0.00388\n",
      "2024-12-23 20:05:40.023297: train_loss -0.7927\n",
      "2024-12-23 20:05:40.023477: val_loss -0.7019\n",
      "2024-12-23 20:05:40.023513: Pseudo dice [np.float32(0.767)]\n",
      "2024-12-23 20:05:40.023552: Epoch time: 112.4 s\n",
      "2024-12-23 20:05:40.623353: \n",
      "2024-12-23 20:05:40.623560: Epoch 652\n",
      "2024-12-23 20:05:40.623636: Current learning rate: 0.00387\n",
      "2024-12-23 20:07:32.588678: train_loss -0.7739\n",
      "2024-12-23 20:07:32.588853: val_loss -0.5215\n",
      "2024-12-23 20:07:32.588896: Pseudo dice [np.float32(0.6092)]\n",
      "2024-12-23 20:07:32.588936: Epoch time: 111.97 s\n",
      "2024-12-23 20:07:33.186283: \n",
      "2024-12-23 20:07:33.186399: Epoch 653\n",
      "2024-12-23 20:07:33.186471: Current learning rate: 0.00386\n",
      "2024-12-23 20:09:25.293065: train_loss -0.7924\n",
      "2024-12-23 20:09:25.293292: val_loss -0.6713\n",
      "2024-12-23 20:09:25.293500: Pseudo dice [np.float32(0.6889)]\n",
      "2024-12-23 20:09:25.293585: Epoch time: 112.11 s\n",
      "2024-12-23 20:09:25.879326: \n",
      "2024-12-23 20:09:25.879445: Epoch 654\n",
      "2024-12-23 20:09:25.879577: Current learning rate: 0.00385\n",
      "2024-12-23 20:11:17.293677: train_loss -0.7899\n",
      "2024-12-23 20:11:17.293909: val_loss -0.7516\n",
      "2024-12-23 20:11:17.293944: Pseudo dice [np.float32(0.8209)]\n",
      "2024-12-23 20:11:17.293992: Epoch time: 111.41 s\n",
      "2024-12-23 20:11:17.891432: \n",
      "Epoch 6553 20:11:17.891540: \n",
      "2024-12-23 20:11:17.891741: Current learning rate: 0.00384\n",
      "2024-12-23 20:13:09.499972: train_loss -0.8016\n",
      "2024-12-23 20:13:09.500259: val_loss -0.718\n",
      "2024-12-23 20:13:09.500305: Pseudo dice [np.float32(0.7856)]\n",
      "2024-12-23 20:13:09.500342: Epoch time: 111.61 s\n",
      "2024-12-23 20:13:10.096598: \n",
      "2024-12-23 20:13:10.096702: Epoch 656\n",
      "2024-12-23 20:13:10.096780: Current learning rate: 0.00383\n",
      "2024-12-23 20:15:01.618208: train_loss -0.8016\n",
      "2024-12-23 20:15:01.618439: val_loss -0.6742\n",
      "2024-12-23 20:15:01.618484: Pseudo dice [np.float32(0.7635)]\n",
      "2024-12-23 20:15:01.618523: Epoch time: 111.52 s\n",
      "2024-12-23 20:15:02.216737: \n",
      "2024-12-23 20:15:02.216848: Epoch 657\n",
      "2024-12-23 20:15:02.216922: Current learning rate: 0.00382\n",
      "2024-12-23 20:16:53.732844: train_loss -0.8235\n",
      "2024-12-23 20:16:53.732999: val_loss -0.7416\n",
      "2024-12-23 20:16:53.733036: Pseudo dice [np.float32(0.7927)]\n",
      "2024-12-23 20:16:53.733073: Epoch time: 111.52 s\n",
      "2024-12-23 20:16:54.324569: \n",
      "2024-12-23 20:16:54.324951: Epoch 658\n",
      "2024-12-23 20:16:54.325125: Current learning rate: 0.00381\n",
      "2024-12-23 20:18:45.793143: train_loss -0.8055\n",
      "2024-12-23 20:18:45.793293: val_loss -0.7164\n",
      "2024-12-23 20:18:45.793329: Pseudo dice [np.float32(0.7495)]\n",
      "2024-12-23 20:18:45.793365: Epoch time: 111.47 s\n",
      "2024-12-23 20:18:46.379915: \n",
      "2024-12-23 20:18:46.380301: Epoch 659\n",
      "2024-12-23 20:18:46.380428: Current learning rate: 0.0038\n",
      "2024-12-23 20:20:37.615084: train_loss -0.7923\n",
      "2024-12-23 20:20:37.615231: val_loss -0.697\n",
      "2024-12-23 20:20:37.615268: Pseudo dice [np.float32(0.7391)]\n",
      "2024-12-23 20:20:37.615308: Epoch time: 111.24 s\n",
      "2024-12-23 20:20:38.202854: \n",
      "2024-12-23 20:20:38.203001: Epoch 660\n",
      "2024-12-23 20:20:38.203077: Current learning rate: 0.00379\n",
      "2024-12-23 20:22:29.384174: train_loss -0.7992\n",
      "2024-12-23 20:22:29.384317: val_loss -0.7411\n",
      "2024-12-23 20:22:29.384353: Pseudo dice [np.float32(0.8015)]\n",
      "2024-12-23 20:22:29.384393: Epoch time: 111.18 s\n",
      "2024-12-23 20:22:29.982831: \n",
      "2024-12-23 20:22:29.983196: Epoch 661\n",
      "2024-12-23 20:22:29.983287: Current learning rate: 0.00378\n",
      "2024-12-23 20:24:21.199564: train_loss -0.8219\n",
      "2024-12-23 20:24:21.199711: val_loss -0.666\n",
      "2024-12-23 20:24:21.199746: Pseudo dice [np.float32(0.7263)]\n",
      "2024-12-23 20:24:21.199782: Epoch time: 111.22 s\n",
      "2024-12-23 20:24:21.788206: \n",
      "2024-12-23 20:24:21.788301: Epoch 662\n",
      "2024-12-23 20:24:21.788373: Current learning rate: 0.00377\n",
      "2024-12-23 20:26:12.978387: train_loss -0.7888\n",
      "2024-12-23 20:26:12.978551: val_loss -0.6967\n",
      "2024-12-23 20:26:12.978587: Pseudo dice [np.float32(0.7486)]\n",
      "2024-12-23 20:26:12.978624: Epoch time: 111.19 s\n",
      "2024-12-23 20:26:14.050248: \n",
      "2024-12-23 20:26:14.050705: Epoch 663\n",
      "2024-12-23 20:26:14.050812: Current learning rate: 0.00376\n",
      "2024-12-23 20:28:05.267520: train_loss -0.8163\n",
      "2024-12-23 20:28:05.267675: val_loss -0.7444\n",
      "2024-12-23 20:28:05.267709: Pseudo dice [np.float32(0.8129)]\n",
      "2024-12-23 20:28:05.267745: Epoch time: 111.22 s\n",
      "2024-12-23 20:28:05.854880: \n",
      "2024-12-23 20:28:05.855019: Epoch 664\n",
      "2024-12-23 20:28:05.855093: Current learning rate: 0.00375\n",
      "2024-12-23 20:29:57.018757: train_loss -0.8289\n",
      "2024-12-23 20:29:57.019232: val_loss -0.7523\n",
      "2024-12-23 20:29:57.019383: Pseudo dice [np.float32(0.8111)]\n",
      "2024-12-23 20:29:57.019430: Epoch time: 111.16 s\n",
      "2024-12-23 20:29:57.616051: \n",
      "2024-12-23 20:29:57.616289: Epoch 665\n",
      "2024-12-23 20:29:57.616440: Current learning rate: 0.00374\n",
      "2024-12-23 20:31:48.857265: train_loss -0.8183\n",
      "2024-12-23 20:31:48.857415: val_loss -0.7122\n",
      "2024-12-23 20:31:48.857475: Pseudo dice [np.float32(0.7761)]\n",
      "2024-12-23 20:31:48.857519: Epoch time: 111.24 s\n",
      "2024-12-23 20:31:49.456471: \n",
      "2024-12-23 20:31:49.456846: Epoch 666\n",
      "2024-12-23 20:31:49.456931: Current learning rate: 0.00373\n",
      "2024-12-23 20:33:40.682188: train_loss -0.8107\n",
      "2024-12-23 20:33:40.682623: val_loss -0.7351\n",
      "2024-12-23 20:33:40.682695: Pseudo dice [np.float32(0.8088)]\n",
      "2024-12-23 20:33:40.682739: Epoch time: 111.23 s\n",
      "2024-12-23 20:33:41.271192: \n",
      "2024-12-23 20:33:41.271319: Epoch 667\n",
      "2024-12-23 20:33:41.271392: Current learning rate: 0.00372\n",
      "2024-12-23 20:35:32.475447: train_loss -0.8245\n",
      "2024-12-23 20:35:32.475661: val_loss -0.7011\n",
      "2024-12-23 20:35:32.475696: Pseudo dice [np.float32(0.7826)]\n",
      "2024-12-23 20:35:32.475734: Epoch time: 111.2 s\n",
      "2024-12-23 20:35:33.077553: \n",
      "2024-12-23 20:35:33.077910: Epoch 668\n",
      "2024-12-23 20:35:33.078028: Current learning rate: 0.00371\n",
      "2024-12-23 20:37:24.295064: train_loss -0.8286\n",
      "2024-12-23 20:37:24.295208: val_loss -0.6988\n",
      "2024-12-23 20:37:24.295241: Pseudo dice [np.float32(0.7559)]\n",
      "2024-12-23 20:37:24.295285: Epoch time: 111.22 s\n",
      "2024-12-23 20:37:24.897758: \n",
      "2024-12-23 20:37:24.897875: Epoch 669\n",
      "2024-12-23 20:37:24.897947: Current learning rate: 0.0037\n",
      "2024-12-23 20:39:16.087480: train_loss -0.8061\n",
      "2024-12-23 20:39:16.087630: val_loss -0.7498\n",
      "2024-12-23 20:39:16.087663: Pseudo dice [np.float32(0.7783)]\n",
      "2024-12-23 20:39:16.087707: Epoch time: 111.19 s\n",
      "2024-12-23 20:39:16.696748: \n",
      "2024-12-23 20:39:16.697024: Epoch 670\n",
      "2024-12-23 20:39:16.697317: Current learning rate: 0.00369\n",
      "2024-12-23 20:41:07.907631: train_loss -0.8388\n",
      "2024-12-23 20:41:07.907854: val_loss -0.6771\n",
      "2024-12-23 20:41:07.907918: Pseudo dice [np.float32(0.7782)]\n",
      "2024-12-23 20:41:07.907963: Epoch time: 111.21 s\n",
      "2024-12-23 20:41:08.532731: \n",
      "2024-12-23 20:41:08.532854: Epoch 671\n",
      "2024-12-23 20:41:08.533164: Current learning rate: 0.00368\n",
      "2024-12-23 20:42:59.826703: train_loss -0.8422\n",
      "2024-12-23 20:42:59.826849: val_loss -0.7102\n",
      "2024-12-23 20:42:59.826885: Pseudo dice [np.float32(0.7855)]\n",
      "2024-12-23 20:42:59.826920: Epoch time: 111.29 s\n",
      "2024-12-23 20:43:00.416267: \n",
      "2024-12-23 20:43:00.416380: Epoch 672\n",
      "2024-12-23 20:43:00.416454: Current learning rate: 0.00367\n",
      "2024-12-23 20:44:51.683045: train_loss -0.8172\n",
      "2024-12-23 20:44:51.683177: val_loss -0.6977\n",
      "2024-12-23 20:44:51.683212: Pseudo dice [np.float32(0.8139)]\n",
      "2024-12-23 20:44:51.683247: Epoch time: 111.27 s\n",
      "2024-12-23 20:44:52.285080: \n",
      "2024-12-23 20:44:52.285284: Epoch 673\n",
      "2024-12-23 20:44:52.285362: Current learning rate: 0.00366\n",
      "2024-12-23 20:46:43.528208: train_loss -0.8048\n",
      "2024-12-23 20:46:43.528414: val_loss -0.7247\n",
      "2024-12-23 20:46:43.528508: Pseudo dice [np.float32(0.7472)]\n",
      "2024-12-23 20:46:43.528612: Epoch time: 111.24 s\n",
      "2024-12-23 20:46:44.125898: \n",
      "2024-12-23 20:46:44.126008: Epoch 674\n",
      "2024-12-23 20:46:44.126083: Current learning rate: 0.00365\n",
      "2024-12-23 20:48:35.337472: train_loss -0.8277\n",
      "2024-12-23 20:48:35.337682: val_loss -0.7447\n",
      "2024-12-23 20:48:35.337715: Pseudo dice [np.float32(0.794)]\n",
      "2024-12-23 20:48:35.337752: Epoch time: 111.21 s\n",
      "2024-12-23 20:48:35.925617: \n",
      "2024-12-23 20:48:35.925824: Epoch 675\n",
      "2024-12-23 20:48:35.925958: Current learning rate: 0.00364\n",
      "2024-12-23 20:50:27.147068: train_loss -0.8108\n",
      "2024-12-23 20:50:27.147270: val_loss -0.74\n",
      "2024-12-23 20:50:27.147303: Pseudo dice [np.float32(0.8033)]\n",
      "2024-12-23 20:50:27.147340: Epoch time: 111.22 s\n",
      "2024-12-23 20:50:27.742459: \n",
      "2024-12-23 20:50:27.742978: Epoch 676\n",
      "2024-12-23 20:50:27.743084: Current learning rate: 0.00363\n",
      "2024-12-23 20:52:19.015865: train_loss -0.8292\n",
      "2024-12-23 20:52:19.016375: val_loss -0.748\n",
      "2024-12-23 20:52:19.016450: Pseudo dice [np.float32(0.7969)]\n",
      "2024-12-23 20:52:19.016494: Epoch time: 111.27 s\n",
      "2024-12-23 20:52:19.617882: \n",
      "2024-12-23 20:52:19.617982: Epoch 677\n",
      "2024-12-23 20:52:19.618055: Current learning rate: 0.00362\n",
      "2024-12-23 20:54:10.867302: train_loss -0.8296\n",
      "2024-12-23 20:54:10.867517: val_loss -0.6995\n",
      "2024-12-23 20:54:10.867552: Pseudo dice [np.float32(0.7483)]\n",
      "2024-12-23 20:54:10.867589: Epoch time: 111.25 s\n",
      "2024-12-23 20:54:11.468292: \n",
      "2024-12-23 20:54:11.468389: Epoch 678\n",
      "2024-12-23 20:54:11.468462: Current learning rate: 0.00361\n",
      "2024-12-23 20:56:02.746251: train_loss -0.8366\n",
      "2024-12-23 20:56:02.746401: val_loss -0.7577\n",
      "2024-12-23 20:56:02.746433: Pseudo dice [np.float32(0.7784)]\n",
      "2024-12-23 20:56:02.746469: Epoch time: 111.28 s\n",
      "2024-12-23 20:56:03.346198: \n",
      "2024-12-23 20:56:03.346313: Epoch 679\n",
      "2024-12-23 20:56:03.346420: Current learning rate: 0.0036\n",
      "2024-12-23 20:57:54.566355: train_loss -0.8437\n",
      "2024-12-23 20:57:54.566518: val_loss -0.6953\n",
      "2024-12-23 20:57:54.566559: Pseudo dice [np.float32(0.7598)]\n",
      "2024-12-23 20:57:54.566598: Epoch time: 111.22 s\n",
      "2024-12-23 20:57:55.168556: \n",
      "Epoch 6803 20:57:55.168650: \n",
      "2024-12-23 20:57:55.168823: Current learning rate: 0.00359\n",
      "2024-12-23 20:59:46.792265: train_loss -0.8335\n",
      "2024-12-23 20:59:46.792412: val_loss -0.7579\n",
      "2024-12-23 20:59:46.792518: Pseudo dice [np.float32(0.8089)]\n",
      "2024-12-23 20:59:46.792624: Epoch time: 111.62 s\n",
      "2024-12-23 20:59:47.387747: \n",
      "2024-12-23 20:59:47.388173: Epoch 681\n",
      "2024-12-23 20:59:47.388324: Current learning rate: 0.00358\n",
      "2024-12-23 21:01:38.648282: train_loss -0.8247\n",
      "2024-12-23 21:01:38.648550: val_loss -0.6574\n",
      "2024-12-23 21:01:38.648594: Pseudo dice [np.float32(0.7255)]\n",
      "2024-12-23 21:01:38.648631: Epoch time: 111.26 s\n",
      "2024-12-23 21:01:39.244543: \n",
      "2024-12-23 21:01:39.244676: Epoch 682\n",
      "2024-12-23 21:01:39.244750: Current learning rate: 0.00357\n",
      "2024-12-23 21:03:30.532878: train_loss -0.8178\n",
      "2024-12-23 21:03:30.533034: val_loss -0.6497\n",
      "2024-12-23 21:03:30.533069: Pseudo dice [np.float32(0.7308)]\n",
      "2024-12-23 21:03:30.533104: Epoch time: 111.29 s\n",
      "2024-12-23 21:03:31.124774: \n",
      "2024-12-23 21:03:31.124922: Epoch 683\n",
      "2024-12-23 21:03:31.125050: Current learning rate: 0.00356\n",
      "2024-12-23 21:05:22.358553: train_loss -0.8246\n",
      "2024-12-23 21:05:22.358693: val_loss -0.7195\n",
      "2024-12-23 21:05:22.358725: Pseudo dice [np.float32(0.7505)]\n",
      "2024-12-23 21:05:22.358761: Epoch time: 111.23 s\n",
      "2024-12-23 21:05:22.963432: \n",
      "2024-12-23 21:05:22.963567: Epoch 684\n",
      "2024-12-23 21:05:22.963643: Current learning rate: 0.00355\n",
      "2024-12-23 21:07:14.202751: train_loss -0.8409\n",
      "2024-12-23 21:07:14.202935: val_loss -0.7567\n",
      "2024-12-23 21:07:14.203013: Pseudo dice [np.float32(0.8313)]\n",
      "2024-12-23 21:07:14.203059: Epoch time: 111.24 s\n",
      "2024-12-23 21:07:14.803671: \n",
      "2024-12-23 21:07:14.804096: Epoch 685\n",
      "2024-12-23 21:07:14.804208: Current learning rate: 0.00354\n",
      "2024-12-23 21:09:06.109944: train_loss -0.8181\n",
      "2024-12-23 21:09:06.110174: val_loss -0.7305\n",
      "2024-12-23 21:09:06.110217: Pseudo dice [np.float32(0.7857)]\n",
      "2024-12-23 21:09:06.110268: Epoch time: 111.31 s\n",
      "2024-12-23 21:09:06.708708: \n",
      "2024-12-23 21:09:06.708818: Epoch 686\n",
      "2024-12-23 21:09:06.708888: Current learning rate: 0.00353\n",
      "2024-12-23 21:10:57.913752: train_loss -0.8315\n",
      "2024-12-23 21:10:57.913914: val_loss -0.6975\n",
      "2024-12-23 21:10:57.913950: Pseudo dice [np.float32(0.745)]\n",
      "2024-12-23 21:10:57.914077: Epoch time: 111.21 s\n",
      "2024-12-23 21:10:58.523869: \n",
      "2024-12-23 21:10:58.524052: Epoch 687\n",
      "2024-12-23 21:10:58.524127: Current learning rate: 0.00352\n",
      "2024-12-23 21:12:49.773767: train_loss -0.8378\n",
      "2024-12-23 21:12:49.774128: val_loss -0.7552\n",
      "2024-12-23 21:12:49.774168: Pseudo dice [np.float32(0.8143)]\n",
      "2024-12-23 21:12:49.774241: Epoch time: 111.25 s\n",
      "2024-12-23 21:12:50.372875: \n",
      "2024-12-23 21:12:50.372987: Epoch 688\n",
      "2024-12-23 21:12:50.373057: Current learning rate: 0.00351\n",
      "2024-12-23 21:14:41.606178: train_loss -0.8365\n",
      "2024-12-23 21:14:41.606457: val_loss -0.6568\n",
      "2024-12-23 21:14:41.606675: Pseudo dice [np.float32(0.7127)]\n",
      "2024-12-23 21:14:41.606774: Epoch time: 111.23 s\n",
      "2024-12-23 21:14:42.211437: \n",
      "2024-12-23 21:14:42.211548: Epoch 689\n",
      "2024-12-23 21:14:42.211617: Current learning rate: 0.0035\n",
      "2024-12-23 21:16:33.385601: train_loss -0.8128\n",
      "2024-12-23 21:16:33.385751: val_loss -0.7111\n",
      "2024-12-23 21:16:33.385788: Pseudo dice [np.float32(0.7668)]\n",
      "2024-12-23 21:16:33.385826: Epoch time: 111.17 s\n",
      "2024-12-23 21:16:33.983462: \n",
      "2024-12-23 21:16:33.983575: Epoch 690\n",
      "2024-12-23 21:16:33.983645: Current learning rate: 0.00349\n",
      "2024-12-23 21:18:25.194348: train_loss -0.8469\n",
      "2024-12-23 21:18:25.194505: val_loss -0.7128\n",
      "2024-12-23 21:18:25.194540: Pseudo dice [np.float32(0.7763)]\n",
      "2024-12-23 21:18:25.194578: Epoch time: 111.21 s\n",
      "2024-12-23 21:18:25.799082: \n",
      "2024-12-23 21:18:25.799487: Epoch 691\n",
      "2024-12-23 21:18:25.799574: Current learning rate: 0.00348\n",
      "2024-12-23 21:20:17.010223: train_loss -0.8245\n",
      "2024-12-23 21:20:17.010556: val_loss -0.7297\n",
      "2024-12-23 21:20:17.010604: Pseudo dice [np.float32(0.7699)]\n",
      "2024-12-23 21:20:17.010643: Epoch time: 111.21 s\n",
      "2024-12-23 21:20:17.606436: \n",
      "2024-12-23 21:20:17.606863: Epoch 692\n",
      "2024-12-23 21:20:17.607097: Current learning rate: 0.00346\n",
      "2024-12-23 21:22:08.864693: train_loss -0.8165\n",
      "2024-12-23 21:22:08.864839: val_loss -0.7683\n",
      "2024-12-23 21:22:08.864873: Pseudo dice [np.float32(0.8363)]\n",
      "2024-12-23 21:22:08.864910: Epoch time: 111.26 s\n",
      "2024-12-23 21:22:09.460697: \n",
      "2024-12-23 21:22:09.460885: Epoch 693\n",
      "2024-12-23 21:22:09.460964: Current learning rate: 0.00345\n",
      "2024-12-23 21:24:00.689734: train_loss -0.8316\n",
      "2024-12-23 21:24:00.689937: val_loss -0.779\n",
      "2024-12-23 21:24:00.690014: Pseudo dice [np.float32(0.8132)]\n",
      "2024-12-23 21:24:00.690053: Epoch time: 111.23 s\n",
      "2024-12-23 21:24:01.291241: \n",
      "2024-12-23 21:24:01.291568: Epoch 694\n",
      "2024-12-23 21:24:01.291678: Current learning rate: 0.00344\n",
      "2024-12-23 21:25:52.534005: train_loss -0.8291\n",
      "2024-12-23 21:25:52.534155: val_loss -0.7057\n",
      "2024-12-23 21:25:52.534190: Pseudo dice [np.float32(0.7592)]\n",
      "2024-12-23 21:25:52.534225: Epoch time: 111.24 s\n",
      "2024-12-23 21:25:53.127211: \n",
      "2024-12-23 21:25:53.127639: Epoch 695\n",
      "2024-12-23 21:25:53.127718: Current learning rate: 0.00343\n",
      "2024-12-23 21:27:44.356324: train_loss -0.8398\n",
      "2024-12-23 21:27:44.356482: val_loss -0.7172\n",
      "2024-12-23 21:27:44.356514: Pseudo dice [np.float32(0.7691)]\n",
      "2024-12-23 21:27:44.356549: Epoch time: 111.23 s\n",
      "2024-12-23 21:27:44.953356: \n",
      "2024-12-23 21:27:44.953609: Epoch 696\n",
      "2024-12-23 21:27:44.953703: Current learning rate: 0.00342\n",
      "2024-12-23 21:29:36.180811: train_loss -0.8398\n",
      "2024-12-23 21:29:36.181009: val_loss -0.7489\n",
      "2024-12-23 21:29:36.181132: Pseudo dice [np.float32(0.7841)]\n",
      "2024-12-23 21:29:36.181183: Epoch time: 111.23 s\n",
      "2024-12-23 21:29:36.775982: \n",
      "2024-12-23 21:29:36.776309: Epoch 697\n",
      "2024-12-23 21:29:36.776398: Current learning rate: 0.00341\n",
      "2024-12-23 21:31:27.940370: train_loss -0.8166\n",
      "2024-12-23 21:31:27.940512: val_loss -0.7464\n",
      "2024-12-23 21:31:27.940546: Pseudo dice [np.float32(0.7999)]\n",
      "2024-12-23 21:31:27.940582: Epoch time: 111.16 s\n",
      "2024-12-23 21:31:28.978813: \n",
      "2024-12-23 21:31:28.978934: Epoch 698\n",
      "2024-12-23 21:31:28.979044: Current learning rate: 0.0034\n",
      "2024-12-23 21:33:20.304632: train_loss -0.844\n",
      "2024-12-23 21:33:20.304784: val_loss -0.7643\n",
      "2024-12-23 21:33:20.304820: Pseudo dice [np.float32(0.8216)]\n",
      "2024-12-23 21:33:20.304857: Epoch time: 111.33 s\n",
      "2024-12-23 21:33:20.892717: \n",
      "2024-12-23 21:33:20.893128: Epoch 699\n",
      "2024-12-23 21:33:20.893212: Current learning rate: 0.00339\n",
      "2024-12-23 21:35:12.122758: train_loss -0.8423\n",
      "2024-12-23 21:35:12.122915: val_loss -0.7538\n",
      "2024-12-23 21:35:12.122949: Pseudo dice [np.float32(0.8175)]\n",
      "2024-12-23 21:35:12.122983: Epoch time: 111.23 s\n",
      "2024-12-23 21:35:12.989732: \n",
      "2024-12-23 21:35:12.989982: Epoch 700\n",
      "2024-12-23 21:35:12.990061: Current learning rate: 0.00338\n",
      "2024-12-23 21:37:05.342494: train_loss -0.8522\n",
      "2024-12-23 21:37:05.342728: val_loss -0.7475\n",
      "2024-12-23 21:37:05.342834: Pseudo dice [np.float32(0.8206)]\n",
      "2024-12-23 21:37:05.342883: Epoch time: 112.35 s\n",
      "2024-12-23 21:37:05.969290: \n",
      "2024-12-23 21:37:05.969437: Epoch 701\n",
      "2024-12-23 21:37:05.969512: Current learning rate: 0.00337\n",
      "2024-12-23 21:38:59.869279: train_loss -0.8355\n",
      "2024-12-23 21:38:59.869631: val_loss -0.7481\n",
      "2024-12-23 21:38:59.869740: Pseudo dice [np.float32(0.7634)]\n",
      "2024-12-23 21:38:59.869786: Epoch time: 113.9 s\n",
      "2024-12-23 21:39:00.476899: \n",
      "2024-12-23 21:39:00.477026: Epoch 702\n",
      "2024-12-23 21:39:00.477102: Current learning rate: 0.00336\n",
      "2024-12-23 21:40:51.967256: train_loss -0.8418\n",
      "2024-12-23 21:40:51.967408: val_loss -0.7085\n",
      "2024-12-23 21:40:51.967441: Pseudo dice [np.float32(0.7538)]\n",
      "2024-12-23 21:40:51.967476: Epoch time: 111.49 s\n",
      "2024-12-23 21:40:52.567812: \n",
      "2024-12-23 21:40:52.568150: Epoch 703\n",
      "2024-12-23 21:40:52.568230: Current learning rate: 0.00335\n",
      "2024-12-23 21:42:44.208283: train_loss -0.7752\n",
      "2024-12-23 21:42:44.208436: val_loss -0.7319\n",
      "2024-12-23 21:42:44.208471: Pseudo dice [np.float32(0.7839)]\n",
      "2024-12-23 21:42:44.208508: Epoch time: 111.64 s\n",
      "2024-12-23 21:42:44.895540: \n",
      "2024-12-23 21:42:44.895845: Epoch 704\n",
      "2024-12-23 21:42:44.895948: Current learning rate: 0.00334\n",
      "2024-12-23 21:44:36.442038: train_loss -0.7897\n",
      "2024-12-23 21:44:36.442189: val_loss -0.74\n",
      "2024-12-23 21:44:36.442223: Pseudo dice [np.float32(0.8129)]\n",
      "2024-12-23 21:44:36.442259: Epoch time: 111.55 s\n",
      "2024-12-23 21:44:37.045628: \n",
      "Epoch 7053 21:44:37.045740: \n",
      "2024-12-23 21:44:37.045936: Current learning rate: 0.00333\n",
      "2024-12-23 21:46:28.571343: train_loss -0.783\n",
      "2024-12-23 21:46:28.571540: val_loss -0.6872\n",
      "2024-12-23 21:46:28.571908: Pseudo dice [np.float32(0.7239)]\n",
      "2024-12-23 21:46:28.572148: Epoch time: 111.53 s\n",
      "2024-12-23 21:46:29.175001: \n",
      "2024-12-23 21:46:29.175109: Epoch 706\n",
      "2024-12-23 21:46:29.175179: Current learning rate: 0.00332\n",
      "2024-12-23 21:48:20.719203: train_loss -0.8094\n",
      "2024-12-23 21:48:20.719346: val_loss -0.7398\n",
      "2024-12-23 21:48:20.719381: Pseudo dice [np.float32(0.7718)]\n",
      "2024-12-23 21:48:20.719418: Epoch time: 111.54 s\n",
      "2024-12-23 21:48:21.329025: \n",
      "2024-12-23 21:48:21.329388: Epoch 707\n",
      "2024-12-23 21:48:21.329467: Current learning rate: 0.00331\n",
      "2024-12-23 21:50:12.726651: train_loss -0.8197\n",
      "2024-12-23 21:50:12.726788: val_loss -0.7337\n",
      "2024-12-23 21:50:12.726821: Pseudo dice [np.float32(0.789)]\n",
      "2024-12-23 21:50:12.726858: Epoch time: 111.4 s\n",
      "2024-12-23 21:50:13.316941: \n",
      "2024-12-23 21:50:13.317307: Epoch 708\n",
      "2024-12-23 21:50:13.317385: Current learning rate: 0.0033\n",
      "2024-12-23 21:52:04.537153: train_loss -0.8424\n",
      "2024-12-23 21:52:04.537301: val_loss -0.7124\n",
      "2024-12-23 21:52:04.537337: Pseudo dice [np.float32(0.7753)]\n",
      "2024-12-23 21:52:04.537374: Epoch time: 111.22 s\n",
      "2024-12-23 21:52:05.146867: \n",
      "2024-12-23 21:52:05.146975: Epoch 709\n",
      "2024-12-23 21:52:05.147045: Current learning rate: 0.00329\n",
      "2024-12-23 21:53:56.392037: train_loss -0.7897\n",
      "2024-12-23 21:53:56.392182: val_loss -0.7107\n",
      "2024-12-23 21:53:56.392219: Pseudo dice [np.float32(0.8076)]\n",
      "2024-12-23 21:53:56.392257: Epoch time: 111.25 s\n",
      "2024-12-23 21:53:57.000242: \n",
      "2024-12-23 21:53:57.000592: Epoch 710\n",
      "2024-12-23 21:53:57.000708: Current learning rate: 0.00328\n",
      "2024-12-23 21:55:48.190415: train_loss -0.8222\n",
      "2024-12-23 21:55:48.190631: val_loss -0.7355\n",
      "2024-12-23 21:55:48.190769: Pseudo dice [np.float32(0.7945)]\n",
      "2024-12-23 21:55:48.190813: Epoch time: 111.19 s\n",
      "2024-12-23 21:55:48.784217: \n",
      "2024-12-23 21:55:48.784642: Epoch 711\n",
      "2024-12-23 21:55:48.784752: Current learning rate: 0.00327\n",
      "2024-12-23 21:57:39.975790: train_loss -0.8042\n",
      "2024-12-23 21:57:39.975931: val_loss -0.754\n",
      "2024-12-23 21:57:39.975966: Pseudo dice [np.float32(0.8026)]\n",
      "2024-12-23 21:57:39.976002: Epoch time: 111.19 s\n",
      "2024-12-23 21:57:40.591975: \n",
      "2024-12-23 21:57:40.592174: Epoch 712\n",
      "2024-12-23 21:57:40.592258: Current learning rate: 0.00326\n",
      "2024-12-23 21:59:31.783965: train_loss -0.8202\n",
      "2024-12-23 21:59:31.784106: val_loss -0.7043\n",
      "2024-12-23 21:59:31.784140: Pseudo dice [np.float32(0.7277)]\n",
      "2024-12-23 21:59:31.784177: Epoch time: 111.19 s\n",
      "2024-12-23 21:59:32.390317: \n",
      "2024-12-23 21:59:32.390410: Epoch 713\n",
      "2024-12-23 21:59:32.390482: Current learning rate: 0.00325\n",
      "2024-12-23 22:01:23.604295: train_loss -0.8172\n",
      "2024-12-23 22:01:23.604442: val_loss -0.7882\n",
      "2024-12-23 22:01:23.604477: Pseudo dice [np.float32(0.8304)]\n",
      "2024-12-23 22:01:23.604515: Epoch time: 111.21 s\n",
      "2024-12-23 22:01:24.198889: \n",
      "2024-12-23 22:01:24.198991: Epoch 714\n",
      "2024-12-23 22:01:24.199101: Current learning rate: 0.00324\n",
      "2024-12-23 22:03:15.439505: train_loss -0.8309\n",
      "2024-12-23 22:03:15.439698: val_loss -0.7784\n",
      "2024-12-23 22:03:15.439736: Pseudo dice [np.float32(0.8092)]\n",
      "2024-12-23 22:03:15.439774: Epoch time: 111.24 s\n",
      "2024-12-23 22:03:16.508425: \n",
      "2024-12-23 22:03:16.508551: Epoch 715\n",
      "2024-12-23 22:03:16.508637: Current learning rate: 0.00323\n",
      "2024-12-23 22:05:07.703580: train_loss -0.8156\n",
      "2024-12-23 22:05:07.703732: val_loss -0.7519\n",
      "2024-12-23 22:05:07.703772: Pseudo dice [np.float32(0.8074)]\n",
      "2024-12-23 22:05:07.703809: Epoch time: 111.2 s\n",
      "2024-12-23 22:05:08.317044: \n",
      "2024-12-23 22:05:08.317312: Epoch 716\n",
      "2024-12-23 22:05:08.317615: Current learning rate: 0.00322\n",
      "2024-12-23 22:06:59.521768: train_loss -0.8019\n",
      "2024-12-23 22:06:59.521914: val_loss -0.7853\n",
      "2024-12-23 22:06:59.521952: Pseudo dice [np.float32(0.8405)]\n",
      "2024-12-23 22:06:59.521989: Epoch time: 111.21 s\n",
      "2024-12-23 22:06:59.522010: Yayy! New best EMA pseudo Dice: 0.7947999835014343\n",
      "2024-12-23 22:07:00.381382: \n",
      "2024-12-23 22:07:00.381531: Epoch 717\n",
      "2024-12-23 22:07:00.381606: Current learning rate: 0.00321\n",
      "2024-12-23 22:08:51.607883: train_loss -0.791\n",
      "2024-12-23 22:08:51.608049: val_loss -0.7237\n",
      "2024-12-23 22:08:51.608092: Pseudo dice [np.float32(0.7658)]\n",
      "2024-12-23 22:08:51.608131: Epoch time: 111.23 s\n",
      "2024-12-23 22:08:52.214685: \n",
      "2024-12-23 22:08:52.215210: Epoch 718\n",
      "2024-12-23 22:08:52.215308: Current learning rate: 0.0032\n",
      "2024-12-23 22:10:43.460962: train_loss -0.8066\n",
      "2024-12-23 22:10:43.461159: val_loss -0.7888\n",
      "2024-12-23 22:10:43.461194: Pseudo dice [np.float32(0.8425)]\n",
      "2024-12-23 22:10:43.461231: Epoch time: 111.25 s\n",
      "2024-12-23 22:10:43.461256: Yayy! New best EMA pseudo Dice: 0.7968999743461609\n",
      "2024-12-23 22:10:44.315850: \n",
      "2024-12-23 22:10:44.316067: Epoch 719\n",
      "2024-12-23 22:10:44.316147: Current learning rate: 0.00319\n",
      "2024-12-23 22:12:35.488200: train_loss -0.808\n",
      "2024-12-23 22:12:35.488336: val_loss -0.7269\n",
      "2024-12-23 22:12:35.488371: Pseudo dice [np.float32(0.8018)]\n",
      "2024-12-23 22:12:35.488409: Epoch time: 111.17 s\n",
      "2024-12-23 22:12:35.488431: Yayy! New best EMA pseudo Dice: 0.7973999977111816\n",
      "2024-12-23 22:12:36.322991: \n",
      "2024-12-23 22:12:36.323199: Epoch 720\n",
      "2024-12-23 22:12:36.323276: Current learning rate: 0.00318\n",
      "2024-12-23 22:14:27.565880: train_loss -0.8142\n",
      "2024-12-23 22:14:27.566032: val_loss -0.7396\n",
      "2024-12-23 22:14:27.566068: Pseudo dice [np.float32(0.793)]\n",
      "2024-12-23 22:14:27.566106: Epoch time: 111.24 s\n",
      "2024-12-23 22:14:28.201943: \n",
      "2024-12-23 22:14:28.202089: Epoch 721\n",
      "2024-12-23 22:14:28.202191: Current learning rate: 0.00317\n",
      "2024-12-23 22:16:19.398426: train_loss -0.7989\n",
      "2024-12-23 22:16:19.398660: val_loss -0.7647\n",
      "2024-12-23 22:16:19.398696: Pseudo dice [np.float32(0.8047)]\n",
      "2024-12-23 22:16:19.398731: Epoch time: 111.2 s\n",
      "2024-12-23 22:16:19.398752: Yayy! New best EMA pseudo Dice: 0.7978000044822693\n",
      "2024-12-23 22:16:20.237885: \n",
      "2024-12-23 22:16:20.238245: Epoch 722\n",
      "2024-12-23 22:16:20.238324: Current learning rate: 0.00316\n",
      "2024-12-23 22:18:11.393402: train_loss -0.8199\n",
      "2024-12-23 22:18:11.393545: val_loss -0.7845\n",
      "2024-12-23 22:18:11.393580: Pseudo dice [np.float32(0.8007)]\n",
      "2024-12-23 22:18:11.393617: Epoch time: 111.16 s\n",
      "2024-12-23 22:18:11.393639: Yayy! New best EMA pseudo Dice: 0.7980999946594238\n",
      "2024-12-23 22:18:12.227166: \n",
      "2024-12-23 22:18:12.227281: Epoch 723\n",
      "2024-12-23 22:18:12.227354: Current learning rate: 0.00315\n",
      "2024-12-23 22:20:03.408962: train_loss -0.7769\n",
      "2024-12-23 22:20:03.409115: val_loss -0.8017\n",
      "2024-12-23 22:20:03.409148: Pseudo dice [np.float32(0.8346)]\n",
      "2024-12-23 22:20:03.409184: Epoch time: 111.18 s\n",
      "2024-12-23 22:20:03.409205: Yayy! New best EMA pseudo Dice: 0.8016999959945679\n",
      "2024-12-23 22:20:04.270348: \n",
      "2024-12-23 22:20:04.270684: Epoch 724\n",
      "2024-12-23 22:20:04.270766: Current learning rate: 0.00314\n",
      "2024-12-23 22:21:55.481690: train_loss -0.8024\n",
      "2024-12-23 22:21:55.481837: val_loss -0.7501\n",
      "2024-12-23 22:21:55.481871: Pseudo dice [np.float32(0.8007)]\n",
      "2024-12-23 22:21:55.481909: Epoch time: 111.21 s\n",
      "2024-12-23 22:21:56.086181: \n",
      "2024-12-23 22:21:56.086648: Epoch 725\n",
      "2024-12-23 22:21:56.086765: Current learning rate: 0.00313\n",
      "2024-12-23 22:23:47.290523: train_loss -0.8036\n",
      "2024-12-23 22:23:47.290727: val_loss -0.7016\n",
      "2024-12-23 22:23:47.290763: Pseudo dice [np.float32(0.7494)]\n",
      "2024-12-23 22:23:47.290799: Epoch time: 111.2 s\n",
      "2024-12-23 22:23:47.887158: \n",
      "2024-12-23 22:23:47.887259: Epoch 726\n",
      "2024-12-23 22:23:47.887330: Current learning rate: 0.00312\n",
      "2024-12-23 22:25:38.964572: train_loss -0.7779\n",
      "2024-12-23 22:25:38.964714: val_loss -0.7027\n",
      "2024-12-23 22:25:38.964749: Pseudo dice [np.float32(0.7357)]\n",
      "2024-12-23 22:25:38.964787: Epoch time: 111.08 s\n",
      "2024-12-23 22:25:39.560718: \n",
      "2024-12-23 22:25:39.560839: Epoch 727\n",
      "2024-12-23 22:25:39.560970: Current learning rate: 0.00311\n",
      "2024-12-23 22:27:30.806048: train_loss -0.8227\n",
      "2024-12-23 22:27:30.806208: val_loss -0.6931\n",
      "2024-12-23 22:27:30.806242: Pseudo dice [np.float32(0.7598)]\n",
      "2024-12-23 22:27:30.806278: Epoch time: 111.25 s\n",
      "2024-12-23 22:27:31.407752: \n",
      "2024-12-23 22:27:31.407859: Epoch 728\n",
      "2024-12-23 22:27:31.407932: Current learning rate: 0.0031\n",
      "2024-12-23 22:29:22.577845: train_loss -0.8184\n",
      "2024-12-23 22:29:22.577984: val_loss -0.6702\n",
      "2024-12-23 22:29:22.578026: Pseudo dice [np.float32(0.7049)]\n",
      "2024-12-23 22:29:22.578170: Epoch time: 111.17 s\n",
      "2024-12-23 22:29:23.188493: \n",
      "2024-12-23 22:29:23.188804: Epoch 729\n",
      "2024-12-23 22:29:23.188885: Current learning rate: 0.00309\n",
      "2024-12-23 22:31:14.414305: train_loss -0.832\n",
      "2024-12-23 22:31:14.414456: val_loss -0.7795\n",
      "2024-12-23 22:31:14.414491: Pseudo dice [np.float32(0.8346)]\n",
      "2024-12-23 22:31:14.414527: Epoch time: 111.23 s\n",
      "2024-12-23 22:31:15.079519: \n",
      "2024-12-23 22:31:15.079645: Epoch 730\n",
      "2024-12-23 22:31:15.079821: Current learning rate: 0.00308\n",
      "2024-12-23 22:33:06.315786: train_loss -0.7771\n",
      "2024-12-23 22:33:06.315915: val_loss -0.6956\n",
      "2024-12-23 22:33:06.315950: Pseudo dice [np.float32(0.7738)]\n",
      "2024-12-23 22:33:06.315985: Epoch time: 111.24 s\n",
      "2024-12-23 22:33:06.896371: \n",
      "2024-12-23 22:33:06.896571: Epoch 731\n",
      "2024-12-23 22:33:06.896653: Current learning rate: 0.00307\n",
      "2024-12-23 22:34:57.971314: train_loss -0.8092\n",
      "2024-12-23 22:34:57.971454: val_loss -0.7174\n",
      "2024-12-23 22:34:57.971622: Pseudo dice [np.float32(0.7641)]\n",
      "2024-12-23 22:34:57.971704: Epoch time: 111.08 s\n",
      "2024-12-23 22:34:58.958347: \n",
      "2024-12-23 22:34:58.958609: Epoch 732\n",
      "2024-12-23 22:34:58.958686: Current learning rate: 0.00306\n",
      "2024-12-23 22:36:49.988300: train_loss -0.8344\n",
      "2024-12-23 22:36:49.988477: val_loss -0.7036\n",
      "2024-12-23 22:36:49.988572: Pseudo dice [np.float32(0.7069)]\n",
      "2024-12-23 22:36:49.988634: Epoch time: 111.03 s\n",
      "2024-12-23 22:36:50.569656: \n",
      "2024-12-23 22:36:50.569973: Epoch 733\n",
      "2024-12-23 22:36:50.570113: Current learning rate: 0.00305\n",
      "2024-12-23 22:38:41.711213: train_loss -0.853\n",
      "2024-12-23 22:38:41.711391: val_loss -0.6451\n",
      "2024-12-23 22:38:41.711430: Pseudo dice [np.float32(0.7345)]\n",
      "2024-12-23 22:38:41.711467: Epoch time: 111.14 s\n",
      "2024-12-23 22:38:42.303127: \n",
      "2024-12-23 22:38:42.303376: Epoch 734\n",
      "2024-12-23 22:38:42.303452: Current learning rate: 0.00304\n",
      "2024-12-23 22:40:33.353248: train_loss -0.8316\n",
      "2024-12-23 22:40:33.353446: val_loss -0.7165\n",
      "2024-12-23 22:40:33.353490: Pseudo dice [np.float32(0.7782)]\n",
      "2024-12-23 22:40:33.353528: Epoch time: 111.05 s\n",
      "2024-12-23 22:40:33.942048: \n",
      "2024-12-23 22:40:33.942296: Epoch 735\n",
      "2024-12-23 22:40:33.942397: Current learning rate: 0.00303\n",
      "2024-12-23 22:42:24.996749: train_loss -0.8122\n",
      "2024-12-23 22:42:24.996961: val_loss -0.6317\n",
      "2024-12-23 22:42:24.997128: Pseudo dice [np.float32(0.6873)]\n",
      "2024-12-23 22:42:24.997211: Epoch time: 111.06 s\n",
      "2024-12-23 22:42:25.590350: \n",
      "2024-12-23 22:42:25.590457: Epoch 736\n",
      "2024-12-23 22:42:25.590528: Current learning rate: 0.00302\n",
      "2024-12-23 22:44:16.675194: train_loss -0.8313\n",
      "2024-12-23 22:44:16.675330: val_loss -0.6983\n",
      "2024-12-23 22:44:16.675371: Pseudo dice [np.float32(0.7905)]\n",
      "2024-12-23 22:44:16.675411: Epoch time: 111.09 s\n",
      "2024-12-23 22:44:17.263134: \n",
      "2024-12-23 22:44:17.263299: Epoch 737\n",
      "2024-12-23 22:44:17.263374: Current learning rate: 0.00301\n",
      "2024-12-23 22:46:08.359214: train_loss -0.824\n",
      "2024-12-23 22:46:08.359519: val_loss -0.7331\n",
      "2024-12-23 22:46:08.359585: Pseudo dice [np.float32(0.79)]\n",
      "2024-12-23 22:46:08.359630: Epoch time: 111.1 s\n",
      "2024-12-23 22:46:08.955561: \n",
      "2024-12-23 22:46:08.955787: Epoch 738\n",
      "2024-12-23 22:46:08.955869: Current learning rate: 0.003\n",
      "2024-12-23 22:47:59.999725: train_loss -0.8188\n",
      "2024-12-23 22:47:59.999942: val_loss -0.7046\n",
      "2024-12-23 22:47:59.999978: Pseudo dice [np.float32(0.7241)]\n",
      "2024-12-23 22:48:00.000013: Epoch time: 111.04 s\n",
      "2024-12-23 22:48:00.588243: \n",
      "2024-12-23 22:48:00.588399: Epoch 739\n",
      "2024-12-23 22:48:00.588474: Current learning rate: 0.00299\n",
      "2024-12-23 22:49:51.699820: train_loss -0.8002\n",
      "2024-12-23 22:49:51.699975: val_loss -0.6938\n",
      "2024-12-23 22:49:51.700015: Pseudo dice [np.float32(0.7448)]\n",
      "2024-12-23 22:49:51.700058: Epoch time: 111.11 s\n",
      "2024-12-23 22:49:52.285530: \n",
      "2024-12-23 22:49:52.285695: Epoch 740\n",
      "2024-12-23 22:49:52.285772: Current learning rate: 0.00297\n",
      "2024-12-23 22:51:43.345066: train_loss -0.8347\n",
      "2024-12-23 22:51:43.345307: val_loss -0.7731\n",
      "2024-12-23 22:51:43.345342: Pseudo dice [np.float32(0.8285)]\n",
      "2024-12-23 22:51:43.345382: Epoch time: 111.06 s\n",
      "2024-12-23 22:51:43.941440: \n",
      "2024-12-23 22:51:43.941602: Epoch 741\n",
      "2024-12-23 22:51:43.941677: Current learning rate: 0.00296\n",
      "2024-12-23 22:53:35.049089: train_loss -0.8297\n",
      "2024-12-23 22:53:35.049245: val_loss -0.6411\n",
      "2024-12-23 22:53:35.049283: Pseudo dice [np.float32(0.7215)]\n",
      "2024-12-23 22:53:35.049320: Epoch time: 111.11 s\n",
      "2024-12-23 22:53:35.640703: \n",
      "2024-12-23 22:53:35.640931: Epoch 742\n",
      "2024-12-23 22:53:35.641016: Current learning rate: 0.00295\n",
      "2024-12-23 22:55:26.758005: train_loss -0.8127\n",
      "2024-12-23 22:55:26.758133: val_loss -0.6431\n",
      "2024-12-23 22:55:26.758167: Pseudo dice [np.float32(0.732)]\n",
      "2024-12-23 22:55:26.758204: Epoch time: 111.12 s\n",
      "2024-12-23 22:55:27.343364: \n",
      "2024-12-23 22:55:27.343566: Epoch 743\n",
      "2024-12-23 22:55:27.343640: Current learning rate: 0.00294\n",
      "2024-12-23 22:57:18.442491: train_loss -0.8239\n",
      "2024-12-23 22:57:18.442712: val_loss -0.6746\n",
      "2024-12-23 22:57:18.442756: Pseudo dice [np.float32(0.7339)]\n",
      "2024-12-23 22:57:18.442792: Epoch time: 111.1 s\n",
      "2024-12-23 22:57:19.022839: \n",
      "2024-12-23 22:57:19.023012: Epoch 744\n",
      "2024-12-23 22:57:19.023089: Current learning rate: 0.00293\n",
      "2024-12-23 22:59:10.116098: train_loss -0.8266\n",
      "2024-12-23 22:59:10.116237: val_loss -0.7589\n",
      "2024-12-23 22:59:10.116271: Pseudo dice [np.float32(0.7964)]\n",
      "2024-12-23 22:59:10.116306: Epoch time: 111.09 s\n",
      "2024-12-23 22:59:10.701248: \n",
      "2024-12-23 22:59:10.701406: Epoch 745\n",
      "2024-12-23 22:59:10.701486: Current learning rate: 0.00292\n",
      "2024-12-23 23:01:01.822247: train_loss -0.8334\n",
      "2024-12-23 23:01:01.822383: val_loss -0.7769\n",
      "2024-12-23 23:01:01.822418: Pseudo dice [np.float32(0.811)]\n",
      "2024-12-23 23:01:01.822454: Epoch time: 111.12 s\n",
      "2024-12-23 23:01:02.405309: \n",
      "2024-12-23 23:01:02.405543: Epoch 746\n",
      "2024-12-23 23:01:02.405705: Current learning rate: 0.00291\n",
      "2024-12-23 23:02:53.482600: train_loss -0.8193\n",
      "2024-12-23 23:02:53.482750: val_loss -0.6211\n",
      "2024-12-23 23:02:53.482927: Pseudo dice [np.float32(0.6774)]\n",
      "2024-12-23 23:02:53.483000: Epoch time: 111.08 s\n",
      "2024-12-23 23:02:54.071860: \n",
      "2024-12-23 23:02:54.072008: Epoch 747\n",
      "2024-12-23 23:02:54.072092: Current learning rate: 0.0029\n",
      "2024-12-23 23:04:45.126019: train_loss -0.8242\n",
      "2024-12-23 23:04:45.126222: val_loss -0.6162\n",
      "2024-12-23 23:04:45.126279: Pseudo dice [np.float32(0.7456)]\n",
      "2024-12-23 23:04:45.126319: Epoch time: 111.05 s\n",
      "2024-12-23 23:04:45.715868: \n",
      "2024-12-23 23:04:45.716148: Epoch 748\n",
      "2024-12-23 23:04:45.716284: Current learning rate: 0.00289\n",
      "2024-12-23 23:06:36.756987: train_loss -0.8295\n",
      "2024-12-23 23:06:36.757322: val_loss -0.7393\n",
      "2024-12-23 23:06:36.757370: Pseudo dice [np.float32(0.818)]\n",
      "2024-12-23 23:06:36.757407: Epoch time: 111.04 s\n",
      "2024-12-23 23:06:37.766052: \n",
      "2024-12-23 23:06:37.766511: Epoch 749\n",
      "2024-12-23 23:06:37.766690: Current learning rate: 0.00288\n",
      "2024-12-23 23:08:28.836420: train_loss -0.8139\n",
      "2024-12-23 23:08:28.836643: val_loss -0.7673\n",
      "2024-12-23 23:08:28.836679: Pseudo dice [np.float32(0.8209)]\n",
      "2024-12-23 23:08:28.836715: Epoch time: 111.07 s\n",
      "2024-12-23 23:08:29.655728: \n",
      "2024-12-23 23:08:29.655939: Epoch 750\n",
      "2024-12-23 23:08:29.656039: Current learning rate: 0.00287\n",
      "2024-12-23 23:10:20.743242: train_loss -0.8341\n",
      "2024-12-23 23:10:20.743447: val_loss -0.7123\n",
      "2024-12-23 23:10:20.743481: Pseudo dice [np.float32(0.7983)]\n",
      "2024-12-23 23:10:20.743515: Epoch time: 111.09 s\n",
      "2024-12-23 23:10:21.327575: \n",
      "2024-12-23 23:10:21.327788: Epoch 751\n",
      "2024-12-23 23:10:21.327862: Current learning rate: 0.00286\n",
      "2024-12-23 23:12:12.420100: train_loss -0.8384\n",
      "2024-12-23 23:12:12.420237: val_loss -0.6433\n",
      "2024-12-23 23:12:12.420272: Pseudo dice [np.float32(0.7197)]\n",
      "2024-12-23 23:12:12.420307: Epoch time: 111.09 s\n",
      "2024-12-23 23:12:13.009881: \n",
      "2024-12-23 23:12:13.010082: Epoch 752\n",
      "2024-12-23 23:12:13.010168: Current learning rate: 0.00285\n",
      "2024-12-23 23:14:04.096549: train_loss -0.8318\n",
      "2024-12-23 23:14:04.096929: val_loss -0.7245\n",
      "2024-12-23 23:14:04.096975: Pseudo dice [np.float32(0.7946)]\n",
      "2024-12-23 23:14:04.097011: Epoch time: 111.09 s\n",
      "2024-12-23 23:14:04.688416: \n",
      "2024-12-23 23:14:04.688600: Epoch 753\n",
      "2024-12-23 23:14:04.688672: Current learning rate: 0.00284\n",
      "2024-12-23 23:15:55.769689: train_loss -0.8419\n",
      "2024-12-23 23:15:55.769824: val_loss -0.7311\n",
      "2024-12-23 23:15:55.769859: Pseudo dice [np.float32(0.7763)]\n",
      "2024-12-23 23:15:55.769990: Epoch time: 111.08 s\n",
      "2024-12-23 23:15:56.367356: \n",
      "2024-12-23 23:15:56.367743: Epoch 754\n",
      "2024-12-23 23:15:56.367849: Current learning rate: 0.00283\n",
      "2024-12-23 23:17:47.463701: train_loss -0.8176\n",
      "2024-12-23 23:17:47.463830: val_loss -0.7724\n",
      "2024-12-23 23:17:47.463865: Pseudo dice [np.float32(0.8283)]\n",
      "2024-12-23 23:17:47.463900: Epoch time: 111.1 s\n",
      "2024-12-23 23:17:48.062590: \n",
      "2024-12-23 23:17:48.062929: Epoch 755\n",
      "2024-12-23 23:17:48.063054: Current learning rate: 0.00282\n",
      "2024-12-23 23:19:39.169810: train_loss -0.8367\n",
      "2024-12-23 23:19:39.170025: val_loss -0.7375\n",
      "2024-12-23 23:19:39.170060: Pseudo dice [np.float32(0.7917)]\n",
      "2024-12-23 23:19:39.170098: Epoch time: 111.11 s\n",
      "2024-12-23 23:19:39.762233: \n",
      "2024-12-23 23:19:39.762626: Epoch 756\n",
      "2024-12-23 23:19:39.762725: Current learning rate: 0.00281\n",
      "2024-12-23 23:21:30.835473: train_loss -0.8421\n",
      "2024-12-23 23:21:30.835603: val_loss -0.7844\n",
      "2024-12-23 23:21:30.835634: Pseudo dice [np.float32(0.8341)]\n",
      "2024-12-23 23:21:30.835669: Epoch time: 111.07 s\n",
      "2024-12-23 23:21:31.426408: \n",
      "2024-12-23 23:21:31.426520: Epoch 757\n",
      "2024-12-23 23:21:31.426592: Current learning rate: 0.0028\n",
      "2024-12-23 23:23:22.469301: train_loss -0.8482\n",
      "2024-12-23 23:23:22.469508: val_loss -0.7038\n",
      "2024-12-23 23:23:22.469543: Pseudo dice [np.float32(0.7556)]\n",
      "2024-12-23 23:23:22.469578: Epoch time: 111.04 s\n",
      "2024-12-23 23:23:23.049949: \n",
      "2024-12-23 23:23:23.050052: Epoch 758\n",
      "2024-12-23 23:23:23.050121: Current learning rate: 0.00279\n",
      "2024-12-23 23:25:14.075744: train_loss -0.8319\n",
      "2024-12-23 23:25:14.075885: val_loss -0.7205\n",
      "2024-12-23 23:25:14.076008: Pseudo dice [np.float32(0.787)]\n",
      "2024-12-23 23:25:14.076054: Epoch time: 111.03 s\n",
      "2024-12-23 23:25:14.660735: \n",
      "2024-12-23 23:25:14.661133: Epoch 759\n",
      "2024-12-23 23:25:14.661211: Current learning rate: 0.00278\n",
      "2024-12-23 23:27:05.725174: train_loss -0.8357\n",
      "2024-12-23 23:27:05.725463: val_loss -0.7454\n",
      "2024-12-23 23:27:05.725513: Pseudo dice [np.float32(0.8192)]\n",
      "2024-12-23 23:27:05.725548: Epoch time: 111.06 s\n",
      "2024-12-23 23:27:06.316870: \n",
      "2024-12-23 23:27:06.316957: Epoch 760\n",
      "2024-12-23 23:27:06.317026: Current learning rate: 0.00277\n",
      "2024-12-23 23:28:57.400914: train_loss -0.8456\n",
      "2024-12-23 23:28:57.401104: val_loss -0.7459\n",
      "2024-12-23 23:28:57.401150: Pseudo dice [np.float32(0.8323)]\n",
      "2024-12-23 23:28:57.401186: Epoch time: 111.08 s\n",
      "2024-12-23 23:28:57.988824: \n",
      "2024-12-23 23:28:57.988981: Epoch 761\n",
      "2024-12-23 23:28:57.989057: Current learning rate: 0.00276\n",
      "2024-12-23 23:30:49.033552: train_loss -0.8431\n",
      "2024-12-23 23:30:49.033692: val_loss -0.7941\n",
      "2024-12-23 23:30:49.033726: Pseudo dice [np.float32(0.8356)]\n",
      "2024-12-23 23:30:49.033762: Epoch time: 111.05 s\n",
      "2024-12-23 23:30:49.621205: \n",
      "2024-12-23 23:30:49.621618: Epoch 762\n",
      "2024-12-23 23:30:49.621695: Current learning rate: 0.00275\n",
      "2024-12-23 23:32:40.802873: train_loss -0.8425\n",
      "2024-12-23 23:32:40.803067: val_loss -0.7751\n",
      "2024-12-23 23:32:40.803102: Pseudo dice [np.float32(0.8304)]\n",
      "2024-12-23 23:32:40.803154: Epoch time: 111.18 s\n",
      "2024-12-23 23:32:41.408026: \n",
      "2024-12-23 23:32:41.408177: Epoch 763\n",
      "2024-12-23 23:32:41.408256: Current learning rate: 0.00274\n",
      "2024-12-23 23:34:32.550426: train_loss -0.79\n",
      "2024-12-23 23:34:32.550562: val_loss -0.6984\n",
      "2024-12-23 23:34:32.550595: Pseudo dice [np.float32(0.7582)]\n",
      "2024-12-23 23:34:32.550632: Epoch time: 111.14 s\n",
      "2024-12-23 23:34:33.150753: \n",
      "2024-12-23 23:34:33.150939: Epoch 764\n",
      "2024-12-23 23:34:33.151016: Current learning rate: 0.00273\n",
      "2024-12-23 23:36:24.323184: train_loss -0.8249\n",
      "2024-12-23 23:36:24.323601: val_loss -0.6756\n",
      "2024-12-23 23:36:24.323660: Pseudo dice [np.float32(0.7191)]\n",
      "2024-12-23 23:36:24.323703: Epoch time: 111.17 s\n",
      "2024-12-23 23:36:24.920099: \n",
      "2024-12-23 23:36:24.920192: Epoch 765\n",
      "2024-12-23 23:36:24.920262: Current learning rate: 0.00272\n",
      "2024-12-23 23:38:16.066334: train_loss -0.7959\n",
      "2024-12-23 23:38:16.066563: val_loss -0.7222\n",
      "2024-12-23 23:38:16.066602: Pseudo dice [np.float32(0.7683)]\n",
      "2024-12-23 23:38:16.066639: Epoch time: 111.15 s\n",
      "2024-12-23 23:38:17.139443: \n",
      "2024-12-23 23:38:17.139557: Epoch 766\n",
      "2024-12-23 23:38:17.139646: Current learning rate: 0.00271\n",
      "2024-12-23 23:40:08.364618: train_loss -0.8453\n",
      "2024-12-23 23:40:08.364842: val_loss -0.6615\n",
      "2024-12-23 23:40:08.364879: Pseudo dice [np.float32(0.7282)]\n",
      "2024-12-23 23:40:08.364913: Epoch time: 111.23 s\n",
      "2024-12-23 23:40:08.967302: \n",
      "2024-12-23 23:40:08.967425: Epoch 767\n",
      "2024-12-23 23:40:08.967502: Current learning rate: 0.0027\n",
      "2024-12-23 23:42:00.173250: train_loss -0.8322\n",
      "2024-12-23 23:42:00.173500: val_loss -0.6782\n",
      "2024-12-23 23:42:00.173545: Pseudo dice [np.float32(0.6941)]\n",
      "2024-12-23 23:42:00.173580: Epoch time: 111.21 s\n",
      "2024-12-23 23:42:00.774039: \n",
      "2024-12-23 23:42:00.774166: Epoch 768\n",
      "2024-12-23 23:42:00.774242: Current learning rate: 0.00268\n",
      "2024-12-23 23:43:52.017304: train_loss -0.8495\n",
      "2024-12-23 23:43:52.017499: val_loss -0.7225\n",
      "2024-12-23 23:43:52.017532: Pseudo dice [np.float32(0.7904)]\n",
      "2024-12-23 23:43:52.017567: Epoch time: 111.24 s\n",
      "2024-12-23 23:43:52.617884: \n",
      "2024-12-23 23:43:52.618247: Epoch 769\n",
      "2024-12-23 23:43:52.618331: Current learning rate: 0.00267\n",
      "2024-12-23 23:45:43.857374: train_loss -0.8319\n",
      "2024-12-23 23:45:43.857516: val_loss -0.7503\n",
      "2024-12-23 23:45:43.857551: Pseudo dice [np.float32(0.781)]\n",
      "2024-12-23 23:45:43.857587: Epoch time: 111.24 s\n",
      "2024-12-23 23:45:44.461913: \n",
      "2024-12-23 23:45:44.462069: Epoch 770\n",
      "2024-12-23 23:45:44.462190: Current learning rate: 0.00266\n",
      "2024-12-23 23:47:35.637003: train_loss -0.8373\n",
      "2024-12-23 23:47:35.637409: val_loss -0.7374\n",
      "2024-12-23 23:47:35.637449: Pseudo dice [np.float32(0.8084)]\n",
      "2024-12-23 23:47:35.637487: Epoch time: 111.18 s\n",
      "2024-12-23 23:47:36.261636: \n",
      "2024-12-23 23:47:36.261845: Epoch 771\n",
      "2024-12-23 23:47:36.261925: Current learning rate: 0.00265\n",
      "2024-12-23 23:49:27.446897: train_loss -0.8269\n",
      "2024-12-23 23:49:27.447028: val_loss -0.7672\n",
      "2024-12-23 23:49:27.447062: Pseudo dice [np.float32(0.8249)]\n",
      "2024-12-23 23:49:27.447096: Epoch time: 111.19 s\n",
      "2024-12-23 23:49:28.068372: \n",
      "2024-12-23 23:49:28.068755: Epoch 772\n",
      "2024-12-23 23:49:28.068869: Current learning rate: 0.00264\n",
      "2024-12-23 23:51:19.286003: train_loss -0.8499\n",
      "2024-12-23 23:51:19.286151: val_loss -0.6876\n",
      "2024-12-23 23:51:19.286185: Pseudo dice [np.float32(0.755)]\n",
      "2024-12-23 23:51:19.286223: Epoch time: 111.22 s\n",
      "2024-12-23 23:51:19.884530: \n",
      "2024-12-23 23:51:19.884698: Epoch 773\n",
      "2024-12-23 23:51:19.884773: Current learning rate: 0.00263\n",
      "2024-12-23 23:53:11.069511: train_loss -0.8218\n",
      "2024-12-23 23:53:11.069648: val_loss -0.7492\n",
      "2024-12-23 23:53:11.069798: Pseudo dice [np.float32(0.8041)]\n",
      "2024-12-23 23:53:11.069945: Epoch time: 111.19 s\n",
      "2024-12-23 23:53:11.679785: \n",
      "2024-12-23 23:53:11.679888: Epoch 774\n",
      "2024-12-23 23:53:11.679960: Current learning rate: 0.00262\n",
      "2024-12-23 23:55:02.880635: train_loss -0.8387\n",
      "2024-12-23 23:55:02.880779: val_loss -0.739\n",
      "2024-12-23 23:55:02.880812: Pseudo dice [np.float32(0.7555)]\n",
      "2024-12-23 23:55:02.880846: Epoch time: 111.2 s\n",
      "2024-12-23 23:55:03.485831: \n",
      "2024-12-23 23:55:03.486097: Epoch 775\n",
      "2024-12-23 23:55:03.486193: Current learning rate: 0.00261\n",
      "2024-12-23 23:56:54.669904: train_loss -0.8438\n",
      "2024-12-23 23:56:54.670054: val_loss -0.6872\n",
      "2024-12-23 23:56:54.670088: Pseudo dice [np.float32(0.7075)]\n",
      "2024-12-23 23:56:54.670124: Epoch time: 111.18 s\n",
      "2024-12-23 23:56:55.280873: \n",
      "2024-12-23 23:56:55.280988: Epoch 776\n",
      "2024-12-23 23:56:55.281173: Current learning rate: 0.0026\n",
      "2024-12-23 23:58:46.499948: train_loss -0.8511\n",
      "2024-12-23 23:58:46.500101: val_loss -0.7796\n",
      "2024-12-23 23:58:46.500347: Pseudo dice [np.float32(0.8009)]\n",
      "2024-12-23 23:58:46.500460: Epoch time: 111.22 s\n",
      "2024-12-23 23:58:47.102607: \n",
      "2024-12-23 23:58:47.102715: Epoch 777\n",
      "2024-12-23 23:58:47.102855: Current learning rate: 0.00259\n",
      "2024-12-24 00:00:38.276211: train_loss -0.8439\n",
      "2024-12-24 00:00:38.276383: val_loss -0.7923\n",
      "2024-12-24 00:00:38.276428: Pseudo dice [np.float32(0.8374)]\n",
      "2024-12-24 00:00:38.276465: Epoch time: 111.17 s\n",
      "2024-12-24 00:00:38.886827: \n",
      "2024-12-24 00:00:38.887210: Epoch 778\n",
      "2024-12-24 00:00:38.887292: Current learning rate: 0.00258\n",
      "2024-12-24 00:02:30.150488: train_loss -0.8187\n",
      "2024-12-24 00:02:30.150621: val_loss -0.703\n",
      "2024-12-24 00:02:30.150726: Pseudo dice [np.float32(0.7795)]\n",
      "2024-12-24 00:02:30.150973: Epoch time: 111.26 s\n",
      "2024-12-24 00:02:30.755288: \n",
      "2024-12-24 00:02:30.755491: Epoch 779\n",
      "2024-12-24 00:02:30.755584: Current learning rate: 0.00257\n",
      "2024-12-24 00:04:21.947911: train_loss -0.8103\n",
      "2024-12-24 00:04:21.948075: val_loss -0.7343\n",
      "2024-12-24 00:04:21.948111: Pseudo dice [np.float32(0.7644)]\n",
      "2024-12-24 00:04:21.948146: Epoch time: 111.19 s\n",
      "2024-12-24 00:04:22.561041: \n",
      "2024-12-24 00:04:22.561315: Epoch 780\n",
      "2024-12-24 00:04:22.561464: Current learning rate: 0.00256\n",
      "2024-12-24 00:06:13.741067: train_loss -0.8208\n",
      "2024-12-24 00:06:13.741218: val_loss -0.7338\n",
      "2024-12-24 00:06:13.741253: Pseudo dice [np.float32(0.771)]\n",
      "2024-12-24 00:06:13.741289: Epoch time: 111.18 s\n",
      "2024-12-24 00:06:14.356041: \n",
      "2024-12-24 00:06:14.356350: Epoch 781\n",
      "2024-12-24 00:06:14.356565: Current learning rate: 0.00255\n",
      "2024-12-24 00:08:05.551952: train_loss -0.8253\n",
      "2024-12-24 00:08:05.552102: val_loss -0.7143\n",
      "2024-12-24 00:08:05.552140: Pseudo dice [np.float32(0.7366)]\n",
      "2024-12-24 00:08:05.552176: Epoch time: 111.2 s\n",
      "2024-12-24 00:08:06.158128: \n",
      "2024-12-24 00:08:06.158258: Epoch 782\n",
      "2024-12-24 00:08:06.158332: Current learning rate: 0.00254\n",
      "2024-12-24 00:09:57.386012: train_loss -0.8274\n",
      "2024-12-24 00:09:57.386168: val_loss -0.7233\n",
      "2024-12-24 00:09:57.386205: Pseudo dice [np.float32(0.8013)]\n",
      "2024-12-24 00:09:57.386242: Epoch time: 111.23 s\n",
      "2024-12-24 00:09:58.426050: \n",
      "2024-12-24 00:09:58.426271: Epoch 783\n",
      "2024-12-24 00:09:58.426357: Current learning rate: 0.00253\n",
      "2024-12-24 00:11:49.641582: train_loss -0.8311\n",
      "2024-12-24 00:11:49.641736: val_loss -0.7544\n",
      "2024-12-24 00:11:49.641772: Pseudo dice [np.float32(0.805)]\n",
      "2024-12-24 00:11:49.641815: Epoch time: 111.22 s\n",
      "2024-12-24 00:11:50.271537: \n",
      "2024-12-24 00:11:50.271918: Epoch 784\n",
      "2024-12-24 00:11:50.272003: Current learning rate: 0.00252\n",
      "2024-12-24 00:13:41.522526: train_loss -0.8333\n",
      "2024-12-24 00:13:41.522728: val_loss -0.693\n",
      "2024-12-24 00:13:41.522763: Pseudo dice [np.float32(0.7169)]\n",
      "2024-12-24 00:13:41.522799: Epoch time: 111.25 s\n",
      "2024-12-24 00:13:42.126501: \n",
      "2024-12-24 00:13:42.126880: Epoch 785\n",
      "2024-12-24 00:13:42.126971: Current learning rate: 0.00251\n",
      "2024-12-24 00:15:33.407209: train_loss -0.8451\n",
      "2024-12-24 00:15:33.407361: val_loss -0.7674\n",
      "2024-12-24 00:15:33.407399: Pseudo dice [np.float32(0.829)]\n",
      "2024-12-24 00:15:33.407435: Epoch time: 111.28 s\n",
      "2024-12-24 00:15:34.014945: \n",
      "2024-12-24 00:15:34.015130: Epoch 786\n",
      "2024-12-24 00:15:34.015210: Current learning rate: 0.0025\n",
      "2024-12-24 00:17:25.285462: train_loss -0.8545\n",
      "2024-12-24 00:17:25.285777: val_loss -0.6872\n",
      "2024-12-24 00:17:25.285815: Pseudo dice [np.float32(0.7508)]\n",
      "2024-12-24 00:17:25.285855: Epoch time: 111.27 s\n",
      "2024-12-24 00:17:25.893668: \n",
      "2024-12-24 00:17:25.893833: Epoch 787\n",
      "2024-12-24 00:17:25.893909: Current learning rate: 0.00249\n",
      "2024-12-24 00:19:17.174924: train_loss -0.8318\n",
      "2024-12-24 00:19:17.175085: val_loss -0.7465\n",
      "2024-12-24 00:19:17.175121: Pseudo dice [np.float32(0.7837)]\n",
      "2024-12-24 00:19:17.175159: Epoch time: 111.28 s\n",
      "2024-12-24 00:19:17.790024: \n",
      "2024-12-24 00:19:17.790256: Epoch 788\n",
      "2024-12-24 00:19:17.790380: Current learning rate: 0.00248\n",
      "2024-12-24 00:21:09.072384: train_loss -0.833\n",
      "2024-12-24 00:21:09.072693: val_loss -0.7554\n",
      "2024-12-24 00:21:09.072745: Pseudo dice [np.float32(0.8239)]\n",
      "2024-12-24 00:21:09.072798: Epoch time: 111.28 s\n",
      "2024-12-24 00:21:09.684996: \n",
      "2024-12-24 00:21:09.685176: Epoch 789\n",
      "2024-12-24 00:21:09.685251: Current learning rate: 0.00247\n",
      "2024-12-24 00:23:00.881280: train_loss -0.8417\n",
      "2024-12-24 00:23:00.881419: val_loss -0.7493\n",
      "2024-12-24 00:23:00.881452: Pseudo dice [np.float32(0.8002)]\n",
      "2024-12-24 00:23:00.881538: Epoch time: 111.2 s\n",
      "2024-12-24 00:23:01.485613: \n",
      "2024-12-24 00:23:01.485815: Epoch 790\n",
      "2024-12-24 00:23:01.485894: Current learning rate: 0.00245\n",
      "2024-12-24 00:24:52.670041: train_loss -0.8374\n",
      "2024-12-24 00:24:52.670200: val_loss -0.7737\n",
      "2024-12-24 00:24:52.670234: Pseudo dice [np.float32(0.8)]\n",
      "2024-12-24 00:24:52.670325: Epoch time: 111.19 s\n",
      "2024-12-24 00:24:53.271396: \n",
      "2024-12-24 00:24:53.271777: Epoch 791\n",
      "2024-12-24 00:24:53.271864: Current learning rate: 0.00244\n",
      "2024-12-24 00:26:44.490565: train_loss -0.8351\n",
      "2024-12-24 00:26:44.490719: val_loss -0.7597\n",
      "2024-12-24 00:26:44.490753: Pseudo dice [np.float32(0.8069)]\n",
      "2024-12-24 00:26:44.490789: Epoch time: 111.22 s\n",
      "2024-12-24 00:26:45.090034: \n",
      "2024-12-24 00:26:45.090160: Epoch 792\n",
      "2024-12-24 00:26:45.090330: Current learning rate: 0.00243\n",
      "2024-12-24 00:28:36.355199: train_loss -0.841\n",
      "2024-12-24 00:28:36.355378: val_loss -0.7473\n",
      "2024-12-24 00:28:36.355454: Pseudo dice [np.float32(0.7953)]\n",
      "2024-12-24 00:28:36.355500: Epoch time: 111.27 s\n",
      "2024-12-24 00:28:36.969427: \n",
      "2024-12-24 00:28:36.969742: Epoch 793\n",
      "2024-12-24 00:28:36.969826: Current learning rate: 0.00242\n",
      "2024-12-24 00:30:28.202787: train_loss -0.8414\n",
      "2024-12-24 00:30:28.202921: val_loss -0.712\n",
      "2024-12-24 00:30:28.202954: Pseudo dice [np.float32(0.7606)]\n",
      "2024-12-24 00:30:28.202987: Epoch time: 111.23 s\n",
      "2024-12-24 00:30:28.803540: \n",
      "2024-12-24 00:30:28.803923: Epoch 794\n",
      "2024-12-24 00:30:28.803999: Current learning rate: 0.00241\n",
      "2024-12-24 00:32:20.033339: train_loss -0.8294\n",
      "2024-12-24 00:32:20.033677: val_loss -0.7259\n",
      "2024-12-24 00:32:20.033715: Pseudo dice [np.float32(0.7791)]\n",
      "2024-12-24 00:32:20.033754: Epoch time: 111.23 s\n",
      "2024-12-24 00:32:20.638105: \n",
      "2024-12-24 00:32:20.638424: Epoch 795\n",
      "2024-12-24 00:32:20.638515: Current learning rate: 0.0024\n",
      "2024-12-24 00:34:11.838083: train_loss -0.8335\n",
      "2024-12-24 00:34:11.838212: val_loss -0.7814\n",
      "2024-12-24 00:34:11.838246: Pseudo dice [np.float32(0.8197)]\n",
      "2024-12-24 00:34:11.838282: Epoch time: 111.2 s\n",
      "2024-12-24 00:34:12.445861: \n",
      "2024-12-24 00:34:12.446055: Epoch 796\n",
      "2024-12-24 00:34:12.446131: Current learning rate: 0.00239\n",
      "2024-12-24 00:36:03.655215: train_loss -0.8398\n",
      "2024-12-24 00:36:03.655373: val_loss -0.7638\n",
      "2024-12-24 00:36:03.655409: Pseudo dice [np.float32(0.8179)]\n",
      "2024-12-24 00:36:03.655447: Epoch time: 111.21 s\n",
      "2024-12-24 00:36:04.269939: \n",
      "2024-12-24 00:36:04.270097: Epoch 797\n",
      "2024-12-24 00:36:04.270176: Current learning rate: 0.00238\n",
      "2024-12-24 00:37:55.626907: train_loss -0.8234\n",
      "2024-12-24 00:37:55.627131: val_loss -0.7312\n",
      "2024-12-24 00:37:55.627173: Pseudo dice [np.float32(0.8019)]\n",
      "2024-12-24 00:37:55.627211: Epoch time: 111.36 s\n",
      "2024-12-24 00:37:56.231221: \n",
      "2024-12-24 00:37:56.231398: Epoch 798\n",
      "2024-12-24 00:37:56.231473: Current learning rate: 0.00237\n",
      "2024-12-24 00:39:47.457340: train_loss -0.8168\n",
      "2024-12-24 00:39:47.457469: val_loss -0.7257\n",
      "2024-12-24 00:39:47.457502: Pseudo dice [np.float32(0.7965)]\n",
      "2024-12-24 00:39:47.457538: Epoch time: 111.23 s\n",
      "2024-12-24 00:39:48.062629: \n",
      "2024-12-24 00:39:48.062793: Epoch 799\n",
      "2024-12-24 00:39:48.062868: Current learning rate: 0.00236\n",
      "2024-12-24 00:41:39.291472: train_loss -0.8382\n",
      "2024-12-24 00:41:39.291624: val_loss -0.7562\n",
      "2024-12-24 00:41:39.291658: Pseudo dice [np.float32(0.7946)]\n",
      "2024-12-24 00:41:39.291705: Epoch time: 111.23 s\n",
      "2024-12-24 00:41:40.634142: \n",
      "2024-12-24 00:41:40.634547: Epoch 800\n",
      "2024-12-24 00:41:40.634772: Current learning rate: 0.00235\n",
      "2024-12-24 00:43:31.875353: train_loss -0.86\n",
      "2024-12-24 00:43:31.875507: val_loss -0.7649\n",
      "2024-12-24 00:43:31.875542: Pseudo dice [np.float32(0.791)]\n",
      "2024-12-24 00:43:31.875579: Epoch time: 111.24 s\n",
      "2024-12-24 00:43:32.496241: \n",
      "2024-12-24 00:43:32.496432: Epoch 801\n",
      "2024-12-24 00:43:32.496506: Current learning rate: 0.00234\n",
      "2024-12-24 00:45:23.757864: train_loss -0.8394\n",
      "2024-12-24 00:45:23.758018: val_loss -0.7708\n",
      "2024-12-24 00:45:23.758052: Pseudo dice [np.float32(0.8302)]\n",
      "2024-12-24 00:45:23.758088: Epoch time: 111.26 s\n",
      "2024-12-24 00:45:24.372914: \n",
      "2024-12-24 00:45:24.373248: Epoch 802\n",
      "2024-12-24 00:45:24.373525: Current learning rate: 0.00233\n",
      "2024-12-24 00:47:15.421673: train_loss -0.8472\n",
      "2024-12-24 00:47:15.421947: val_loss -0.7652\n",
      "2024-12-24 00:47:15.421984: Pseudo dice [np.float32(0.8209)]\n",
      "2024-12-24 00:47:15.422020: Epoch time: 111.05 s\n",
      "2024-12-24 00:47:16.040261: \n",
      "2024-12-24 00:47:16.040668: Epoch 803\n",
      "2024-12-24 00:47:16.040762: Current learning rate: 0.00232\n",
      "2024-12-24 00:49:07.259194: train_loss -0.8368\n",
      "2024-12-24 00:49:07.259331: val_loss -0.7129\n",
      "2024-12-24 00:49:07.259365: Pseudo dice [np.float32(0.7579)]\n",
      "2024-12-24 00:49:07.259469: Epoch time: 111.22 s\n",
      "2024-12-24 00:49:07.865054: \n",
      "2024-12-24 00:49:07.865342: Epoch 804\n",
      "2024-12-24 00:49:07.865492: Current learning rate: 0.00231\n",
      "2024-12-24 00:50:59.097709: train_loss -0.8573\n",
      "2024-12-24 00:50:59.098069: val_loss -0.7478\n",
      "2024-12-24 00:50:59.098162: Pseudo dice [np.float32(0.7996)]\n",
      "2024-12-24 00:50:59.098207: Epoch time: 111.23 s\n",
      "2024-12-24 00:50:59.710073: \n",
      "2024-12-24 00:50:59.710513: Epoch 805\n",
      "2024-12-24 00:50:59.710624: Current learning rate: 0.0023\n",
      "2024-12-24 00:52:50.920106: train_loss -0.8473\n",
      "2024-12-24 00:52:50.920261: val_loss -0.6962\n",
      "2024-12-24 00:52:50.920295: Pseudo dice [np.float32(0.7909)]\n",
      "2024-12-24 00:52:50.920331: Epoch time: 111.21 s\n",
      "2024-12-24 00:52:51.536904: \n",
      "2024-12-24 00:52:51.537282: Epoch 806\n",
      "2024-12-24 00:52:51.537506: Current learning rate: 0.00229\n",
      "2024-12-24 00:54:42.778808: train_loss -0.8491\n",
      "2024-12-24 00:54:42.778953: val_loss -0.7964\n",
      "2024-12-24 00:54:42.778986: Pseudo dice [np.float32(0.8296)]\n",
      "2024-12-24 00:54:42.779022: Epoch time: 111.24 s\n",
      "2024-12-24 00:54:43.399974: \n",
      "2024-12-24 00:54:43.400168: Epoch 807\n",
      "2024-12-24 00:54:43.400251: Current learning rate: 0.00228\n",
      "2024-12-24 00:56:34.618974: train_loss -0.8417\n",
      "2024-12-24 00:56:34.619120: val_loss -0.6413\n",
      "2024-12-24 00:56:34.619385: Pseudo dice [np.float32(0.7673)]\n",
      "2024-12-24 00:56:34.619460: Epoch time: 111.22 s\n",
      "2024-12-24 00:56:35.218185: \n",
      "2024-12-24 00:56:35.218591: Epoch 808\n",
      "2024-12-24 00:56:35.218777: Current learning rate: 0.00226\n",
      "2024-12-24 00:58:26.429499: train_loss -0.8187\n",
      "2024-12-24 00:58:26.429653: val_loss -0.6889\n",
      "2024-12-24 00:58:26.429796: Pseudo dice [np.float32(0.7261)]\n",
      "2024-12-24 00:58:26.429849: Epoch time: 111.21 s\n",
      "2024-12-24 00:58:27.043454: \n",
      "2024-12-24 00:58:27.043695: Epoch 809\n",
      "2024-12-24 00:58:27.043779: Current learning rate: 0.00225\n",
      "2024-12-24 01:00:18.681069: train_loss -0.8503\n",
      "2024-12-24 01:00:18.681211: val_loss -0.7392\n",
      "2024-12-24 01:00:18.681244: Pseudo dice [np.float32(0.786)]\n",
      "2024-12-24 01:00:18.681281: Epoch time: 111.64 s\n",
      "2024-12-24 01:00:19.300021: \n",
      "2024-12-24 01:00:19.300342: Epoch 810\n",
      "2024-12-24 01:00:19.300425: Current learning rate: 0.00224\n",
      "2024-12-24 01:02:10.817417: train_loss -0.828\n",
      "2024-12-24 01:02:10.817606: val_loss -0.7009\n",
      "2024-12-24 01:02:10.817638: Pseudo dice [np.float32(0.7346)]\n",
      "2024-12-24 01:02:10.817674: Epoch time: 111.52 s\n",
      "2024-12-24 01:02:11.440663: \n",
      "2024-12-24 01:02:11.440990: Epoch 811\n",
      "2024-12-24 01:02:11.441070: Current learning rate: 0.00223\n",
      "2024-12-24 01:04:03.003784: train_loss -0.8301\n",
      "2024-12-24 01:04:03.004115: val_loss -0.6723\n",
      "2024-12-24 01:04:03.004194: Pseudo dice [np.float32(0.7717)]\n",
      "2024-12-24 01:04:03.004241: Epoch time: 111.56 s\n",
      "2024-12-24 01:04:03.613683: \n",
      "2024-12-24 01:04:03.613868: Epoch 812\n",
      "2024-12-24 01:04:03.613942: Current learning rate: 0.00222\n",
      "2024-12-24 01:05:55.154368: train_loss -0.8411\n",
      "2024-12-24 01:05:55.154525: val_loss -0.7347\n",
      "2024-12-24 01:05:55.154562: Pseudo dice [np.float32(0.7863)]\n",
      "2024-12-24 01:05:55.154604: Epoch time: 111.54 s\n",
      "2024-12-24 01:05:55.814915: \n",
      "2024-12-24 01:05:55.815233: Epoch 813\n",
      "2024-12-24 01:05:55.815314: Current learning rate: 0.00221\n",
      "2024-12-24 01:07:47.305379: train_loss -0.8389\n",
      "2024-12-24 01:07:47.305587: val_loss -0.7399\n",
      "2024-12-24 01:07:47.305623: Pseudo dice [np.float32(0.7579)]\n",
      "2024-12-24 01:07:47.305659: Epoch time: 111.49 s\n",
      "2024-12-24 01:07:47.923025: \n",
      "2024-12-24 01:07:47.923401: Epoch 814\n",
      "2024-12-24 01:07:47.923483: Current learning rate: 0.0022\n",
      "2024-12-24 01:09:39.443579: train_loss -0.8446\n",
      "2024-12-24 01:09:39.443728: val_loss -0.7587\n",
      "2024-12-24 01:09:39.443763: Pseudo dice [np.float32(0.7912)]\n",
      "2024-12-24 01:09:39.443800: Epoch time: 111.52 s\n",
      "2024-12-24 01:09:40.051326: \n",
      "2024-12-24 01:09:40.051543: Epoch 815\n",
      "2024-12-24 01:09:40.051623: Current learning rate: 0.00219\n",
      "2024-12-24 01:11:31.435055: train_loss -0.8286\n",
      "2024-12-24 01:11:31.435193: val_loss -0.6515\n",
      "2024-12-24 01:11:31.435366: Pseudo dice [np.float32(0.7404)]\n",
      "2024-12-24 01:11:31.435426: Epoch time: 111.38 s\n",
      "2024-12-24 01:11:32.044145: \n",
      "2024-12-24 01:11:32.044351: Epoch 816\n",
      "2024-12-24 01:11:32.044476: Current learning rate: 0.00218\n",
      "2024-12-24 01:13:23.309462: train_loss -0.8241\n",
      "2024-12-24 01:13:23.309688: val_loss -0.6408\n",
      "2024-12-24 01:13:23.309724: Pseudo dice [np.float32(0.6992)]\n",
      "2024-12-24 01:13:23.309760: Epoch time: 111.27 s\n",
      "2024-12-24 01:13:24.373988: \n",
      "2024-12-24 01:13:24.374222: Epoch 817\n",
      "2024-12-24 01:13:24.374333: Current learning rate: 0.00217\n",
      "2024-12-24 01:15:15.624435: train_loss -0.849\n",
      "2024-12-24 01:15:15.624582: val_loss -0.6506\n",
      "2024-12-24 01:15:15.624618: Pseudo dice [np.float32(0.7545)]\n",
      "2024-12-24 01:15:15.624736: Epoch time: 111.25 s\n",
      "2024-12-24 01:15:16.236479: \n",
      "2024-12-24 01:15:16.236849: Epoch 818\n",
      "2024-12-24 01:15:16.236977: Current learning rate: 0.00216\n",
      "2024-12-24 01:17:07.472955: train_loss -0.8456\n",
      "2024-12-24 01:17:07.473196: val_loss -0.6945\n",
      "2024-12-24 01:17:07.473266: Pseudo dice [np.float32(0.771)]\n",
      "2024-12-24 01:17:07.473307: Epoch time: 111.24 s\n",
      "2024-12-24 01:17:08.085508: \n",
      "2024-12-24 01:17:08.085853: Epoch 819\n",
      "2024-12-24 01:17:08.085973: Current learning rate: 0.00215\n",
      "2024-12-24 01:18:59.328624: train_loss -0.8523\n",
      "2024-12-24 01:18:59.328768: val_loss -0.7314\n",
      "2024-12-24 01:18:59.328803: Pseudo dice [np.float32(0.8082)]\n",
      "2024-12-24 01:18:59.328840: Epoch time: 111.24 s\n",
      "2024-12-24 01:18:59.920618: \n",
      "2024-12-24 01:18:59.920787: Epoch 820\n",
      "2024-12-24 01:18:59.920934: Current learning rate: 0.00214\n",
      "2024-12-24 01:20:51.170088: train_loss -0.8412\n",
      "2024-12-24 01:20:51.170264: val_loss -0.7494\n",
      "2024-12-24 01:20:51.170371: Pseudo dice [np.float32(0.7655)]\n",
      "2024-12-24 01:20:51.170440: Epoch time: 111.25 s\n",
      "2024-12-24 01:20:51.763836: \n",
      "2024-12-24 01:20:51.764192: Epoch 821\n",
      "2024-12-24 01:20:51.764273: Current learning rate: 0.00213\n",
      "2024-12-24 01:22:43.039315: train_loss -0.83\n",
      "2024-12-24 01:22:43.039621: val_loss -0.7331\n",
      "2024-12-24 01:22:43.039682: Pseudo dice [np.float32(0.78)]\n",
      "2024-12-24 01:22:43.039724: Epoch time: 111.28 s\n",
      "2024-12-24 01:22:43.628194: \n",
      "2024-12-24 01:22:43.628413: Epoch 822\n",
      "2024-12-24 01:22:43.628493: Current learning rate: 0.00212\n",
      "2024-12-24 01:24:34.860970: train_loss -0.8446\n",
      "2024-12-24 01:24:34.861130: val_loss -0.7663\n",
      "2024-12-24 01:24:34.861164: Pseudo dice [np.float32(0.8318)]\n",
      "2024-12-24 01:24:34.861199: Epoch time: 111.23 s\n",
      "2024-12-24 01:24:35.454246: \n",
      "2024-12-24 01:24:35.454572: Epoch 823\n",
      "2024-12-24 01:24:35.454787: Current learning rate: 0.0021\n",
      "2024-12-24 01:26:26.708493: train_loss -0.8444\n",
      "2024-12-24 01:26:26.708646: val_loss -0.7086\n",
      "2024-12-24 01:26:26.708682: Pseudo dice [np.float32(0.7335)]\n",
      "2024-12-24 01:26:26.708720: Epoch time: 111.25 s\n",
      "2024-12-24 01:26:27.308127: \n",
      "2024-12-24 01:26:27.308255: Epoch 824\n",
      "2024-12-24 01:26:27.308338: Current learning rate: 0.00209\n",
      "2024-12-24 01:28:18.541102: train_loss -0.8427\n",
      "2024-12-24 01:28:18.541530: val_loss -0.7444\n",
      "2024-12-24 01:28:18.541603: Pseudo dice [np.float32(0.8303)]\n",
      "2024-12-24 01:28:18.541650: Epoch time: 111.23 s\n",
      "2024-12-24 01:28:19.132326: \n",
      "2024-12-24 01:28:19.132480: Epoch 825\n",
      "2024-12-24 01:28:19.132553: Current learning rate: 0.00208\n",
      "2024-12-24 01:30:10.324300: train_loss -0.8458\n",
      "2024-12-24 01:30:10.324516: val_loss -0.6695\n",
      "2024-12-24 01:30:10.324552: Pseudo dice [np.float32(0.7104)]\n",
      "2024-12-24 01:30:10.324589: Epoch time: 111.19 s\n",
      "2024-12-24 01:30:10.916249: \n",
      "2024-12-24 01:30:10.916468: Epoch 826\n",
      "2024-12-24 01:30:10.916545: Current learning rate: 0.00207\n",
      "2024-12-24 01:32:02.133371: train_loss -0.842\n",
      "2024-12-24 01:32:02.133585: val_loss -0.747\n",
      "2024-12-24 01:32:02.133628: Pseudo dice [np.float32(0.8059)]\n",
      "2024-12-24 01:32:02.133665: Epoch time: 111.22 s\n",
      "2024-12-24 01:32:02.726527: \n",
      "2024-12-24 01:32:02.726638: Epoch 827\n",
      "2024-12-24 01:32:02.726721: Current learning rate: 0.00206\n",
      "2024-12-24 01:33:53.945011: train_loss -0.847\n",
      "2024-12-24 01:33:53.945171: val_loss -0.6119\n",
      "2024-12-24 01:33:53.945205: Pseudo dice [np.float32(0.7237)]\n",
      "2024-12-24 01:33:53.945239: Epoch time: 111.22 s\n",
      "2024-12-24 01:33:54.548644: \n",
      "2024-12-24 01:33:54.548753: Epoch 828\n",
      "2024-12-24 01:33:54.548826: Current learning rate: 0.00205\n",
      "2024-12-24 01:35:45.772252: train_loss -0.8473\n",
      "2024-12-24 01:35:45.772725: val_loss -0.7312\n",
      "2024-12-24 01:35:45.772810: Pseudo dice [np.float32(0.8129)]\n",
      "2024-12-24 01:35:45.772963: Epoch time: 111.22 s\n",
      "2024-12-24 01:35:46.370300: \n",
      "2024-12-24 01:35:46.370421: Epoch 829\n",
      "2024-12-24 01:35:46.370498: Current learning rate: 0.00204\n",
      "2024-12-24 01:37:37.573479: train_loss -0.8411\n",
      "2024-12-24 01:37:37.573728: val_loss -0.7259\n",
      "2024-12-24 01:37:37.573766: Pseudo dice [np.float32(0.7764)]\n",
      "2024-12-24 01:37:37.573804: Epoch time: 111.2 s\n",
      "2024-12-24 01:37:38.156186: \n",
      "2024-12-24 01:37:38.156378: Epoch 830\n",
      "2024-12-24 01:37:38.156454: Current learning rate: 0.00203\n",
      "2024-12-24 01:39:29.370555: train_loss -0.8397\n",
      "2024-12-24 01:39:29.370902: val_loss -0.7004\n",
      "2024-12-24 01:39:29.370941: Pseudo dice [np.float32(0.7495)]\n",
      "2024-12-24 01:39:29.370978: Epoch time: 111.21 s\n",
      "2024-12-24 01:39:29.953599: \n",
      "2024-12-24 01:39:29.953945: Epoch 831\n",
      "2024-12-24 01:39:29.954027: Current learning rate: 0.00202\n",
      "2024-12-24 01:41:21.181105: train_loss -0.8455\n",
      "2024-12-24 01:41:21.181267: val_loss -0.7036\n",
      "2024-12-24 01:41:21.181302: Pseudo dice [np.float32(0.7961)]\n",
      "2024-12-24 01:41:21.181339: Epoch time: 111.23 s\n",
      "2024-12-24 01:41:21.782837: \n",
      "2024-12-24 01:41:21.783110: Epoch 832\n",
      "2024-12-24 01:41:21.783278: Current learning rate: 0.00201\n",
      "2024-12-24 01:43:12.994975: train_loss -0.8402\n",
      "2024-12-24 01:43:12.995118: val_loss -0.6895\n",
      "2024-12-24 01:43:12.995152: Pseudo dice [np.float32(0.766)]\n",
      "2024-12-24 01:43:12.995186: Epoch time: 111.21 s\n",
      "2024-12-24 01:43:13.585011: \n",
      "2024-12-24 01:43:13.585310: Epoch 833\n",
      "2024-12-24 01:43:13.585391: Current learning rate: 0.002\n",
      "2024-12-24 01:45:04.829409: train_loss -0.8378\n",
      "2024-12-24 01:45:04.829563: val_loss -0.6059\n",
      "2024-12-24 01:45:04.829600: Pseudo dice [np.float32(0.7042)]\n",
      "2024-12-24 01:45:04.829640: Epoch time: 111.24 s\n",
      "2024-12-24 01:45:05.427356: \n",
      "2024-12-24 01:45:05.427759: Epoch 834\n",
      "2024-12-24 01:45:05.427954: Current learning rate: 0.00199\n",
      "2024-12-24 01:46:56.676426: train_loss -0.834\n",
      "2024-12-24 01:46:56.676711: val_loss -0.6828\n",
      "2024-12-24 01:46:56.676758: Pseudo dice [np.float32(0.7746)]\n",
      "2024-12-24 01:46:56.676795: Epoch time: 111.25 s\n",
      "2024-12-24 01:46:57.726898: \n",
      "2024-12-24 01:46:57.727294: Epoch 835\n",
      "2024-12-24 01:46:57.727485: Current learning rate: 0.00198\n",
      "2024-12-24 01:48:48.981007: train_loss -0.8517\n",
      "2024-12-24 01:48:48.981171: val_loss -0.7242\n",
      "2024-12-24 01:48:48.981364: Pseudo dice [np.float32(0.7808)]\n",
      "2024-12-24 01:48:48.981415: Epoch time: 111.25 s\n",
      "2024-12-24 01:48:49.563837: \n",
      "2024-12-24 01:48:49.564303: Epoch 836\n",
      "2024-12-24 01:48:49.564407: Current learning rate: 0.00196\n",
      "2024-12-24 01:50:40.816923: train_loss -0.8384\n",
      "2024-12-24 01:50:40.817083: val_loss -0.7179\n",
      "2024-12-24 01:50:40.817127: Pseudo dice [np.float32(0.7614)]\n",
      "2024-12-24 01:50:40.817164: Epoch time: 111.25 s\n",
      "2024-12-24 01:50:41.405057: \n",
      "2024-12-24 01:50:41.405288: Epoch 837\n",
      "2024-12-24 01:50:41.405370: Current learning rate: 0.00195\n",
      "2024-12-24 01:52:32.609014: train_loss -0.8459\n",
      "2024-12-24 01:52:32.609169: val_loss -0.7313\n",
      "2024-12-24 01:52:32.609204: Pseudo dice [np.float32(0.781)]\n",
      "2024-12-24 01:52:32.609240: Epoch time: 111.2 s\n",
      "2024-12-24 01:52:33.218207: \n",
      "2024-12-24 01:52:33.218401: Epoch 838\n",
      "2024-12-24 01:52:33.218477: Current learning rate: 0.00194\n",
      "2024-12-24 01:54:24.419771: train_loss -0.8531\n",
      "2024-12-24 01:54:24.419917: val_loss -0.7403\n",
      "2024-12-24 01:54:24.419949: Pseudo dice [np.float32(0.8024)]\n",
      "2024-12-24 01:54:24.419992: Epoch time: 111.2 s\n",
      "2024-12-24 01:54:25.025180: \n",
      "2024-12-24 01:54:25.025369: Epoch 839\n",
      "2024-12-24 01:54:25.025454: Current learning rate: 0.00193\n",
      "2024-12-24 01:56:16.243151: train_loss -0.8641\n",
      "2024-12-24 01:56:16.243357: val_loss -0.7535\n",
      "2024-12-24 01:56:16.243414: Pseudo dice [np.float32(0.7996)]\n",
      "2024-12-24 01:56:16.243469: Epoch time: 111.22 s\n",
      "2024-12-24 01:56:16.838092: \n",
      "2024-12-24 01:56:16.838211: Epoch 840\n",
      "2024-12-24 01:56:16.838401: Current learning rate: 0.00192\n",
      "2024-12-24 01:58:08.060125: train_loss -0.8525\n",
      "2024-12-24 01:58:08.060288: val_loss -0.7018\n",
      "2024-12-24 01:58:08.060323: Pseudo dice [np.float32(0.8154)]\n",
      "2024-12-24 01:58:08.060360: Epoch time: 111.22 s\n",
      "2024-12-24 01:58:08.653007: \n",
      "2024-12-24 01:58:08.653127: Epoch 841\n",
      "2024-12-24 01:58:08.653200: Current learning rate: 0.00191\n",
      "2024-12-24 01:59:59.686398: train_loss -0.8417\n",
      "2024-12-24 01:59:59.686545: val_loss -0.7441\n",
      "2024-12-24 01:59:59.686579: Pseudo dice [np.float32(0.8141)]\n",
      "2024-12-24 01:59:59.686614: Epoch time: 111.03 s\n",
      "2024-12-24 02:00:00.280730: \n",
      "2024-12-24 02:00:00.280984: Epoch 842\n",
      "2024-12-24 02:00:00.281069: Current learning rate: 0.0019\n",
      "2024-12-24 02:01:51.597476: train_loss -0.85\n",
      "2024-12-24 02:01:51.597629: val_loss -0.7459\n",
      "2024-12-24 02:01:51.597668: Pseudo dice [np.float32(0.8107)]\n",
      "2024-12-24 02:01:51.597733: Epoch time: 111.32 s\n",
      "2024-12-24 02:01:52.192771: \n",
      "2024-12-24 02:01:52.193192: Epoch 843\n",
      "2024-12-24 02:01:52.193280: Current learning rate: 0.00189\n",
      "2024-12-24 02:03:43.390001: train_loss -0.8403\n",
      "2024-12-24 02:03:43.390341: val_loss -0.7107\n",
      "2024-12-24 02:03:43.390391: Pseudo dice [np.float32(0.7606)]\n",
      "2024-12-24 02:03:43.390429: Epoch time: 111.2 s\n",
      "2024-12-24 02:03:43.982373: \n",
      "2024-12-24 02:03:43.982759: Epoch 844\n",
      "2024-12-24 02:03:43.982853: Current learning rate: 0.00188\n",
      "2024-12-24 02:05:35.226655: train_loss -0.8536\n",
      "2024-12-24 02:05:35.226804: val_loss -0.6613\n",
      "2024-12-24 02:05:35.226838: Pseudo dice [np.float32(0.7302)]\n",
      "2024-12-24 02:05:35.226873: Epoch time: 111.24 s\n",
      "2024-12-24 02:05:35.808813: \n",
      "2024-12-24 02:05:35.809153: Epoch 845\n",
      "2024-12-24 02:05:35.809265: Current learning rate: 0.00187\n",
      "2024-12-24 02:07:27.014876: train_loss -0.8543\n",
      "2024-12-24 02:07:27.015013: val_loss -0.7877\n",
      "2024-12-24 02:07:27.015046: Pseudo dice [np.float32(0.8243)]\n",
      "2024-12-24 02:07:27.015080: Epoch time: 111.21 s\n",
      "2024-12-24 02:07:27.612134: \n",
      "2024-12-24 02:07:27.612428: Epoch 846\n",
      "2024-12-24 02:07:27.612537: Current learning rate: 0.00186\n",
      "2024-12-24 02:09:18.834187: train_loss -0.8425\n",
      "2024-12-24 02:09:18.834372: val_loss -0.7576\n",
      "2024-12-24 02:09:18.834453: Pseudo dice [np.float32(0.8099)]\n",
      "2024-12-24 02:09:18.834503: Epoch time: 111.22 s\n",
      "2024-12-24 02:09:19.423649: \n",
      "2024-12-24 02:09:19.423764: Epoch 847\n",
      "2024-12-24 02:09:19.423844: Current learning rate: 0.00185\n",
      "2024-12-24 02:11:10.611742: train_loss -0.8204\n",
      "2024-12-24 02:11:10.612034: val_loss -0.7987\n",
      "2024-12-24 02:11:10.612092: Pseudo dice [np.float32(0.8304)]\n",
      "2024-12-24 02:11:10.612133: Epoch time: 111.19 s\n",
      "2024-12-24 02:11:11.198411: \n",
      "2024-12-24 02:11:11.198643: Epoch 848\n",
      "2024-12-24 02:11:11.198721: Current learning rate: 0.00184\n",
      "2024-12-24 02:13:02.398704: train_loss -0.8535\n",
      "2024-12-24 02:13:02.398859: val_loss -0.7431\n",
      "2024-12-24 02:13:02.398896: Pseudo dice [np.float32(0.7829)]\n",
      "2024-12-24 02:13:02.398933: Epoch time: 111.2 s\n",
      "2024-12-24 02:13:02.992455: \n",
      "2024-12-24 02:13:02.992567: Epoch 849\n",
      "2024-12-24 02:13:02.992640: Current learning rate: 0.00182\n",
      "2024-12-24 02:14:54.209826: train_loss -0.8081\n",
      "2024-12-24 02:14:54.210042: val_loss -0.752\n",
      "2024-12-24 02:14:54.210078: Pseudo dice [np.float32(0.8095)]\n",
      "2024-12-24 02:14:54.210114: Epoch time: 111.22 s\n",
      "2024-12-24 02:14:55.073714: \n",
      "2024-12-24 02:14:55.074077: Epoch 850\n",
      "2024-12-24 02:14:55.074189: Current learning rate: 0.00181\n",
      "2024-12-24 02:16:46.268197: train_loss -0.8271\n",
      "2024-12-24 02:16:46.268718: val_loss -0.7196\n",
      "2024-12-24 02:16:46.268785: Pseudo dice [np.float32(0.7814)]\n",
      "2024-12-24 02:16:46.268830: Epoch time: 111.2 s\n",
      "2024-12-24 02:16:46.858100: \n",
      "2024-12-24 02:16:46.858212: Epoch 851\n",
      "2024-12-24 02:16:46.858287: Current learning rate: 0.0018\n",
      "2024-12-24 02:18:38.043085: train_loss -0.8289\n",
      "2024-12-24 02:18:38.043530: val_loss -0.783\n",
      "2024-12-24 02:18:38.043572: Pseudo dice [np.float32(0.8028)]\n",
      "2024-12-24 02:18:38.043607: Epoch time: 111.19 s\n",
      "2024-12-24 02:18:38.618534: \n",
      "2024-12-24 02:18:38.618815: Epoch 852\n",
      "2024-12-24 02:18:38.618984: Current learning rate: 0.00179\n",
      "2024-12-24 02:20:29.820698: train_loss -0.844\n",
      "2024-12-24 02:20:29.820836: val_loss -0.7827\n",
      "2024-12-24 02:20:29.820868: Pseudo dice [np.float32(0.8094)]\n",
      "2024-12-24 02:20:29.820939: Epoch time: 111.2 s\n",
      "2024-12-24 02:20:30.406168: \n",
      "2024-12-24 02:20:30.406476: Epoch 853\n",
      "2024-12-24 02:20:30.406582: Current learning rate: 0.00178\n",
      "2024-12-24 02:22:21.532438: train_loss -0.8414\n",
      "2024-12-24 02:22:21.532581: val_loss -0.7062\n",
      "2024-12-24 02:22:21.532616: Pseudo dice [np.float32(0.8002)]\n",
      "2024-12-24 02:22:21.532734: Epoch time: 111.13 s\n",
      "2024-12-24 02:22:22.554986: \n",
      "2024-12-24 02:22:22.555335: Epoch 854\n",
      "2024-12-24 02:22:22.555481: Current learning rate: 0.00177\n",
      "2024-12-24 02:24:13.756773: train_loss -0.8411\n",
      "2024-12-24 02:24:13.757166: val_loss -0.6707\n",
      "2024-12-24 02:24:13.757215: Pseudo dice [np.float32(0.7692)]\n",
      "2024-12-24 02:24:13.757254: Epoch time: 111.2 s\n",
      "2024-12-24 02:24:14.347151: \n",
      "2024-12-24 02:24:14.347475: Epoch 855\n",
      "2024-12-24 02:24:14.347587: Current learning rate: 0.00176\n",
      "2024-12-24 02:26:05.536550: train_loss -0.8534\n",
      "2024-12-24 02:26:05.536769: val_loss -0.7304\n",
      "2024-12-24 02:26:05.536820: Pseudo dice [np.float32(0.798)]\n",
      "2024-12-24 02:26:05.536859: Epoch time: 111.19 s\n",
      "2024-12-24 02:26:06.121314: \n",
      "2024-12-24 02:26:06.121682: Epoch 856\n",
      "2024-12-24 02:26:06.121763: Current learning rate: 0.00175\n",
      "2024-12-24 02:27:57.327298: train_loss -0.8423\n",
      "2024-12-24 02:27:57.327438: val_loss -0.7299\n",
      "2024-12-24 02:27:57.327473: Pseudo dice [np.float32(0.7783)]\n",
      "2024-12-24 02:27:57.327509: Epoch time: 111.21 s\n",
      "2024-12-24 02:27:57.906276: \n",
      "2024-12-24 02:27:57.906390: Epoch 857\n",
      "2024-12-24 02:27:57.906464: Current learning rate: 0.00174\n",
      "2024-12-24 02:29:49.125107: train_loss -0.8548\n",
      "2024-12-24 02:29:49.125263: val_loss -0.7281\n",
      "2024-12-24 02:29:49.125301: Pseudo dice [np.float32(0.7495)]\n",
      "2024-12-24 02:29:49.125337: Epoch time: 111.22 s\n",
      "2024-12-24 02:29:49.719116: \n",
      "2024-12-24 02:29:49.719228: Epoch 858\n",
      "2024-12-24 02:29:49.719324: Current learning rate: 0.00173\n",
      "2024-12-24 02:31:40.952215: train_loss -0.8447\n",
      "2024-12-24 02:31:40.952425: val_loss -0.7366\n",
      "2024-12-24 02:31:40.952474: Pseudo dice [np.float32(0.7988)]\n",
      "2024-12-24 02:31:40.952513: Epoch time: 111.23 s\n",
      "2024-12-24 02:31:41.538621: \n",
      "2024-12-24 02:31:41.539094: Epoch 859\n",
      "2024-12-24 02:31:41.539252: Current learning rate: 0.00172\n",
      "2024-12-24 02:33:32.822412: train_loss -0.8405\n",
      "2024-12-24 02:33:32.822567: val_loss -0.7666\n",
      "2024-12-24 02:33:32.822603: Pseudo dice [np.float32(0.816)]\n",
      "2024-12-24 02:33:32.822640: Epoch time: 111.28 s\n",
      "2024-12-24 02:33:33.409529: \n",
      "2024-12-24 02:33:33.409626: Epoch 860\n",
      "2024-12-24 02:33:33.409698: Current learning rate: 0.0017\n",
      "2024-12-24 02:35:24.641267: train_loss -0.855\n",
      "2024-12-24 02:35:24.641440: val_loss -0.7318\n",
      "2024-12-24 02:35:24.641477: Pseudo dice [np.float32(0.7796)]\n",
      "2024-12-24 02:35:24.641515: Epoch time: 111.23 s\n",
      "2024-12-24 02:35:25.231912: \n",
      "2024-12-24 02:35:25.232036: Epoch 861\n",
      "2024-12-24 02:35:25.232109: Current learning rate: 0.00169\n",
      "2024-12-24 02:37:16.422238: train_loss -0.8432\n",
      "2024-12-24 02:37:16.422385: val_loss -0.7751\n",
      "2024-12-24 02:37:16.422420: Pseudo dice [np.float32(0.8118)]\n",
      "2024-12-24 02:37:16.422457: Epoch time: 111.19 s\n",
      "2024-12-24 02:37:17.012707: \n",
      "2024-12-24 02:37:17.012816: Epoch 862\n",
      "2024-12-24 02:37:17.012956: Current learning rate: 0.00168\n",
      "2024-12-24 02:39:08.227766: train_loss -0.8421\n",
      "2024-12-24 02:39:08.227903: val_loss -0.7797\n",
      "2024-12-24 02:39:08.227938: Pseudo dice [np.float32(0.8434)]\n",
      "2024-12-24 02:39:08.227973: Epoch time: 111.22 s\n",
      "2024-12-24 02:39:08.812484: \n",
      "2024-12-24 02:39:08.812594: Epoch 863\n",
      "2024-12-24 02:39:08.812675: Current learning rate: 0.00167\n",
      "2024-12-24 02:41:00.044625: train_loss -0.8567\n",
      "2024-12-24 02:41:00.044770: val_loss -0.7451\n",
      "2024-12-24 02:41:00.044803: Pseudo dice [np.float32(0.8063)]\n",
      "2024-12-24 02:41:00.044839: Epoch time: 111.23 s\n",
      "2024-12-24 02:41:00.632351: \n",
      "2024-12-24 02:41:00.632701: Epoch 864\n",
      "2024-12-24 02:41:00.632890: Current learning rate: 0.00166\n",
      "2024-12-24 02:42:51.881276: train_loss -0.848\n",
      "2024-12-24 02:42:51.881486: val_loss -0.7286\n",
      "2024-12-24 02:42:51.881529: Pseudo dice [np.float32(0.7881)]\n",
      "2024-12-24 02:42:51.881565: Epoch time: 111.25 s\n",
      "2024-12-24 02:42:52.468327: \n",
      "2024-12-24 02:42:52.468446: Epoch 865\n",
      "2024-12-24 02:42:52.468520: Current learning rate: 0.00165\n",
      "2024-12-24 02:44:43.678152: train_loss -0.8639\n",
      "2024-12-24 02:44:43.678298: val_loss -0.7569\n",
      "2024-12-24 02:44:43.678346: Pseudo dice [np.float32(0.8219)]\n",
      "2024-12-24 02:44:43.678726: Epoch time: 111.21 s\n",
      "2024-12-24 02:44:44.263011: \n",
      "2024-12-24 02:44:44.263191: Epoch 866\n",
      "2024-12-24 02:44:44.263270: Current learning rate: 0.00164\n",
      "2024-12-24 02:46:35.512024: train_loss -0.8515\n",
      "2024-12-24 02:46:35.512164: val_loss -0.7617\n",
      "2024-12-24 02:46:35.512198: Pseudo dice [np.float32(0.7967)]\n",
      "2024-12-24 02:46:35.512235: Epoch time: 111.25 s\n",
      "2024-12-24 02:46:36.101765: \n",
      "2024-12-24 02:46:36.101869: Epoch 867\n",
      "2024-12-24 02:46:36.101940: Current learning rate: 0.00163\n",
      "2024-12-24 02:48:27.298629: train_loss -0.8642\n",
      "2024-12-24 02:48:27.298774: val_loss -0.7733\n",
      "2024-12-24 02:48:27.298808: Pseudo dice [np.float32(0.8281)]\n",
      "2024-12-24 02:48:27.298844: Epoch time: 111.2 s\n",
      "2024-12-24 02:48:27.298865: Yayy! New best EMA pseudo Dice: 0.8019999861717224\n",
      "2024-12-24 02:48:28.154453: \n",
      "2024-12-24 02:48:28.154561: Epoch 868\n",
      "2024-12-24 02:48:28.154634: Current learning rate: 0.00162\n",
      "2024-12-24 02:50:19.344908: train_loss -0.8345\n",
      "2024-12-24 02:50:19.345052: val_loss -0.7778\n",
      "2024-12-24 02:50:19.345086: Pseudo dice [np.float32(0.82)]\n",
      "2024-12-24 02:50:19.345124: Epoch time: 111.19 s\n",
      "2024-12-24 02:50:19.345146: Yayy! New best EMA pseudo Dice: 0.8037999868392944\n",
      "2024-12-24 02:50:20.163339: \n",
      "2024-12-24 02:50:20.163530: Epoch 869\n",
      "2024-12-24 02:50:20.163606: Current learning rate: 0.00161\n",
      "2024-12-24 02:52:11.388558: train_loss -0.8484\n",
      "2024-12-24 02:52:11.388729: val_loss -0.6769\n",
      "2024-12-24 02:52:11.388765: Pseudo dice [np.float32(0.7352)]\n",
      "2024-12-24 02:52:11.388802: Epoch time: 111.23 s\n",
      "2024-12-24 02:52:11.980168: \n",
      "2024-12-24 02:52:11.980444: Epoch 870\n",
      "2024-12-24 02:52:11.980603: Current learning rate: 0.00159\n",
      "2024-12-24 02:54:03.236693: train_loss -0.8521\n",
      "2024-12-24 02:54:03.236842: val_loss -0.7508\n",
      "2024-12-24 02:54:03.236879: Pseudo dice [np.float32(0.8098)]\n",
      "2024-12-24 02:54:03.236917: Epoch time: 111.26 s\n",
      "2024-12-24 02:54:03.825051: \n",
      "2024-12-24 02:54:03.825155: Epoch 871\n",
      "2024-12-24 02:54:03.825227: Current learning rate: 0.00158\n",
      "2024-12-24 02:55:55.033018: train_loss -0.8543\n",
      "2024-12-24 02:55:55.033232: val_loss -0.7372\n",
      "2024-12-24 02:55:55.033270: Pseudo dice [np.float32(0.7915)]\n",
      "2024-12-24 02:55:55.033306: Epoch time: 111.21 s\n",
      "2024-12-24 02:55:55.624317: \n",
      "2024-12-24 02:55:55.624421: Epoch 872\n",
      "2024-12-24 02:55:55.624495: Current learning rate: 0.00157\n",
      "2024-12-24 02:57:46.835840: train_loss -0.8343\n",
      "2024-12-24 02:57:46.835979: val_loss -0.7643\n",
      "2024-12-24 02:57:46.836011: Pseudo dice [np.float32(0.8218)]\n",
      "2024-12-24 02:57:46.836048: Epoch time: 111.21 s\n",
      "2024-12-24 02:57:47.829149: \n",
      "2024-12-24 02:57:47.829392: Epoch 873\n",
      "2024-12-24 02:57:47.829556: Current learning rate: 0.00156\n",
      "2024-12-24 02:59:39.088016: train_loss -0.8687\n",
      "2024-12-24 02:59:39.088230: val_loss -0.7301\n",
      "2024-12-24 02:59:39.088276: Pseudo dice [np.float32(0.7172)]\n",
      "2024-12-24 02:59:39.088314: Epoch time: 111.26 s\n",
      "2024-12-24 02:59:39.676296: \n",
      "2024-12-24 02:59:39.676651: Epoch 874\n",
      "2024-12-24 02:59:39.676740: Current learning rate: 0.00155\n",
      "2024-12-24 03:01:30.987904: train_loss -0.863\n",
      "2024-12-24 03:01:30.988051: val_loss -0.7675\n",
      "2024-12-24 03:01:30.988086: Pseudo dice [np.float32(0.8091)]\n",
      "2024-12-24 03:01:30.988124: Epoch time: 111.31 s\n",
      "2024-12-24 03:01:31.571959: \n",
      "2024-12-24 03:01:31.572321: Epoch 875\n",
      "2024-12-24 03:01:31.572404: Current learning rate: 0.00154\n",
      "2024-12-24 03:03:22.812917: train_loss -0.8597\n",
      "2024-12-24 03:03:22.813164: val_loss -0.7061\n",
      "2024-12-24 03:03:22.813218: Pseudo dice [np.float32(0.7444)]\n",
      "2024-12-24 03:03:22.813352: Epoch time: 111.24 s\n",
      "2024-12-24 03:03:23.408642: \n",
      "2024-12-24 03:03:23.408768: Epoch 876\n",
      "2024-12-24 03:03:23.408844: Current learning rate: 0.00153\n",
      "2024-12-24 03:05:14.640853: train_loss -0.8543\n",
      "2024-12-24 03:05:14.641031: val_loss -0.7909\n",
      "2024-12-24 03:05:14.641075: Pseudo dice [np.float32(0.8277)]\n",
      "2024-12-24 03:05:14.641115: Epoch time: 111.23 s\n",
      "2024-12-24 03:05:15.233729: \n",
      "2024-12-24 03:05:15.234142: Epoch 877\n",
      "2024-12-24 03:05:15.234233: Current learning rate: 0.00152\n",
      "2024-12-24 03:07:06.466411: train_loss -0.8427\n",
      "2024-12-24 03:07:06.466569: val_loss -0.7265\n",
      "2024-12-24 03:07:06.466603: Pseudo dice [np.float32(0.7688)]\n",
      "2024-12-24 03:07:06.466639: Epoch time: 111.23 s\n",
      "2024-12-24 03:07:07.046092: \n",
      "Epoch 8784 03:07:07.046205: \n",
      "2024-12-24 03:07:07.046342: Current learning rate: 0.00151\n",
      "2024-12-24 03:08:58.266119: train_loss -0.8467\n",
      "2024-12-24 03:08:58.266269: val_loss -0.7035\n",
      "2024-12-24 03:08:58.266376: Pseudo dice [np.float32(0.762)]\n",
      "2024-12-24 03:08:58.266511: Epoch time: 111.22 s\n",
      "2024-12-24 03:08:58.860210: \n",
      "2024-12-24 03:08:58.860370: Epoch 879\n",
      "2024-12-24 03:08:58.860443: Current learning rate: 0.00149\n",
      "2024-12-24 03:10:50.043326: train_loss -0.8436\n",
      "2024-12-24 03:10:50.043459: val_loss -0.7307\n",
      "2024-12-24 03:10:50.043492: Pseudo dice [np.float32(0.7936)]\n",
      "2024-12-24 03:10:50.043526: Epoch time: 111.18 s\n",
      "2024-12-24 03:10:50.633920: \n",
      "2024-12-24 03:10:50.634063: Epoch 880\n",
      "2024-12-24 03:10:50.634138: Current learning rate: 0.00148\n",
      "2024-12-24 03:12:41.715121: train_loss -0.8409\n",
      "2024-12-24 03:12:41.715269: val_loss -0.6573\n",
      "2024-12-24 03:12:41.715308: Pseudo dice [np.float32(0.7565)]\n",
      "2024-12-24 03:12:41.715347: Epoch time: 111.08 s\n",
      "2024-12-24 03:12:42.297942: \n",
      "2024-12-24 03:12:42.298119: Epoch 881\n",
      "2024-12-24 03:12:42.298197: Current learning rate: 0.00147\n",
      "2024-12-24 03:14:33.518288: train_loss -0.8428\n",
      "2024-12-24 03:14:33.518431: val_loss -0.6715\n",
      "2024-12-24 03:14:33.518465: Pseudo dice [np.float32(0.7819)]\n",
      "2024-12-24 03:14:33.518501: Epoch time: 111.22 s\n",
      "2024-12-24 03:14:34.099872: \n",
      "2024-12-24 03:14:34.100423: Epoch 882\n",
      "2024-12-24 03:14:34.100527: Current learning rate: 0.00146\n",
      "2024-12-24 03:16:25.333037: train_loss -0.8346\n",
      "2024-12-24 03:16:25.333257: val_loss -0.7511\n",
      "2024-12-24 03:16:25.333306: Pseudo dice [np.float32(0.8007)]\n",
      "2024-12-24 03:16:25.333343: Epoch time: 111.23 s\n",
      "2024-12-24 03:16:25.918433: \n",
      "2024-12-24 03:16:25.918720: Epoch 883\n",
      "2024-12-24 03:16:25.918834: Current learning rate: 0.00145\n",
      "2024-12-24 03:18:17.166852: train_loss -0.8492\n",
      "2024-12-24 03:18:17.166989: val_loss -0.7395\n",
      "2024-12-24 03:18:17.167021: Pseudo dice [np.float32(0.7927)]\n",
      "2024-12-24 03:18:17.167059: Epoch time: 111.25 s\n",
      "2024-12-24 03:18:17.760440: \n",
      "2024-12-24 03:18:17.760809: Epoch 884\n",
      "2024-12-24 03:18:17.760898: Current learning rate: 0.00144\n",
      "2024-12-24 03:20:08.995748: train_loss -0.8533\n",
      "2024-12-24 03:20:08.995892: val_loss -0.6942\n",
      "2024-12-24 03:20:08.996081: Pseudo dice [np.float32(0.7416)]\n",
      "2024-12-24 03:20:08.996156: Epoch time: 111.24 s\n",
      "2024-12-24 03:20:09.585973: \n",
      "2024-12-24 03:20:09.586087: Epoch 885\n",
      "2024-12-24 03:20:09.586159: Current learning rate: 0.00143\n",
      "2024-12-24 03:22:00.827165: train_loss -0.8386\n",
      "2024-12-24 03:22:00.827429: val_loss -0.7145\n",
      "2024-12-24 03:22:00.827478: Pseudo dice [np.float32(0.747)]\n",
      "2024-12-24 03:22:00.827603: Epoch time: 111.24 s\n",
      "2024-12-24 03:22:01.407137: \n",
      "2024-12-24 03:22:01.407480: Epoch 886\n",
      "2024-12-24 03:22:01.407624: Current learning rate: 0.00142\n",
      "2024-12-24 03:23:52.640461: train_loss -0.846\n",
      "2024-12-24 03:23:52.640597: val_loss -0.6742\n",
      "2024-12-24 03:23:52.640631: Pseudo dice [np.float32(0.7499)]\n",
      "2024-12-24 03:23:52.640665: Epoch time: 111.23 s\n",
      "2024-12-24 03:23:53.220517: \n",
      "2024-12-24 03:23:53.220614: Epoch 887\n",
      "2024-12-24 03:23:53.220682: Current learning rate: 0.00141\n",
      "2024-12-24 03:25:44.441303: train_loss -0.8424\n",
      "2024-12-24 03:25:44.441450: val_loss -0.7207\n",
      "2024-12-24 03:25:44.441486: Pseudo dice [np.float32(0.7407)]\n",
      "2024-12-24 03:25:44.441524: Epoch time: 111.22 s\n",
      "2024-12-24 03:25:45.037951: \n",
      "2024-12-24 03:25:45.038295: Epoch 888\n",
      "2024-12-24 03:25:45.038395: Current learning rate: 0.00139\n",
      "2024-12-24 03:27:36.277573: train_loss -0.855\n",
      "2024-12-24 03:27:36.277729: val_loss -0.6418\n",
      "2024-12-24 03:27:36.277765: Pseudo dice [np.float32(0.6966)]\n",
      "2024-12-24 03:27:36.277803: Epoch time: 111.24 s\n",
      "2024-12-24 03:27:36.865129: \n",
      "2024-12-24 03:27:36.865504: Epoch 889\n",
      "2024-12-24 03:27:36.865606: Current learning rate: 0.00138\n",
      "2024-12-24 03:29:28.098677: train_loss -0.8473\n",
      "2024-12-24 03:29:28.098814: val_loss -0.707\n",
      "2024-12-24 03:29:28.098849: Pseudo dice [np.float32(0.767)]\n",
      "2024-12-24 03:29:28.098887: Epoch time: 111.23 s\n",
      "2024-12-24 03:29:28.680293: \n",
      "2024-12-24 03:29:28.680395: Epoch 890\n",
      "2024-12-24 03:29:28.680470: Current learning rate: 0.00137\n",
      "2024-12-24 03:31:19.878996: train_loss -0.8521\n",
      "2024-12-24 03:31:19.879149: val_loss -0.6504\n",
      "2024-12-24 03:31:19.879185: Pseudo dice [np.float32(0.7495)]\n",
      "2024-12-24 03:31:19.879222: Epoch time: 111.2 s\n",
      "2024-12-24 03:31:20.460591: \n",
      "2024-12-24 03:31:20.461063: Epoch 891\n",
      "2024-12-24 03:31:20.461157: Current learning rate: 0.00136\n",
      "2024-12-24 03:33:11.715008: train_loss -0.8534\n",
      "2024-12-24 03:33:11.715167: val_loss -0.7377\n",
      "2024-12-24 03:33:11.715201: Pseudo dice [np.float32(0.7693)]\n",
      "2024-12-24 03:33:11.715235: Epoch time: 111.26 s\n",
      "2024-12-24 03:33:12.720097: \n",
      "2024-12-24 03:33:12.720482: Epoch 892\n",
      "2024-12-24 03:33:12.720602: Current learning rate: 0.00135\n",
      "2024-12-24 03:35:03.997770: train_loss -0.8474\n",
      "2024-12-24 03:35:03.997910: val_loss -0.7447\n",
      "2024-12-24 03:35:03.997946: Pseudo dice [np.float32(0.7913)]\n",
      "2024-12-24 03:35:03.997982: Epoch time: 111.28 s\n",
      "2024-12-24 03:35:04.585016: \n",
      "2024-12-24 03:35:04.585236: Epoch 893\n",
      "2024-12-24 03:35:04.585466: Current learning rate: 0.00134\n",
      "2024-12-24 03:36:55.819342: train_loss -0.8487\n",
      "2024-12-24 03:36:55.819491: val_loss -0.7577\n",
      "2024-12-24 03:36:55.819527: Pseudo dice [np.float32(0.82)]\n",
      "2024-12-24 03:36:55.819564: Epoch time: 111.23 s\n",
      "2024-12-24 03:36:56.405326: \n",
      "2024-12-24 03:36:56.405453: Epoch 894\n",
      "2024-12-24 03:36:56.405534: Current learning rate: 0.00133\n",
      "2024-12-24 03:38:47.690694: train_loss -0.84\n",
      "2024-12-24 03:38:47.691137: val_loss -0.7243\n",
      "2024-12-24 03:38:47.691182: Pseudo dice [np.float32(0.7779)]\n",
      "2024-12-24 03:38:47.691220: Epoch time: 111.29 s\n",
      "2024-12-24 03:38:48.287570: \n",
      "2024-12-24 03:38:48.287687: Epoch 895\n",
      "2024-12-24 03:38:48.287760: Current learning rate: 0.00132\n",
      "2024-12-24 03:40:39.529789: train_loss -0.8522\n",
      "2024-12-24 03:40:39.529932: val_loss -0.7165\n",
      "2024-12-24 03:40:39.529967: Pseudo dice [np.float32(0.7665)]\n",
      "2024-12-24 03:40:39.530003: Epoch time: 111.24 s\n",
      "2024-12-24 03:40:40.121078: \n",
      "2024-12-24 03:40:40.121197: Epoch 896\n",
      "2024-12-24 03:40:40.121267: Current learning rate: 0.0013\n",
      "2024-12-24 03:42:31.378624: train_loss -0.8638\n",
      "2024-12-24 03:42:31.378881: val_loss -0.7052\n",
      "2024-12-24 03:42:31.378944: Pseudo dice [np.float32(0.7335)]\n",
      "2024-12-24 03:42:31.378987: Epoch time: 111.26 s\n",
      "2024-12-24 03:42:31.968914: \n",
      "2024-12-24 03:42:31.969322: Epoch 897\n",
      "2024-12-24 03:42:31.969585: Current learning rate: 0.00129\n",
      "2024-12-24 03:44:23.242647: train_loss -0.8419\n",
      "2024-12-24 03:44:23.242785: val_loss -0.7067\n",
      "2024-12-24 03:44:23.242820: Pseudo dice [np.float32(0.779)]\n",
      "2024-12-24 03:44:23.242856: Epoch time: 111.27 s\n",
      "2024-12-24 03:44:23.839944: \n",
      "2024-12-24 03:44:23.840065: Epoch 898\n",
      "2024-12-24 03:44:23.840137: Current learning rate: 0.00128\n",
      "2024-12-24 03:46:15.051467: train_loss -0.8506\n",
      "2024-12-24 03:46:15.051648: val_loss -0.7041\n",
      "2024-12-24 03:46:15.051728: Pseudo dice [np.float32(0.7678)]\n",
      "2024-12-24 03:46:15.051772: Epoch time: 111.21 s\n",
      "2024-12-24 03:46:15.649126: \n",
      "2024-12-24 03:46:15.649758: Epoch 899\n",
      "2024-12-24 03:46:15.649896: Current learning rate: 0.00127\n",
      "2024-12-24 03:48:06.922040: train_loss -0.8572\n",
      "2024-12-24 03:48:06.922176: val_loss -0.731\n",
      "2024-12-24 03:48:06.922336: Pseudo dice [np.float32(0.8061)]\n",
      "2024-12-24 03:48:06.922429: Epoch time: 111.27 s\n",
      "2024-12-24 03:48:07.766037: \n",
      "2024-12-24 03:48:07.766493: Epoch 900\n",
      "2024-12-24 03:48:07.766644: Current learning rate: 0.00126\n",
      "2024-12-24 03:49:58.968452: train_loss -0.8451\n",
      "2024-12-24 03:49:58.968595: val_loss -0.6867\n",
      "2024-12-24 03:49:58.968627: Pseudo dice [np.float32(0.7323)]\n",
      "2024-12-24 03:49:58.968661: Epoch time: 111.2 s\n",
      "2024-12-24 03:49:59.565140: \n",
      "2024-12-24 03:49:59.565369: Epoch 901\n",
      "2024-12-24 03:49:59.565518: Current learning rate: 0.00125\n",
      "2024-12-24 03:51:50.875911: train_loss -0.8608\n",
      "2024-12-24 03:51:50.876440: val_loss -0.707\n",
      "2024-12-24 03:51:50.876518: Pseudo dice [np.float32(0.7919)]\n",
      "2024-12-24 03:51:50.876564: Epoch time: 111.31 s\n",
      "2024-12-24 03:51:51.467634: \n",
      "2024-12-24 03:51:51.468028: Epoch 902\n",
      "2024-12-24 03:51:51.468191: Current learning rate: 0.00124\n",
      "2024-12-24 03:53:42.710210: train_loss -0.8664\n",
      "2024-12-24 03:53:42.710360: val_loss -0.7183\n",
      "2024-12-24 03:53:42.710398: Pseudo dice [np.float32(0.7696)]\n",
      "2024-12-24 03:53:42.710434: Epoch time: 111.24 s\n",
      "2024-12-24 03:53:43.288090: \n",
      "Epoch 9034 03:53:43.288202: \n",
      "2024-12-24 03:53:43.288326: Current learning rate: 0.00122\n",
      "2024-12-24 03:55:34.573696: train_loss -0.865\n",
      "2024-12-24 03:55:34.573828: val_loss -0.6809\n",
      "2024-12-24 03:55:34.573864: Pseudo dice [np.float32(0.7212)]\n",
      "2024-12-24 03:55:34.573901: Epoch time: 111.29 s\n",
      "2024-12-24 03:55:35.162974: \n",
      "2024-12-24 03:55:35.163091: Epoch 904\n",
      "2024-12-24 03:55:35.163161: Current learning rate: 0.00121\n",
      "2024-12-24 03:57:26.391658: train_loss -0.8614\n",
      "2024-12-24 03:57:26.391893: val_loss -0.7309\n",
      "2024-12-24 03:57:26.391931: Pseudo dice [np.float32(0.8057)]\n",
      "2024-12-24 03:57:26.391969: Epoch time: 111.23 s\n",
      "2024-12-24 03:57:26.980574: \n",
      "2024-12-24 03:57:26.980960: Epoch 905\n",
      "2024-12-24 03:57:26.981194: Current learning rate: 0.0012\n",
      "2024-12-24 03:59:18.151248: train_loss -0.8659\n",
      "2024-12-24 03:59:18.151396: val_loss -0.6839\n",
      "2024-12-24 03:59:18.151433: Pseudo dice [np.float32(0.6867)]\n",
      "2024-12-24 03:59:18.151471: Epoch time: 111.17 s\n",
      "2024-12-24 03:59:18.735182: \n",
      "2024-12-24 03:59:18.735482: Epoch 906\n",
      "2024-12-24 03:59:18.735564: Current learning rate: 0.00119\n",
      "2024-12-24 04:01:10.059494: train_loss -0.8565\n",
      "2024-12-24 04:01:10.059641: val_loss -0.7737\n",
      "2024-12-24 04:01:10.059678: Pseudo dice [np.float32(0.8229)]\n",
      "2024-12-24 04:01:10.059714: Epoch time: 111.32 s\n",
      "2024-12-24 04:01:10.678320: \n",
      "2024-12-24 04:01:10.678606: Epoch 907\n",
      "2024-12-24 04:01:10.678761: Current learning rate: 0.00118\n",
      "2024-12-24 04:03:01.956198: train_loss -0.8573\n",
      "2024-12-24 04:03:01.956449: val_loss -0.6471\n",
      "2024-12-24 04:03:01.956496: Pseudo dice [np.float32(0.7009)]\n",
      "2024-12-24 04:03:01.956535: Epoch time: 111.28 s\n",
      "2024-12-24 04:03:02.545987: \n",
      "2024-12-24 04:03:02.546089: Epoch 908\n",
      "2024-12-24 04:03:02.546162: Current learning rate: 0.00117\n",
      "2024-12-24 04:04:53.732833: train_loss -0.8316\n",
      "2024-12-24 04:04:53.733047: val_loss -0.6911\n",
      "2024-12-24 04:04:53.733081: Pseudo dice [np.float32(0.7509)]\n",
      "2024-12-24 04:04:53.733115: Epoch time: 111.19 s\n",
      "2024-12-24 04:04:54.334624: \n",
      "2024-12-24 04:04:54.334962: Epoch 909\n",
      "2024-12-24 04:04:54.335051: Current learning rate: 0.00116\n",
      "2024-12-24 04:06:45.553665: train_loss -0.8664\n",
      "2024-12-24 04:06:45.553880: val_loss -0.668\n",
      "2024-12-24 04:06:45.553916: Pseudo dice [np.float32(0.7365)]\n",
      "2024-12-24 04:06:45.553953: Epoch time: 111.22 s\n",
      "2024-12-24 04:06:46.139623: \n",
      "2024-12-24 04:06:46.139736: Epoch 910\n",
      "2024-12-24 04:06:46.139910: Current learning rate: 0.00115\n",
      "2024-12-24 04:08:37.372557: train_loss -0.8637\n",
      "2024-12-24 04:08:37.372706: val_loss -0.6928\n",
      "2024-12-24 04:08:37.372740: Pseudo dice [np.float32(0.7348)]\n",
      "2024-12-24 04:08:37.372777: Epoch time: 111.23 s\n",
      "2024-12-24 04:08:38.411992: \n",
      "2024-12-24 04:08:38.412408: Epoch 911\n",
      "2024-12-24 04:08:38.412677: Current learning rate: 0.00113\n",
      "2024-12-24 04:10:29.656008: train_loss -0.8585\n",
      "2024-12-24 04:10:29.656204: val_loss -0.7091\n",
      "2024-12-24 04:10:29.656245: Pseudo dice [np.float32(0.79)]\n",
      "2024-12-24 04:10:29.656287: Epoch time: 111.24 s\n",
      "2024-12-24 04:10:30.251631: \n",
      "2024-12-24 04:10:30.251849: Epoch 912\n",
      "2024-12-24 04:10:30.251936: Current learning rate: 0.00112\n",
      "2024-12-24 04:12:21.478821: train_loss -0.8709\n",
      "2024-12-24 04:12:21.478953: val_loss -0.7347\n",
      "2024-12-24 04:12:21.478987: Pseudo dice [np.float32(0.771)]\n",
      "2024-12-24 04:12:21.479022: Epoch time: 111.23 s\n",
      "2024-12-24 04:12:22.071379: \n",
      "2024-12-24 04:12:22.071562: Epoch 913\n",
      "2024-12-24 04:12:22.071639: Current learning rate: 0.00111\n",
      "2024-12-24 04:14:13.328067: train_loss -0.8498\n",
      "2024-12-24 04:14:13.328359: val_loss -0.6953\n",
      "2024-12-24 04:14:13.328445: Pseudo dice [np.float32(0.7815)]\n",
      "2024-12-24 04:14:13.328519: Epoch time: 111.26 s\n",
      "2024-12-24 04:14:13.912986: \n",
      "2024-12-24 04:14:13.913232: Epoch 914\n",
      "2024-12-24 04:14:13.913371: Current learning rate: 0.0011\n",
      "2024-12-24 04:16:05.157812: train_loss -0.8478\n",
      "2024-12-24 04:16:05.158211: val_loss -0.7717\n",
      "2024-12-24 04:16:05.158254: Pseudo dice [np.float32(0.8022)]\n",
      "2024-12-24 04:16:05.158293: Epoch time: 111.25 s\n",
      "2024-12-24 04:16:05.749344: \n",
      "2024-12-24 04:16:05.749586: Epoch 915\n",
      "2024-12-24 04:16:05.749739: Current learning rate: 0.00109\n",
      "2024-12-24 04:17:56.962919: train_loss -0.854\n",
      "2024-12-24 04:17:56.963063: val_loss -0.6944\n",
      "2024-12-24 04:17:56.963096: Pseudo dice [np.float32(0.7481)]\n",
      "2024-12-24 04:17:56.963132: Epoch time: 111.21 s\n",
      "2024-12-24 04:17:57.559774: \n",
      "2024-12-24 04:17:57.560109: Epoch 916\n",
      "2024-12-24 04:17:57.560195: Current learning rate: 0.00108\n",
      "2024-12-24 04:19:48.790452: train_loss -0.8598\n",
      "2024-12-24 04:19:48.790616: val_loss -0.6912\n",
      "2024-12-24 04:19:48.790650: Pseudo dice [np.float32(0.7522)]\n",
      "2024-12-24 04:19:48.790689: Epoch time: 111.23 s\n",
      "2024-12-24 04:19:49.375484: \n",
      "2024-12-24 04:19:49.375628: Epoch 917\n",
      "2024-12-24 04:19:49.375784: Current learning rate: 0.00106\n",
      "2024-12-24 04:21:40.609013: train_loss -0.8309\n",
      "2024-12-24 04:21:40.609159: val_loss -0.7598\n",
      "2024-12-24 04:21:40.609195: Pseudo dice [np.float32(0.8207)]\n",
      "2024-12-24 04:21:40.609242: Epoch time: 111.23 s\n",
      "2024-12-24 04:21:41.196523: \n",
      "2024-12-24 04:21:41.196716: Epoch 918\n",
      "2024-12-24 04:21:41.196800: Current learning rate: 0.00105\n",
      "2024-12-24 04:23:32.448027: train_loss -0.8616\n",
      "2024-12-24 04:23:32.448398: val_loss -0.6907\n",
      "2024-12-24 04:23:32.448565: Pseudo dice [np.float32(0.7328)]\n",
      "2024-12-24 04:23:32.448622: Epoch time: 111.25 s\n",
      "2024-12-24 04:23:33.040243: \n",
      "2024-12-24 04:23:33.040683: Epoch 919\n",
      "2024-12-24 04:23:33.040765: Current learning rate: 0.00104\n",
      "2024-12-24 04:25:24.144146: train_loss -0.8488\n",
      "2024-12-24 04:25:24.144333: val_loss -0.7759\n",
      "2024-12-24 04:25:24.144666: Pseudo dice [np.float32(0.8073)]\n",
      "2024-12-24 04:25:24.144839: Epoch time: 111.1 s\n",
      "2024-12-24 04:25:24.731841: \n",
      "2024-12-24 04:25:24.732024: Epoch 920\n",
      "2024-12-24 04:25:24.732105: Current learning rate: 0.00103\n",
      "2024-12-24 04:27:15.950273: train_loss -0.8768\n",
      "2024-12-24 04:27:15.950627: val_loss -0.7185\n",
      "2024-12-24 04:27:15.950696: Pseudo dice [np.float32(0.7851)]\n",
      "2024-12-24 04:27:15.950741: Epoch time: 111.22 s\n",
      "2024-12-24 04:27:16.547450: \n",
      "2024-12-24 04:27:16.547631: Epoch 921\n",
      "2024-12-24 04:27:16.547704: Current learning rate: 0.00102\n",
      "2024-12-24 04:29:07.777617: train_loss -0.8451\n",
      "2024-12-24 04:29:07.777767: val_loss -0.7237\n",
      "2024-12-24 04:29:07.777801: Pseudo dice [np.float32(0.7803)]\n",
      "2024-12-24 04:29:07.777838: Epoch time: 111.23 s\n",
      "2024-12-24 04:29:08.363197: \n",
      "2024-12-24 04:29:08.363387: Epoch 922\n",
      "2024-12-24 04:29:08.363466: Current learning rate: 0.00101\n",
      "2024-12-24 04:30:59.616060: train_loss -0.861\n",
      "2024-12-24 04:30:59.616207: val_loss -0.7077\n",
      "2024-12-24 04:30:59.616241: Pseudo dice [np.float32(0.7762)]\n",
      "2024-12-24 04:30:59.616277: Epoch time: 111.25 s\n",
      "2024-12-24 04:31:00.208660: \n",
      "2024-12-24 04:31:00.208855: Epoch 923\n",
      "2024-12-24 04:31:00.208932: Current learning rate: 0.001\n",
      "2024-12-24 04:32:51.456301: train_loss -0.8668\n",
      "2024-12-24 04:32:51.456733: val_loss -0.7191\n",
      "2024-12-24 04:32:51.456860: Pseudo dice [np.float32(0.7741)]\n",
      "2024-12-24 04:32:51.456904: Epoch time: 111.25 s\n",
      "2024-12-24 04:32:52.037903: \n",
      "2024-12-24 04:32:52.038133: Epoch 924\n",
      "2024-12-24 04:32:52.038337: Current learning rate: 0.00098\n",
      "2024-12-24 04:34:43.234792: train_loss -0.8667\n",
      "2024-12-24 04:34:43.235017: val_loss -0.7411\n",
      "2024-12-24 04:34:43.235059: Pseudo dice [np.float32(0.7962)]\n",
      "2024-12-24 04:34:43.235111: Epoch time: 111.2 s\n",
      "2024-12-24 04:34:43.820050: \n",
      "2024-12-24 04:34:43.820381: Epoch 925\n",
      "2024-12-24 04:34:43.820537: Current learning rate: 0.00097\n",
      "2024-12-24 04:36:35.037830: train_loss -0.8629\n",
      "2024-12-24 04:36:35.038046: val_loss -0.7642\n",
      "2024-12-24 04:36:35.038085: Pseudo dice [np.float32(0.8086)]\n",
      "2024-12-24 04:36:35.038158: Epoch time: 111.22 s\n",
      "2024-12-24 04:36:35.631265: \n",
      "2024-12-24 04:36:35.631660: Epoch 926\n",
      "2024-12-24 04:36:35.631749: Current learning rate: 0.00096\n",
      "2024-12-24 04:38:26.808520: train_loss -0.8718\n",
      "2024-12-24 04:38:26.809004: val_loss -0.7109\n",
      "2024-12-24 04:38:26.809048: Pseudo dice [np.float32(0.7892)]\n",
      "2024-12-24 04:38:26.809090: Epoch time: 111.18 s\n",
      "2024-12-24 04:38:27.389375: \n",
      "2024-12-24 04:38:27.389473: Epoch 927\n",
      "2024-12-24 04:38:27.389544: Current learning rate: 0.00095\n",
      "2024-12-24 04:40:18.590104: train_loss -0.8533\n",
      "2024-12-24 04:40:18.590312: val_loss -0.7085\n",
      "2024-12-24 04:40:18.590357: Pseudo dice [np.float32(0.7579)]\n",
      "2024-12-24 04:40:18.590394: Epoch time: 111.2 s\n",
      "2024-12-24 04:40:19.181411: \n",
      "Epoch 9284 04:40:19.181821: \n",
      "2024-12-24 04:40:19.181977: Current learning rate: 0.00094\n",
      "2024-12-24 04:42:10.393839: train_loss -0.8582\n",
      "2024-12-24 04:42:10.394011: val_loss -0.6243\n",
      "2024-12-24 04:42:10.394049: Pseudo dice [np.float32(0.7157)]\n",
      "2024-12-24 04:42:10.394085: Epoch time: 111.21 s\n",
      "2024-12-24 04:42:10.991757: \n",
      "2024-12-24 04:42:10.991922: Epoch 929\n",
      "2024-12-24 04:42:10.991995: Current learning rate: 0.00092\n",
      "2024-12-24 04:44:02.229505: train_loss -0.8714\n",
      "2024-12-24 04:44:02.229672: val_loss -0.7267\n",
      "2024-12-24 04:44:02.229711: Pseudo dice [np.float32(0.7627)]\n",
      "2024-12-24 04:44:02.229748: Epoch time: 111.24 s\n",
      "2024-12-24 04:44:03.239887: \n",
      "2024-12-24 04:44:03.240017: Epoch 930\n",
      "2024-12-24 04:44:03.240103: Current learning rate: 0.00091\n",
      "2024-12-24 04:45:54.498253: train_loss -0.8471\n",
      "2024-12-24 04:45:54.498424: val_loss -0.7503\n",
      "2024-12-24 04:45:54.498459: Pseudo dice [np.float32(0.7995)]\n",
      "2024-12-24 04:45:54.498494: Epoch time: 111.26 s\n",
      "2024-12-24 04:45:55.093511: \n",
      "2024-12-24 04:45:55.093702: Epoch 931\n",
      "2024-12-24 04:45:55.093775: Current learning rate: 0.0009\n",
      "2024-12-24 04:47:46.337794: train_loss -0.86\n",
      "2024-12-24 04:47:46.338042: val_loss -0.7288\n",
      "2024-12-24 04:47:46.338082: Pseudo dice [np.float32(0.7746)]\n",
      "2024-12-24 04:47:46.338119: Epoch time: 111.24 s\n",
      "2024-12-24 04:47:46.924220: \n",
      "2024-12-24 04:47:46.924585: Epoch 932\n",
      "2024-12-24 04:47:46.924720: Current learning rate: 0.00089\n",
      "2024-12-24 04:49:38.188237: train_loss -0.8558\n",
      "2024-12-24 04:49:38.188423: val_loss -0.7006\n",
      "2024-12-24 04:49:38.188465: Pseudo dice [np.float32(0.7541)]\n",
      "2024-12-24 04:49:38.188504: Epoch time: 111.26 s\n",
      "2024-12-24 04:49:38.818285: \n",
      "2024-12-24 04:49:38.818459: Epoch 933\n",
      "2024-12-24 04:49:38.818542: Current learning rate: 0.00088\n",
      "2024-12-24 04:51:30.023574: train_loss -0.8483\n",
      "2024-12-24 04:51:30.023722: val_loss -0.7107\n",
      "2024-12-24 04:51:30.023760: Pseudo dice [np.float32(0.7263)]\n",
      "2024-12-24 04:51:30.023797: Epoch time: 111.21 s\n",
      "2024-12-24 04:51:30.609493: \n",
      "2024-12-24 04:51:30.609753: Epoch 934\n",
      "2024-12-24 04:51:30.610014: Current learning rate: 0.00087\n",
      "2024-12-24 04:53:21.814143: train_loss -0.8473\n",
      "2024-12-24 04:53:21.814342: val_loss -0.721\n",
      "2024-12-24 04:53:21.814373: Pseudo dice [np.float32(0.7684)]\n",
      "2024-12-24 04:53:21.814409: Epoch time: 111.21 s\n",
      "2024-12-24 04:53:22.398781: \n",
      "2024-12-24 04:53:22.399186: Epoch 935\n",
      "2024-12-24 04:53:22.399339: Current learning rate: 0.00085\n",
      "2024-12-24 04:55:13.597322: train_loss -0.8686\n",
      "2024-12-24 04:55:13.597475: val_loss -0.723\n",
      "2024-12-24 04:55:13.597510: Pseudo dice [np.float32(0.8009)]\n",
      "2024-12-24 04:55:13.597545: Epoch time: 111.2 s\n",
      "2024-12-24 04:55:14.190088: \n",
      "2024-12-24 04:55:14.190479: Epoch 936\n",
      "2024-12-24 04:55:14.190621: Current learning rate: 0.00084\n",
      "2024-12-24 04:57:05.410776: train_loss -0.8724\n",
      "2024-12-24 04:57:05.410941: val_loss -0.728\n",
      "2024-12-24 04:57:05.410977: Pseudo dice [np.float32(0.79)]\n",
      "2024-12-24 04:57:05.411012: Epoch time: 111.22 s\n",
      "2024-12-24 04:57:06.003168: \n",
      "2024-12-24 04:57:06.003280: Epoch 937\n",
      "2024-12-24 04:57:06.003354: Current learning rate: 0.00083\n",
      "2024-12-24 04:58:57.217389: train_loss -0.8679\n",
      "2024-12-24 04:58:57.217540: val_loss -0.7519\n",
      "2024-12-24 04:58:57.217576: Pseudo dice [np.float32(0.8047)]\n",
      "2024-12-24 04:58:57.217613: Epoch time: 111.21 s\n",
      "2024-12-24 04:58:57.802697: \n",
      "2024-12-24 04:58:57.802891: Epoch 938\n",
      "2024-12-24 04:58:57.802985: Current learning rate: 0.00082\n",
      "2024-12-24 05:00:49.108437: train_loss -0.8729\n",
      "2024-12-24 05:00:49.108799: val_loss -0.657\n",
      "2024-12-24 05:00:49.108859: Pseudo dice [np.float32(0.7466)]\n",
      "2024-12-24 05:00:49.108904: Epoch time: 111.31 s\n",
      "2024-12-24 05:00:49.706329: \n",
      "2024-12-24 05:00:49.706796: Epoch 939\n",
      "2024-12-24 05:00:49.706943: Current learning rate: 0.00081\n",
      "2024-12-24 05:02:40.967324: train_loss -0.8643\n",
      "2024-12-24 05:02:40.967477: val_loss -0.7243\n",
      "2024-12-24 05:02:40.967511: Pseudo dice [np.float32(0.7692)]\n",
      "2024-12-24 05:02:40.967547: Epoch time: 111.26 s\n",
      "2024-12-24 05:02:41.562045: \n",
      "2024-12-24 05:02:41.562161: Epoch 940\n",
      "2024-12-24 05:02:41.562237: Current learning rate: 0.00079\n",
      "2024-12-24 05:04:32.758240: train_loss -0.8674\n",
      "2024-12-24 05:04:32.758372: val_loss -0.6735\n",
      "2024-12-24 05:04:32.758406: Pseudo dice [np.float32(0.7284)]\n",
      "2024-12-24 05:04:32.758440: Epoch time: 111.2 s\n",
      "2024-12-24 05:04:33.337875: \n",
      "2024-12-24 05:04:33.338288: Epoch 941\n",
      "2024-12-24 05:04:33.338381: Current learning rate: 0.00078\n",
      "2024-12-24 05:06:24.558791: train_loss -0.8526\n",
      "2024-12-24 05:06:24.558960: val_loss -0.7443\n",
      "2024-12-24 05:06:24.558995: Pseudo dice [np.float32(0.8121)]\n",
      "2024-12-24 05:06:24.559031: Epoch time: 111.22 s\n",
      "2024-12-24 05:06:25.150357: \n",
      "2024-12-24 05:06:25.150800: Epoch 942\n",
      "2024-12-24 05:06:25.150901: Current learning rate: 0.00077\n",
      "2024-12-24 05:08:16.384563: train_loss -0.865\n",
      "2024-12-24 05:08:16.384834: val_loss -0.6965\n",
      "2024-12-24 05:08:16.384924: Pseudo dice [np.float32(0.7628)]\n",
      "2024-12-24 05:08:16.384969: Epoch time: 111.23 s\n",
      "2024-12-24 05:08:16.977028: \n",
      "2024-12-24 05:08:16.977459: Epoch 943\n",
      "2024-12-24 05:08:16.977544: Current learning rate: 0.00076\n",
      "2024-12-24 05:10:08.193031: train_loss -0.8556\n",
      "2024-12-24 05:10:08.193217: val_loss -0.6281\n",
      "2024-12-24 05:10:08.193317: Pseudo dice [np.float32(0.7447)]\n",
      "2024-12-24 05:10:08.193366: Epoch time: 111.22 s\n",
      "2024-12-24 05:10:08.789661: \n",
      "2024-12-24 05:10:08.789810: Epoch 944\n",
      "2024-12-24 05:10:08.789888: Current learning rate: 0.00075\n",
      "2024-12-24 05:12:00.051604: train_loss -0.8562\n",
      "2024-12-24 05:12:00.051759: val_loss -0.6821\n",
      "2024-12-24 05:12:00.051797: Pseudo dice [np.float32(0.7644)]\n",
      "2024-12-24 05:12:00.051832: Epoch time: 111.26 s\n",
      "2024-12-24 05:12:00.632810: \n",
      "2024-12-24 05:12:00.632908: Epoch 945\n",
      "2024-12-24 05:12:00.632994: Current learning rate: 0.00074\n",
      "2024-12-24 05:13:51.872974: train_loss -0.8561\n",
      "2024-12-24 05:13:51.873207: val_loss -0.7114\n",
      "2024-12-24 05:13:51.873270: Pseudo dice [np.float32(0.7837)]\n",
      "2024-12-24 05:13:51.873312: Epoch time: 111.24 s\n",
      "2024-12-24 05:13:52.456877: \n",
      "2024-12-24 05:13:52.457191: Epoch 946\n",
      "2024-12-24 05:13:52.457354: Current learning rate: 0.00072\n",
      "2024-12-24 05:15:43.659936: train_loss -0.8586\n",
      "2024-12-24 05:15:43.660084: val_loss -0.6867\n",
      "2024-12-24 05:15:43.660119: Pseudo dice [np.float32(0.7356)]\n",
      "2024-12-24 05:15:43.660155: Epoch time: 111.2 s\n",
      "2024-12-24 05:15:44.257135: \n",
      "2024-12-24 05:15:44.257298: Epoch 947\n",
      "2024-12-24 05:15:44.257383: Current learning rate: 0.00071\n",
      "2024-12-24 05:17:35.472389: train_loss -0.8551\n",
      "2024-12-24 05:17:35.472618: val_loss -0.6917\n",
      "2024-12-24 05:17:35.472656: Pseudo dice [np.float32(0.7475)]\n",
      "2024-12-24 05:17:35.472693: Epoch time: 111.22 s\n",
      "2024-12-24 05:17:36.063904: \n",
      "2024-12-24 05:17:36.064006: Epoch 948\n",
      "2024-12-24 05:17:36.064079: Current learning rate: 0.0007\n",
      "2024-12-24 05:19:27.308416: train_loss -0.8489\n",
      "2024-12-24 05:19:27.308619: val_loss -0.779\n",
      "2024-12-24 05:19:27.308662: Pseudo dice [np.float32(0.822)]\n",
      "2024-12-24 05:19:27.308727: Epoch time: 111.25 s\n",
      "2024-12-24 05:19:28.322263: \n",
      "2024-12-24 05:19:28.322565: Epoch 949\n",
      "2024-12-24 05:19:28.322858: Current learning rate: 0.00069\n",
      "2024-12-24 05:21:19.556452: train_loss -0.8614\n",
      "2024-12-24 05:21:19.556600: val_loss -0.6202\n",
      "2024-12-24 05:21:19.556633: Pseudo dice [np.float32(0.7456)]\n",
      "2024-12-24 05:21:19.556672: Epoch time: 111.23 s\n",
      "2024-12-24 05:21:20.396193: \n",
      "2024-12-24 05:21:20.396517: Epoch 950\n",
      "2024-12-24 05:21:20.396750: Current learning rate: 0.00067\n",
      "2024-12-24 05:23:11.620668: train_loss -0.8628\n",
      "2024-12-24 05:23:11.620826: val_loss -0.6426\n",
      "2024-12-24 05:23:11.621316: Pseudo dice [np.float32(0.7224)]\n",
      "2024-12-24 05:23:11.621413: Epoch time: 111.23 s\n",
      "2024-12-24 05:23:12.205596: \n",
      "2024-12-24 05:23:12.205930: Epoch 951\n",
      "2024-12-24 05:23:12.206025: Current learning rate: 0.00066\n",
      "2024-12-24 05:25:03.436023: train_loss -0.8556\n",
      "2024-12-24 05:25:03.436249: val_loss -0.7149\n",
      "2024-12-24 05:25:03.436312: Pseudo dice [np.float32(0.8036)]\n",
      "2024-12-24 05:25:03.436356: Epoch time: 111.23 s\n",
      "2024-12-24 05:25:04.067608: \n",
      "2024-12-24 05:25:04.068042: Epoch 952\n",
      "2024-12-24 05:25:04.068131: Current learning rate: 0.00065\n",
      "2024-12-24 05:26:55.325060: train_loss -0.8725\n",
      "2024-12-24 05:26:55.325202: val_loss -0.7063\n",
      "2024-12-24 05:26:55.325237: Pseudo dice [np.float32(0.7912)]\n",
      "2024-12-24 05:26:55.325274: Epoch time: 111.26 s\n",
      "2024-12-24 05:26:55.922864: \n",
      "Epoch 9534 05:26:55.922981: \n",
      "2024-12-24 05:26:55.923112: Current learning rate: 0.00064\n",
      "2024-12-24 05:28:47.137976: train_loss -0.8705\n",
      "2024-12-24 05:28:47.138201: val_loss -0.6579\n",
      "2024-12-24 05:28:47.138237: Pseudo dice [np.float32(0.6964)]\n",
      "2024-12-24 05:28:47.138271: Epoch time: 111.22 s\n",
      "2024-12-24 05:28:47.738245: \n",
      "2024-12-24 05:28:47.738431: Epoch 954\n",
      "2024-12-24 05:28:47.738515: Current learning rate: 0.00063\n",
      "2024-12-24 05:30:38.980053: train_loss -0.8715\n",
      "2024-12-24 05:30:38.980197: val_loss -0.6756\n",
      "2024-12-24 05:30:38.980233: Pseudo dice [np.float32(0.7424)]\n",
      "2024-12-24 05:30:38.980269: Epoch time: 111.24 s\n",
      "2024-12-24 05:30:39.573407: \n",
      "2024-12-24 05:30:39.573556: Epoch 955\n",
      "2024-12-24 05:30:39.573645: Current learning rate: 0.00061\n",
      "2024-12-24 05:32:30.845580: train_loss -0.868\n",
      "2024-12-24 05:32:30.845794: val_loss -0.7614\n",
      "2024-12-24 05:32:30.845827: Pseudo dice [np.float32(0.8142)]\n",
      "2024-12-24 05:32:30.845863: Epoch time: 111.27 s\n",
      "2024-12-24 05:32:31.446257: \n",
      "2024-12-24 05:32:31.446587: Epoch 956\n",
      "2024-12-24 05:32:31.446872: Current learning rate: 0.0006\n",
      "2024-12-24 05:34:22.707246: train_loss -0.8661\n",
      "2024-12-24 05:34:22.707558: val_loss -0.7453\n",
      "2024-12-24 05:34:22.707681: Pseudo dice [np.float32(0.7917)]\n",
      "2024-12-24 05:34:22.707729: Epoch time: 111.26 s\n",
      "2024-12-24 05:34:23.308354: \n",
      "2024-12-24 05:34:23.308460: Epoch 957\n",
      "2024-12-24 05:34:23.308530: Current learning rate: 0.00059\n",
      "2024-12-24 05:36:14.417902: train_loss -0.8604\n",
      "2024-12-24 05:36:14.418070: val_loss -0.6826\n",
      "2024-12-24 05:36:14.418106: Pseudo dice [np.float32(0.7472)]\n",
      "2024-12-24 05:36:14.418139: Epoch time: 111.11 s\n",
      "2024-12-24 05:36:15.013914: \n",
      "2024-12-24 05:36:15.014379: Epoch 958\n",
      "2024-12-24 05:36:15.014480: Current learning rate: 0.00058\n",
      "2024-12-24 05:38:06.233975: train_loss -0.8468\n",
      "2024-12-24 05:38:06.234136: val_loss -0.7229\n",
      "2024-12-24 05:38:06.234174: Pseudo dice [np.float32(0.7957)]\n",
      "2024-12-24 05:38:06.234212: Epoch time: 111.22 s\n",
      "2024-12-24 05:38:06.831696: \n",
      "2024-12-24 05:38:06.831813: Epoch 959\n",
      "2024-12-24 05:38:06.831884: Current learning rate: 0.00056\n",
      "2024-12-24 05:39:58.013413: train_loss -0.8574\n",
      "2024-12-24 05:39:58.013606: val_loss -0.7475\n",
      "2024-12-24 05:39:58.013641: Pseudo dice [np.float32(0.8079)]\n",
      "2024-12-24 05:39:58.013680: Epoch time: 111.18 s\n",
      "2024-12-24 05:39:58.603822: \n",
      "2024-12-24 05:39:58.603941: Epoch 960\n",
      "2024-12-24 05:39:58.604015: Current learning rate: 0.00055\n",
      "2024-12-24 05:41:49.761829: train_loss -0.8608\n",
      "2024-12-24 05:41:49.762010: val_loss -0.7149\n",
      "2024-12-24 05:41:49.762058: Pseudo dice [np.float32(0.7663)]\n",
      "2024-12-24 05:41:49.762117: Epoch time: 111.16 s\n",
      "2024-12-24 05:41:50.361613: \n",
      "2024-12-24 05:41:50.361830: Epoch 961\n",
      "2024-12-24 05:41:50.361914: Current learning rate: 0.00054\n",
      "2024-12-24 05:43:41.554862: train_loss -0.8663\n",
      "2024-12-24 05:43:41.555038: val_loss -0.7448\n",
      "2024-12-24 05:43:41.555109: Pseudo dice [np.float32(0.8053)]\n",
      "2024-12-24 05:43:41.555155: Epoch time: 111.19 s\n",
      "2024-12-24 05:43:42.146717: \n",
      "2024-12-24 05:43:42.147123: Epoch 962\n",
      "2024-12-24 05:43:42.147221: Current learning rate: 0.00053\n",
      "2024-12-24 05:45:33.372319: train_loss -0.8551\n",
      "2024-12-24 05:45:33.372461: val_loss -0.724\n",
      "2024-12-24 05:45:33.372496: Pseudo dice [np.float32(0.789)]\n",
      "2024-12-24 05:45:33.372533: Epoch time: 111.23 s\n",
      "2024-12-24 05:45:33.970491: \n",
      "2024-12-24 05:45:33.970843: Epoch 963\n",
      "2024-12-24 05:45:33.970927: Current learning rate: 0.00051\n",
      "2024-12-24 05:47:25.144292: train_loss -0.8539\n",
      "2024-12-24 05:47:25.144433: val_loss -0.6728\n",
      "2024-12-24 05:47:25.144468: Pseudo dice [np.float32(0.7381)]\n",
      "2024-12-24 05:47:25.144503: Epoch time: 111.17 s\n",
      "2024-12-24 05:47:25.753119: \n",
      "2024-12-24 05:47:25.753232: Epoch 964\n",
      "2024-12-24 05:47:25.753309: Current learning rate: 0.0005\n",
      "2024-12-24 05:49:16.989529: train_loss -0.8538\n",
      "2024-12-24 05:49:16.989672: val_loss -0.704\n",
      "2024-12-24 05:49:16.989704: Pseudo dice [np.float32(0.7595)]\n",
      "2024-12-24 05:49:16.989742: Epoch time: 111.24 s\n",
      "2024-12-24 05:49:17.607976: \n",
      "2024-12-24 05:49:17.608085: Epoch 965\n",
      "2024-12-24 05:49:17.608165: Current learning rate: 0.00049\n",
      "2024-12-24 05:51:08.803489: train_loss -0.8593\n",
      "2024-12-24 05:51:08.803629: val_loss -0.7558\n",
      "2024-12-24 05:51:08.803662: Pseudo dice [np.float32(0.8332)]\n",
      "2024-12-24 05:51:08.803697: Epoch time: 111.2 s\n",
      "2024-12-24 05:51:09.399227: \n",
      "2024-12-24 05:51:09.399580: Epoch 966\n",
      "2024-12-24 05:51:09.399673: Current learning rate: 0.00048\n",
      "2024-12-24 05:53:00.612307: train_loss -0.8369\n",
      "2024-12-24 05:53:00.612446: val_loss -0.7687\n",
      "2024-12-24 05:53:00.612481: Pseudo dice [np.float32(0.816)]\n",
      "2024-12-24 05:53:00.612517: Epoch time: 111.21 s\n",
      "2024-12-24 05:53:01.203191: \n",
      "2024-12-24 05:53:01.203386: Epoch 967\n",
      "2024-12-24 05:53:01.203483: Current learning rate: 0.00046\n",
      "2024-12-24 05:54:52.351767: train_loss -0.8564\n",
      "2024-12-24 05:54:52.351937: val_loss -0.7124\n",
      "2024-12-24 05:54:52.351969: Pseudo dice [np.float32(0.7485)]\n",
      "2024-12-24 05:54:52.352067: Epoch time: 111.15 s\n",
      "2024-12-24 05:54:53.400422: \n",
      "2024-12-24 05:54:53.400646: Epoch 968\n",
      "2024-12-24 05:54:53.400740: Current learning rate: 0.00045\n",
      "2024-12-24 05:56:44.605748: train_loss -0.8407\n",
      "2024-12-24 05:56:44.605903: val_loss -0.7046\n",
      "2024-12-24 05:56:44.605939: Pseudo dice [np.float32(0.7484)]\n",
      "2024-12-24 05:56:44.605974: Epoch time: 111.21 s\n",
      "2024-12-24 05:56:45.201888: \n",
      "2024-12-24 05:56:45.202224: Epoch 969\n",
      "2024-12-24 05:56:45.202303: Current learning rate: 0.00044\n",
      "2024-12-24 05:58:36.406543: train_loss -0.864\n",
      "2024-12-24 05:58:36.406962: val_loss -0.76\n",
      "2024-12-24 05:58:36.407044: Pseudo dice [np.float32(0.825)]\n",
      "2024-12-24 05:58:36.407088: Epoch time: 111.21 s\n",
      "2024-12-24 05:58:37.007165: \n",
      "2024-12-24 05:58:37.007370: Epoch 970\n",
      "2024-12-24 05:58:37.007509: Current learning rate: 0.00043\n",
      "2024-12-24 06:00:28.209231: train_loss -0.8741\n",
      "2024-12-24 06:00:28.209470: val_loss -0.7159\n",
      "2024-12-24 06:00:28.209509: Pseudo dice [np.float32(0.7576)]\n",
      "2024-12-24 06:00:28.209547: Epoch time: 111.2 s\n",
      "2024-12-24 06:00:28.804978: \n",
      "2024-12-24 06:00:28.805192: Epoch 971\n",
      "2024-12-24 06:00:28.805275: Current learning rate: 0.00041\n",
      "2024-12-24 06:02:20.078601: train_loss -0.8683\n",
      "2024-12-24 06:02:20.078777: val_loss -0.7121\n",
      "2024-12-24 06:02:20.078818: Pseudo dice [np.float32(0.7259)]\n",
      "2024-12-24 06:02:20.078854: Epoch time: 111.27 s\n",
      "2024-12-24 06:02:20.676143: \n",
      "2024-12-24 06:02:20.676432: Epoch 972\n",
      "2024-12-24 06:02:20.676681: Current learning rate: 0.0004\n",
      "2024-12-24 06:04:11.865433: train_loss -0.866\n",
      "2024-12-24 06:04:11.865570: val_loss -0.6934\n",
      "2024-12-24 06:04:11.865606: Pseudo dice [np.float32(0.7425)]\n",
      "2024-12-24 06:04:11.865643: Epoch time: 111.19 s\n",
      "2024-12-24 06:04:12.464898: \n",
      "2024-12-24 06:04:12.465348: Epoch 973\n",
      "2024-12-24 06:04:12.465437: Current learning rate: 0.00039\n",
      "2024-12-24 06:06:03.649303: train_loss -0.8661\n",
      "2024-12-24 06:06:03.649462: val_loss -0.7533\n",
      "2024-12-24 06:06:03.649498: Pseudo dice [np.float32(0.8259)]\n",
      "2024-12-24 06:06:03.649537: Epoch time: 111.19 s\n",
      "2024-12-24 06:06:04.248179: \n",
      "2024-12-24 06:06:04.248367: Epoch 974\n",
      "2024-12-24 06:06:04.248441: Current learning rate: 0.00037\n",
      "2024-12-24 06:07:55.452297: train_loss -0.8675\n",
      "2024-12-24 06:07:55.452437: val_loss -0.7404\n",
      "2024-12-24 06:07:55.452473: Pseudo dice [np.float32(0.8008)]\n",
      "2024-12-24 06:07:55.452509: Epoch time: 111.2 s\n",
      "2024-12-24 06:07:56.054074: \n",
      "2024-12-24 06:07:56.054427: Epoch 975\n",
      "2024-12-24 06:07:56.054566: Current learning rate: 0.00036\n",
      "2024-12-24 06:09:47.272972: train_loss -0.8617\n",
      "2024-12-24 06:09:47.273335: val_loss -0.6913\n",
      "2024-12-24 06:09:47.273415: Pseudo dice [np.float32(0.7435)]\n",
      "2024-12-24 06:09:47.273462: Epoch time: 111.22 s\n",
      "2024-12-24 06:09:47.877461: \n",
      "2024-12-24 06:09:47.877639: Epoch 976\n",
      "2024-12-24 06:09:47.877718: Current learning rate: 0.00035\n",
      "2024-12-24 06:11:39.061327: train_loss -0.8793\n",
      "2024-12-24 06:11:39.061569: val_loss -0.7935\n",
      "2024-12-24 06:11:39.061687: Pseudo dice [np.float32(0.8461)]\n",
      "2024-12-24 06:11:39.061729: Epoch time: 111.18 s\n",
      "2024-12-24 06:11:39.655555: \n",
      "2024-12-24 06:11:39.655758: Epoch 977\n",
      "2024-12-24 06:11:39.655841: Current learning rate: 0.00034\n",
      "2024-12-24 06:13:30.879158: train_loss -0.8667\n",
      "2024-12-24 06:13:30.879308: val_loss -0.6651\n",
      "2024-12-24 06:13:30.879343: Pseudo dice [np.float32(0.6954)]\n",
      "2024-12-24 06:13:30.879380: Epoch time: 111.22 s\n",
      "2024-12-24 06:13:31.479575: \n",
      "Epoch 9784 06:13:31.479753: \n",
      "2024-12-24 06:13:31.479887: Current learning rate: 0.00032\n",
      "2024-12-24 06:15:22.716720: train_loss -0.8718\n",
      "2024-12-24 06:15:22.716858: val_loss -0.7184\n",
      "2024-12-24 06:15:22.716893: Pseudo dice [np.float32(0.7625)]\n",
      "2024-12-24 06:15:22.716931: Epoch time: 111.24 s\n",
      "2024-12-24 06:15:23.314089: \n",
      "2024-12-24 06:15:23.314204: Epoch 979\n",
      "2024-12-24 06:15:23.314276: Current learning rate: 0.00031\n",
      "2024-12-24 06:17:14.547035: train_loss -0.868\n",
      "2024-12-24 06:17:14.547237: val_loss -0.7237\n",
      "2024-12-24 06:17:14.547274: Pseudo dice [np.float32(0.7822)]\n",
      "2024-12-24 06:17:14.547312: Epoch time: 111.23 s\n",
      "2024-12-24 06:17:15.148530: \n",
      "2024-12-24 06:17:15.148750: Epoch 980\n",
      "2024-12-24 06:17:15.148824: Current learning rate: 0.0003\n",
      "2024-12-24 06:19:06.369419: train_loss -0.8559\n",
      "2024-12-24 06:19:06.369638: val_loss -0.6847\n",
      "2024-12-24 06:19:06.369673: Pseudo dice [np.float32(0.6956)]\n",
      "2024-12-24 06:19:06.369722: Epoch time: 111.22 s\n",
      "2024-12-24 06:19:06.972795: \n",
      "2024-12-24 06:19:06.973190: Epoch 981\n",
      "2024-12-24 06:19:06.973354: Current learning rate: 0.00028\n",
      "2024-12-24 06:20:58.173317: train_loss -0.867\n",
      "2024-12-24 06:20:58.173456: val_loss -0.7866\n",
      "2024-12-24 06:20:58.173487: Pseudo dice [np.float32(0.8524)]\n",
      "2024-12-24 06:20:58.173523: Epoch time: 111.2 s\n",
      "2024-12-24 06:20:58.770016: \n",
      "2024-12-24 06:20:58.770353: Epoch 982\n",
      "2024-12-24 06:20:58.770438: Current learning rate: 0.00027\n",
      "2024-12-24 06:22:49.947579: train_loss -0.8806\n",
      "2024-12-24 06:22:49.947731: val_loss -0.7518\n",
      "2024-12-24 06:22:49.947768: Pseudo dice [np.float32(0.8082)]\n",
      "2024-12-24 06:22:49.947806: Epoch time: 111.18 s\n",
      "2024-12-24 06:22:50.547754: \n",
      "2024-12-24 06:22:50.548055: Epoch 983\n",
      "2024-12-24 06:22:50.548164: Current learning rate: 0.00026\n",
      "2024-12-24 06:24:41.721771: train_loss -0.8537\n",
      "2024-12-24 06:24:41.721912: val_loss -0.712\n",
      "2024-12-24 06:24:41.721948: Pseudo dice [np.float32(0.7925)]\n",
      "2024-12-24 06:24:41.721985: Epoch time: 111.17 s\n",
      "2024-12-24 06:24:42.318767: \n",
      "2024-12-24 06:24:42.319030: Epoch 984\n",
      "2024-12-24 06:24:42.319122: Current learning rate: 0.00024\n",
      "2024-12-24 06:26:33.539185: train_loss -0.8685\n",
      "2024-12-24 06:26:33.539386: val_loss -0.6917\n",
      "2024-12-24 06:26:33.539425: Pseudo dice [np.float32(0.7666)]\n",
      "2024-12-24 06:26:33.539464: Epoch time: 111.22 s\n",
      "2024-12-24 06:26:34.136601: \n",
      "2024-12-24 06:26:34.136798: Epoch 985\n",
      "2024-12-24 06:26:34.136872: Current learning rate: 0.00023\n",
      "2024-12-24 06:28:25.378472: train_loss -0.8605\n",
      "2024-12-24 06:28:25.378671: val_loss -0.7324\n",
      "2024-12-24 06:28:25.378704: Pseudo dice [np.float32(0.8057)]\n",
      "2024-12-24 06:28:25.378739: Epoch time: 111.24 s\n",
      "2024-12-24 06:28:26.406556: \n",
      "2024-12-24 06:28:26.406743: Epoch 986\n",
      "2024-12-24 06:28:26.406819: Current learning rate: 0.00021\n",
      "2024-12-24 06:30:17.614869: train_loss -0.8699\n",
      "2024-12-24 06:30:17.615023: val_loss -0.7442\n",
      "2024-12-24 06:30:17.615073: Pseudo dice [np.float32(0.7932)]\n",
      "2024-12-24 06:30:17.615111: Epoch time: 111.21 s\n",
      "2024-12-24 06:30:18.217213: \n",
      "2024-12-24 06:30:18.217523: Epoch 987\n",
      "2024-12-24 06:30:18.217619: Current learning rate: 0.0002\n",
      "2024-12-24 06:32:09.431509: train_loss -0.8746\n",
      "2024-12-24 06:32:09.431733: val_loss -0.7062\n",
      "2024-12-24 06:32:09.431767: Pseudo dice [np.float32(0.7307)]\n",
      "2024-12-24 06:32:09.431806: Epoch time: 111.21 s\n",
      "2024-12-24 06:32:10.030748: \n",
      "2024-12-24 06:32:10.030883: Epoch 988\n",
      "2024-12-24 06:32:10.030958: Current learning rate: 0.00019\n",
      "2024-12-24 06:34:01.267712: train_loss -0.8709\n",
      "2024-12-24 06:34:01.268021: val_loss -0.7272\n",
      "2024-12-24 06:34:01.268084: Pseudo dice [np.float32(0.7668)]\n",
      "2024-12-24 06:34:01.268127: Epoch time: 111.24 s\n",
      "2024-12-24 06:34:01.877734: \n",
      "2024-12-24 06:34:01.878069: Epoch 989\n",
      "2024-12-24 06:34:01.878353: Current learning rate: 0.00017\n",
      "2024-12-24 06:35:53.071844: train_loss -0.8634\n",
      "2024-12-24 06:35:53.071998: val_loss -0.7801\n",
      "2024-12-24 06:35:53.072034: Pseudo dice [np.float32(0.8254)]\n",
      "2024-12-24 06:35:53.072071: Epoch time: 111.19 s\n",
      "2024-12-24 06:35:53.669503: \n",
      "2024-12-24 06:35:53.669838: Epoch 990\n",
      "2024-12-24 06:35:53.670001: Current learning rate: 0.00016\n",
      "2024-12-24 06:37:44.824832: train_loss -0.8823\n",
      "2024-12-24 06:37:44.824971: val_loss -0.6796\n",
      "2024-12-24 06:37:44.825006: Pseudo dice [np.float32(0.7039)]\n",
      "2024-12-24 06:37:44.825044: Epoch time: 111.16 s\n",
      "2024-12-24 06:37:45.428474: \n",
      "2024-12-24 06:37:45.428674: Epoch 991\n",
      "2024-12-24 06:37:45.428751: Current learning rate: 0.00014\n",
      "2024-12-24 06:39:36.635990: train_loss -0.8716\n",
      "2024-12-24 06:39:36.636170: val_loss -0.6945\n",
      "2024-12-24 06:39:36.636245: Pseudo dice [np.float32(0.7384)]\n",
      "2024-12-24 06:39:36.636291: Epoch time: 111.21 s\n",
      "2024-12-24 06:39:37.234786: \n",
      "2024-12-24 06:39:37.234984: Epoch 992\n",
      "2024-12-24 06:39:37.235070: Current learning rate: 0.00013\n",
      "2024-12-24 06:41:28.468231: train_loss -0.8691\n",
      "2024-12-24 06:41:28.468375: val_loss -0.649\n",
      "2024-12-24 06:41:28.468408: Pseudo dice [np.float32(0.7253)]\n",
      "2024-12-24 06:41:28.468444: Epoch time: 111.23 s\n",
      "2024-12-24 06:41:29.069789: \n",
      "2024-12-24 06:41:29.069904: Epoch 993\n",
      "2024-12-24 06:41:29.069977: Current learning rate: 0.00011\n",
      "2024-12-24 06:43:20.251288: train_loss -0.8742\n",
      "2024-12-24 06:43:20.251561: val_loss -0.6602\n",
      "2024-12-24 06:43:20.251691: Pseudo dice [np.float32(0.7132)]\n",
      "2024-12-24 06:43:20.251734: Epoch time: 111.18 s\n",
      "2024-12-24 06:43:20.842082: \n",
      "2024-12-24 06:43:20.842277: Epoch 994\n",
      "2024-12-24 06:43:20.842352: Current learning rate: 0.0001\n",
      "2024-12-24 06:45:12.033275: train_loss -0.8783\n",
      "2024-12-24 06:45:12.033425: val_loss -0.6231\n",
      "2024-12-24 06:45:12.033460: Pseudo dice [np.float32(0.6079)]\n",
      "2024-12-24 06:45:12.033496: Epoch time: 111.19 s\n",
      "2024-12-24 06:45:12.631927: \n",
      "2024-12-24 06:45:12.632526: Epoch 995\n",
      "2024-12-24 06:45:12.632669: Current learning rate: 8e-05\n",
      "2024-12-24 06:47:03.856537: train_loss -0.8716\n",
      "2024-12-24 06:47:03.856679: val_loss -0.7137\n",
      "2024-12-24 06:47:03.856714: Pseudo dice [np.float32(0.7403)]\n",
      "2024-12-24 06:47:03.856750: Epoch time: 111.23 s\n",
      "2024-12-24 06:47:04.463955: \n",
      "2024-12-24 06:47:04.464157: Epoch 996\n",
      "2024-12-24 06:47:04.464241: Current learning rate: 7e-05\n",
      "2024-12-24 06:48:55.696582: train_loss -0.8552\n",
      "2024-12-24 06:48:55.697033: val_loss -0.632\n",
      "2024-12-24 06:48:55.697094: Pseudo dice [np.float32(0.6725)]\n",
      "2024-12-24 06:48:55.697135: Epoch time: 111.23 s\n",
      "2024-12-24 06:48:56.327025: \n",
      "2024-12-24 06:48:56.327403: Epoch 997\n",
      "2024-12-24 06:48:56.327495: Current learning rate: 5e-05\n",
      "2024-12-24 06:50:47.560817: train_loss -0.8703\n",
      "2024-12-24 06:50:47.561154: val_loss -0.7187\n",
      "2024-12-24 06:50:47.561223: Pseudo dice [np.float32(0.7844)]\n",
      "2024-12-24 06:50:47.561265: Epoch time: 111.23 s\n",
      "2024-12-24 06:50:48.154093: \n",
      "2024-12-24 06:50:48.154565: Epoch 998\n",
      "2024-12-24 06:50:48.154778: Current learning rate: 4e-05\n",
      "2024-12-24 06:52:39.373487: train_loss -0.8484\n",
      "2024-12-24 06:52:39.373670: val_loss -0.7348\n",
      "2024-12-24 06:52:39.373702: Pseudo dice [np.float32(0.793)]\n",
      "2024-12-24 06:52:39.373735: Epoch time: 111.22 s\n",
      "2024-12-24 06:52:39.989866: \n",
      "2024-12-24 06:52:39.990206: Epoch 999\n",
      "2024-12-24 06:52:39.990411: Current learning rate: 2e-05\n",
      "2024-12-24 06:54:31.262843: train_loss -0.8662\n",
      "2024-12-24 06:54:31.263008: val_loss -0.7637\n",
      "2024-12-24 06:54:31.263044: Pseudo dice [np.float32(0.7943)]\n",
      "2024-12-24 06:54:31.263080: Epoch time: 111.27 s\n",
      "2024-12-24 06:54:32.228870: Training done.\n",
      "2024-12-24 06:54:32.260806: Using splits from existing split file: /home/doodledaron/FYP/nnUnet/FYP-file/nnUNet_preprocessed/Dataset001_Lung/splits_final.json\n",
      "2024-12-24 06:54:32.263103: The split file contains 5 splits.\n",
      "2024-12-24 06:54:32.263264: Desired fold for training: 0\n",
      "2024-12-24 06:54:32.263330: This split has 50 training and 13 validation cases.\n",
      "2024-12-24 06:54:32.263589: predicting lung_006\n",
      "2024-12-24 06:54:32.271795: lung_006, shape torch.Size([1, 285, 637, 637]), rank 0\n",
      "2024-12-24 06:57:08.893263: predicting lung_010\n",
      "2024-12-24 06:57:08.899943: lung_010, shape torch.Size([1, 242, 390, 390]), rank 0\n",
      "2024-12-24 06:57:54.524798: predicting lung_033\n",
      "2024-12-24 06:57:54.529947: lung_033, shape torch.Size([1, 260, 535, 535]), rank 0\n",
      "2024-12-24 06:59:20.275317: predicting lung_034\n",
      "2024-12-24 06:59:20.280941: lung_034, shape torch.Size([1, 296, 586, 586]), rank 0\n",
      "2024-12-24 07:01:40.504489: predicting lung_041\n",
      "2024-12-24 07:01:40.512105: lung_041, shape torch.Size([1, 240, 535, 535]), rank 0\n",
      "2024-12-24 07:02:51.930976: predicting lung_042\n",
      "2024-12-24 07:02:51.935998: lung_042, shape torch.Size([1, 251, 478, 478]), rank 0\n",
      "2024-12-24 07:03:49.152693: predicting lung_046\n",
      "2024-12-24 07:03:49.157081: lung_046, shape torch.Size([1, 226, 509, 509]), rank 0\n",
      "2024-12-24 07:05:00.682962: predicting lung_048\n",
      "2024-12-24 07:05:00.687239: lung_048, shape torch.Size([1, 259, 531, 531]), rank 0\n",
      "2024-12-24 07:06:26.592655: predicting lung_059\n",
      "2024-12-24 07:06:26.598633: lung_059, shape torch.Size([1, 218, 535, 535]), rank 0\n",
      "2024-12-24 07:07:38.069245: predicting lung_065\n",
      "2024-12-24 07:07:38.073259: lung_065, shape torch.Size([1, 257, 474, 474]), rank 0\n",
      "2024-12-24 07:08:35.289185: predicting lung_066\n",
      "2024-12-24 07:08:35.292781: lung_066, shape torch.Size([1, 241, 578, 578]), rank 0\n",
      "2024-12-24 07:10:35.384418: predicting lung_070\n",
      "2024-12-24 07:10:35.390177: lung_070, shape torch.Size([1, 266, 497, 497]), rank 0\n",
      "2024-12-24 07:12:01.376267: predicting lung_079\n",
      "2024-12-24 07:12:01.382218: lung_079, shape torch.Size([1, 251, 606, 606]), rank 0\n",
      "2024-12-24 07:14:19.579409: Validation complete\n",
      "2024-12-24 07:14:19.579691: Mean Validation Dice:  0.620873577017229\n",
      "Standard U-Net training completed for 3d_fullres fold 0 -epoch 500\n",
      "Starting standard U-Net training: nnUNetv2_train 1 3d_fullres 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages/nnunetv2/training/nnUNetTrainer/nnUNetTrainer.py:164: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.grad_scaler = GradScaler() if self.device.type == 'cuda' else None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "############################\n",
      "INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md\n",
      "############################\n",
      "\n",
      "Using device: cuda:0\n",
      "\n",
      "#######################################################################\n",
      "Please cite the following paper when using nnU-Net:\n",
      "Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.\n",
      "#######################################################################\n",
      "\n",
      "2024-12-24 07:14:32.613387: do_dummy_2d_data_aug: False\n",
      "2024-12-24 07:14:32.613778: Using splits from existing split file: /home/doodledaron/FYP/nnUnet/FYP-file/nnUNet_preprocessed/Dataset001_Lung/splits_final.json\n",
      "2024-12-24 07:14:32.614178: The split file contains 5 splits.\n",
      "2024-12-24 07:14:32.614216: Desired fold for training: 1\n",
      "2024-12-24 07:14:32.614234: This split has 50 training and 13 validation cases.\n",
      "using pin_memory on device 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/doodledaron/.local/share/virtualenvs/FYP-file-w_hePvOQ/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using pin_memory on device 0\n",
      "2024-12-24 07:14:42.461506: Using torch.compile...\n",
      "\n",
      "This is the configuration used by this training:\n",
      "Configuration name: 3d_fullres\n",
      " {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 2, 'patch_size': [80, 192, 160], 'median_image_size_in_voxels': [252.0, 512.0, 512.0], 'spacing': [1.244979977607727, 0.78515625, 0.78515625], 'normalization_schemes': ['CTNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True} \n",
      "\n",
      "These are the global plan.json settings:\n",
      " {'dataset_name': 'Dataset001_Lung', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.244979977607727, 0.78515625, 0.78515625], 'original_median_shape_after_transp': [252, 512, 512], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 2671.0, 'mean': -273.4598083496094, 'median': -162.0, 'min': -1024.0, 'percentile_00_5': -1024.0, 'percentile_99_5': 311.0, 'std': 346.9495849609375}}} \n",
      "\n",
      "2024-12-24 07:14:43.846452: unpacking dataset...\n"
     ]
    }
   ],
   "source": [
    "class TrainingConfig:\n",
    "    def __init__(self, dataset_id: int = 1, epoch: int = 1000):\n",
    "        self.dataset_id = dataset_id\n",
    "        self.epoch = epoch\n",
    "        self.configurations = ['2d', '3d_fullres', '3d_lowres', '3d_cascade_fullres']\n",
    "        self.folds = [0, 1, 2, 3, 4]\n",
    "        self.checkpoint_path = os.path.join(os.environ[\"nnUNet_results\"],\n",
    "                                          f\"Dataset{dataset_id:03d}_Lung/checkpoints\")\n",
    "        os.makedirs(self.checkpoint_path, exist_ok=True)\n",
    "\n",
    "    def train_quick_evaluation(self, config: str, fold: int = 0):\n",
    "        \"\"\"Quick training for initial evaluation (1 epoch)\"\"\"\n",
    "        try:\n",
    "            command = f'nnUNetv2_train {self.dataset_id} {config} {fold} -tr nnUNetTrainer_1epoch'\n",
    "            print(f\"Starting quick evaluation training: {command}\")\n",
    "            os.system(command)\n",
    "            print(f\"Quick evaluation training completed for {config} fold {fold}\")\n",
    "            \n",
    "            \n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Quick evaluation failed: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    #first gameplan\n",
    "    def train_standard_unet(self, config: str, epoch):\n",
    "        \"\"\"Train with standard U-Net configuration for all 5 folds\"\"\"\n",
    "        num_folds = 3 #testing purpose, train with 3 folds instead of 5, 500 epochs instead of 1000\n",
    "        for fold in range(num_folds):\n",
    "            try:\n",
    "                # to continue training after paused (Ctrl C) addd  --continue_training flag at the end of command\n",
    "                command = f'nnUNetv2_train {self.dataset_id} {config} {fold}'\n",
    "                print(f\"Starting standard U-Net training: {command}\")\n",
    "                os.system(command)\n",
    "                print(f\"Standard U-Net training completed for {config} fold {fold} -epoch {epoch}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Standard U-Net training failed for fold {fold}: {str(e)}\")\n",
    "                raise\n",
    "        print(f\"Standard U-Net training completed for all {num_folds} folds of {config}\")\n",
    "\n",
    "    def train_residual_encoder(self, config: str, fold: int = 0):\n",
    "        \"\"\"Train with Residual Encoder U-Net configuration\"\"\"\n",
    "        try:\n",
    "            # Plan with ResEnc configuration\n",
    "            os.system(f'nnUNetv2_plan_experiment -d {self.dataset_id} -pl nnUNetPlannerResEncM')\n",
    "\n",
    "            # Train with ResEnc configuration\n",
    "            command = f'nnUNetv2_train {self.dataset_id} {config} {fold} -p nnUNetResEncUNetMPlans'\n",
    "            print(f\"Starting ResEnc training: {command}\")\n",
    "            os.system(command)\n",
    "            print(f\"ResEnc training completed for {config} fold {fold}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"ResEnc training failed: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def compare_model_training(self, fold: int = 0):\n",
    "        \"\"\"Run different model trainings for comparison\"\"\"\n",
    "        print(\"Starting model training comparison...\")\n",
    "\n",
    "        try:\n",
    "            # 1. Quick evaluation of standard 3D U-Net\n",
    "            print(\"Training standard 3D U-Net...\")\n",
    "            self.train_quick_evaluation('3d_fullres', fold)\n",
    "\n",
    "            # 2. Quick evaluation of 2D U-Net\n",
    "            print(\"Training 2D U-Net...\")\n",
    "            self.train_quick_evaluation('2d', fold)\n",
    "\n",
    "            # 3. Quick evaluation of ResEnc\n",
    "            print(\"Training ResEnc U-Net...\")\n",
    "            os.system(f'nnUNetv2_plan_experiment -d {self.dataset_id} -pl nnUNetPlannerResEncM')\n",
    "            command = f'nnUNetv2_train {self.dataset_id} 3d_fullres {fold} -tr nnUNetTrainer_1epoch -p nnUNetResEncUNetMPlans'\n",
    "            os.system(command)\n",
    "\n",
    "            print(\"All model trainings completed successfully\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Model training comparison failed: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "trainer = TrainingConfig()\n",
    "\n",
    "# For quick initial comparison of different models\n",
    "# trainer.compare_models()\n",
    "\n",
    "# After identifying the best model, train it fully\n",
    "# trainer.train_best_model_full('3d_fullres')  # Uncomment to run full training\n",
    "\n",
    "# Or try specific configurations\n",
    "# trainer.train_quick_evaluation('3d_fullres')  # Quick 1-epoch training\n",
    "trainer.train_standard_unet('3d_fullres', 500)     # Full training with standard U-Net\n",
    "# trainer.train_residual_encoder('3d_fullres')  # Training with ResEnc\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wx6e79DDjAfq"
   },
   "source": [
    "# 5.  Evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xesir8bnjE8J"
   },
   "source": [
    "## 5.1 Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 367
    },
    "executionInfo": {
     "elapsed": 482,
     "status": "error",
     "timestamp": 1734447744010,
     "user": {
      "displayName": "Yu Xuan Tang",
      "userId": "11252039200120625827"
     },
     "user_tz": -480
    },
    "id": "R4V-goDx6BFL",
    "outputId": "2ed4a06c-d44d-4f57-f43e-e91d4b6db9fd"
   },
   "outputs": [],
   "source": [
    "class ModelEvaluator:\n",
    "    def __init__(self, dataset_id: int = 1):\n",
    "        self.dataset_id = dataset_id\n",
    "        self.results_path = os.path.join(\n",
    "            os.environ[\"nnUNet_results\"],\n",
    "            f\"Dataset{dataset_id:03d}_Lung\"\n",
    "        )\n",
    "\n",
    "    def calculate_metrics(self, pred: np.ndarray, truth: np.ndarray) -> Dict:\n",
    "        \"\"\"Calculate evaluation metrics\"\"\"\n",
    "        metrics = {}\n",
    "\n",
    "        # Dice score\n",
    "        intersection = np.sum(pred * truth)\n",
    "        dice = (2. * intersection) / (np.sum(pred) + np.sum(truth))\n",
    "        metrics['dice'] = dice\n",
    "\n",
    "        # Hausdorff distance 95\n",
    "        if np.sum(pred) > 0 and np.sum(truth) > 0:\n",
    "            metrics['hd95'] = hd95(pred, truth)\n",
    "        else:\n",
    "            metrics['hd95'] = float('nan')\n",
    "\n",
    "        return metrics\n",
    "\n",
    "    def evaluate_fold(self, config: str, fold: int) -> Dict:\n",
    "        \"\"\"Evaluate specific configuration and fold\"\"\"\n",
    "        fold_results = {}\n",
    "        validation_path = os.path.join(\n",
    "            self.results_path,\n",
    "            f\"nnUNetTrainer__{config}/{fold}/validation\"\n",
    "        )\n",
    "\n",
    "        # Load predictions and ground truth\n",
    "        for file in os.listdir(validation_path):\n",
    "            if file.endswith(\".nii.gz\") and not file.startswith(\"._\"):\n",
    "                pred = nib.load(os.path.join(validation_path, file)).get_fdata()\n",
    "                truth = nib.load(os.path.join(\n",
    "                    os.environ[\"nnUNet_raw\"],\n",
    "                    f\"Dataset{self.dataset_id:03d}_Lung/labelsTr\",\n",
    "                    file\n",
    "                )).get_fdata()\n",
    "\n",
    "                metrics = self.calculate_metrics(pred, truth)\n",
    "                fold_results[file] = metrics\n",
    "\n",
    "        return fold_results\n",
    "    def evaluate_fold(self, config: str, fold: int) -> Dict:\n",
    "        \"\"\"Evaluate specific configuration and fold\"\"\"\n",
    "        try:\n",
    "            # Check GPU memory\n",
    "            available_memory = ErrorHandler.check_gpu_memory()\n",
    "            if available_memory < 2e9:  # Less than 2GB free\n",
    "                raise RuntimeError(\"Insufficient GPU memory for evaluation\")\n",
    "\n",
    "            fold_results = {}\n",
    "            validation_path = os.path.join(\n",
    "                self.results_path,\n",
    "                f\"nnUNetTrainer__{config}/{fold}/validation\"\n",
    "            )\n",
    "\n",
    "            if not os.path.exists(validation_path):\n",
    "                raise FileNotFoundError(f\"Validation path not found: {validation_path}\")\n",
    "            # Load predictions and ground truth\n",
    "            for file in os.listdir(validation_path):\n",
    "                if file.endswith(\".nii.gz\") and not file.startswith(\"._\"):\n",
    "                    pred = nib.load(os.path.join(validation_path, file)).get_fdata()\n",
    "                    truth = nib.load(os.path.join(\n",
    "                        os.environ[\"nnUNet_raw\"],\n",
    "                        f\"Dataset{self.dataset_id:03d}_Lung/labelsTr\",\n",
    "                        file\n",
    "                    )).get_fdata()\n",
    "\n",
    "                    metrics = self.calculate_metrics(pred, truth)\n",
    "                    fold_results[file] = metrics\n",
    "\n",
    "            return fold_results\n",
    "        except Exception as e:\n",
    "            print(f\"Evaluation failed: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "\n",
    "evaluator = ModelEvaluator()\n",
    "\n",
    "\n",
    "# # Then run evaluation separately\n",
    "evaluator = ModelEvaluator(dataset_id=1)\n",
    "\n",
    "# Evaluate each model\n",
    "standard_3d_results = evaluator.evaluate_fold('3d_fullres', 0)\n",
    "# standard_2d_results = evaluator.evaluate_fold('2d', 0)\n",
    "# resenc_results = evaluator.evaluate_fold('3d_fullres', 0)  # with ResEnc plans\n",
    "\n",
    "# Track results\n",
    "# tracker = TrainingTracker(dataset_id=1)\n",
    "# tracker.add_result('3d_fullres', 0, standard_3d_results, \"Standard 3D U-Net\")\n",
    "# tracker.add_result('2d', 0, standard_2d_results, \"2D U-Net (1 epoch)\")\n",
    "# tracker.add_result('3d_fullres_resenc', 0, resenc_results, \"ResEnc U-Net (1 epoch)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NrEs5az6jew0"
   },
   "source": [
    "# 6. Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lNBh4YsSjha2"
   },
   "source": [
    "## 6.1 Results Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vK5TEM-Gjgxi"
   },
   "outputs": [],
   "source": [
    "class ResultVisualizer:\n",
    "    def __init__(self, dataset_id: int = 1):\n",
    "        self.dataset_id = dataset_id\n",
    "\n",
    "    def plot_slice_overlay(self, image: np.ndarray, mask: np.ndarray, pred: np.ndarray, slice_idx: int):\n",
    "        \"\"\"Plot image slice with ground truth and prediction overlay\"\"\"\n",
    "        plt.figure(figsize=(15, 5))\n",
    "\n",
    "        # Original image\n",
    "        plt.subplot(131)\n",
    "        plt.imshow(image[slice_idx], cmap='gray')\n",
    "        plt.title('Original Image')\n",
    "\n",
    "        # Ground truth overlay\n",
    "        plt.subplot(132)\n",
    "        plt.imshow(image[slice_idx], cmap='gray')\n",
    "        plt.imshow(mask[slice_idx], alpha=0.5, cmap='red')\n",
    "        plt.title('Ground Truth')\n",
    "\n",
    "        # Prediction overlay\n",
    "        plt.subplot(133)\n",
    "        plt.imshow(image[slice_idx], cmap='gray')\n",
    "        plt.imshow(pred[slice_idx], alpha=0.5, cmap='blue')\n",
    "        plt.title('Prediction')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def plot_3d_surface(self, mask: np.ndarray):\n",
    "        \"\"\"Create 3D surface plot of segmentation\"\"\"\n",
    "        verts, faces, _, _ = marching_cubes(mask)\n",
    "\n",
    "        fig = plt.figure(figsize=(10, 10))\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "        ax.plot_trisurf(verts[:, 0], verts[:, 1], verts[:, 2],\n",
    "                       triangles=faces, cmap='viridis', alpha=0.7)\n",
    "        plt.show()\n",
    "\n",
    "    def create_evaluation_dashboard(self, results: Dict):\n",
    "        \"\"\"Create dashboard with evaluation results\"\"\"\n",
    "        # Metrics summary\n",
    "        metrics_df = pd.DataFrame(results).T\n",
    "\n",
    "        # Boxplot of metrics\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        sns.boxplot(data=metrics_df)\n",
    "        plt.title('Distribution of Evaluation Metrics')\n",
    "        plt.show()\n",
    "\n",
    "        # Save metrics to wandb\n",
    "        wandb.log({\n",
    "            \"dice_score\": metrics_df['dice'].mean(),\n",
    "            \"hd95\": metrics_df['hd95'].mean()\n",
    "        })\n",
    "\n",
    "visualizer = ResultVisualizer()\n",
    "# Example usage requires loading specific cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e39vEtgbjryN"
   },
   "source": [
    "# 7. Export Visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NEI6oyMRjuDp"
   },
   "source": [
    "## 7.1 Save Predictions and Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ghLFvA8ljtcB"
   },
   "outputs": [],
   "source": [
    "class ResultsExporter:\n",
    "    def __init__(self, dataset_id: int = 1):\n",
    "        self.dataset_id = dataset_id\n",
    "        self.export_path = os.path.join(os.environ[\"nnUNet_results\"],\n",
    "                                      f\"Dataset{self.dataset_id:03d}_Lung/exports\")\n",
    "        os.makedirs(self.export_path, exist_ok=True)\n",
    "\n",
    "    def export_predictions(self, output_folder: str):\n",
    "        \"\"\"Export predictions to NIFTI files\"\"\"\n",
    "        print(\"Exporting predictions...\")\n",
    "        os.system(f'nnUNetv2_export_predictions -i {output_folder} -o {self.export_path}')\n",
    "\n",
    "    def generate_report(self, results: Dict):\n",
    "        \"\"\"Generate PDF report with results\"\"\"\n",
    "        report_path = os.path.join(self.export_path, \"evaluation_report.pdf\")\n",
    "\n",
    "        # Create report using your preferred library\n",
    "        # Example: ReportLab, fpdf, etc.\n",
    "\n",
    "        print(f\"Report generated at {report_path}\")\n",
    "\n",
    "    def save_metrics(self, results: Dict):\n",
    "        \"\"\"Save metrics to JSON\"\"\"\n",
    "        metrics_path = os.path.join(self.export_path, \"metrics.json\")\n",
    "        with open(metrics_path, 'w') as f:\n",
    "            json.dump(results, f, indent=4)\n",
    "        print(f\"Metrics saved to {metrics_path}\")\n",
    "\n",
    "    def export_visualization(self, image: np.ndarray, pred: np.ndarray, truth: np.ndarray):\n",
    "        \"\"\"Export visualization of results\"\"\"\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        viz_path = os.path.join(self.export_path, f\"visualization_{timestamp}\")\n",
    "        os.makedirs(viz_path, exist_ok=True)\n",
    "\n",
    "        # Save middle slice visualization\n",
    "        slice_idx = image.shape[0] // 2\n",
    "        plt.figure(figsize=(15, 5))\n",
    "\n",
    "        plt.subplot(131)\n",
    "        plt.imshow(image[slice_idx], cmap='gray')\n",
    "        plt.title('Original Image')\n",
    "\n",
    "        plt.subplot(132)\n",
    "        plt.imshow(image[slice_idx], cmap='gray')\n",
    "        plt.imshow(truth[slice_idx], alpha=0.5, cmap='red')\n",
    "        plt.title('Ground Truth')\n",
    "\n",
    "        plt.subplot(133)\n",
    "        plt.imshow(image[slice_idx], cmap='gray')\n",
    "        plt.imshow(pred[slice_idx], alpha=0.5, cmap='blue')\n",
    "        plt.title('Prediction')\n",
    "\n",
    "        plt.savefig(os.path.join(viz_path, 'comparison.png'))\n",
    "        plt.close()\n",
    "\n",
    "    def generate_performance_report(self, results: Dict):\n",
    "        \"\"\"Generate detailed performance report\"\"\"\n",
    "        report = {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"dataset_id\": self.dataset_id,\n",
    "            \"metrics\": {\n",
    "                \"dice\": {\n",
    "                    \"mean\": np.mean([r['dice'] for r in results.values()]),\n",
    "                    \"std\": np.std([r['dice'] for r in results.values()]),\n",
    "                    \"min\": np.min([r['dice'] for r in results.values()]),\n",
    "                    \"max\": np.max([r['dice'] for r in results.values()]),\n",
    "                },\n",
    "                \"hd95\": {\n",
    "                    \"mean\": np.mean([r['hd95'] for r in results.values() if not np.isnan(r['hd95'])]),\n",
    "                    \"std\": np.std([r['hd95'] for r in results.values() if not np.isnan(r['hd95'])]),\n",
    "                    \"min\": np.min([r['hd95'] for r in results.values() if not np.isnan(r['hd95'])]),\n",
    "                    \"max\": np.max([r['hd95'] for r in results.values() if not np.isnan(r['hd95'])]),\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "\n",
    "        report_path = os.path.join(self.export_path, \"performance_report.json\")\n",
    "        with open(report_path, 'w') as f:\n",
    "            json.dump(report, f, indent=4)\n",
    "\n",
    "# Example usage\n",
    "exporter = ResultsExporter()\n",
    "# After you have results:\n",
    "# exporter.save_metrics(results)\n",
    "# exporter.generate_report(results)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMS5+MfL2vX9zQV3XuIXqH+",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "FYP-file (Pipenv)",
   "language": "python",
   "name": "fyp-file"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
